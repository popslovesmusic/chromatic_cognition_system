 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/benches/dream_benchmarks.rs b/benches/dream_benchmarks.rs
index b4b7c8dd2caefaa048bed4232e974a89e6471502..376d3964a9c309d61155a68df15223cc6a9c66d9 100644
--- a/benches/dream_benchmarks.rs
+++ b/benches/dream_benchmarks.rs
@@ -1,38 +1,38 @@
 //! Performance benchmarks for dream pool optimizations
 //!
 //! Run with: cargo bench --bench dream_benchmarks
 
-use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
-use chromatic_cognition_core::dream::*;
-use chromatic_cognition_core::dream::soft_index::{SoftIndex, EntryId, Similarity};
-use chromatic_cognition_core::dream::hnsw_index::HnswIndex;
 use chromatic_cognition_core::dream::diversity::{retrieve_diverse_mmr, retrieve_diverse_mmr_fast};
-use chromatic_cognition_core::dream::query_cache::QueryCache;
 use chromatic_cognition_core::dream::embedding::QuerySignature;
+use chromatic_cognition_core::dream::hnsw_index::HnswIndex;
+use chromatic_cognition_core::dream::query_cache::QueryCache;
+use chromatic_cognition_core::dream::soft_index::{EntryId, Similarity, SoftIndex};
+use chromatic_cognition_core::dream::*;
 use chromatic_cognition_core::solver::SolverResult;
 use chromatic_cognition_core::tensor::ChromaticTensor;
+use criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};
 use serde_json::json;
 
 /// Benchmark query cache hit rates
 fn bench_query_cache(c: &mut Criterion) {
     let mut cache = QueryCache::new(128);
 
     let query1 = [1.0, 0.0, 0.0];
     let query2 = [0.0, 1.0, 0.0];
 
     // Pre-populate cache
     cache.get_or_compute(&query1, |q| vec![q[0], q[1], q[2], 0.5]);
 
     c.bench_function("query_cache_hit", |b| {
         b.iter(|| {
             black_box(cache.get_or_compute(&query1, |q| vec![q[0], q[1], q[2], 0.5]));
         });
     });
 
     c.bench_function("query_cache_miss", |b| {
         b.iter(|| {
             black_box(cache.get_or_compute(&query2, |q| vec![q[0], q[1], q[2], 0.5]));
         });
     });
 }
 
@@ -46,124 +46,111 @@ fn bench_hnsw_vs_linear(c: &mut Criterion) {
         for i in 0..*size {
             let id = EntryId::new_v4();
             let embedding: Vec<f32> = (0..64)
                 .map(|j| ((i * 64 + j) as f32) / (size * 64) as f32)
                 .collect();
             embeddings.push((id, embedding));
         }
 
         // Build SoftIndex (linear)
         let mut soft_index = SoftIndex::new(64);
         for (id, emb) in &embeddings {
             soft_index.add(*id, emb.clone()).unwrap();
         }
         soft_index.build();
 
         // Build HNSW
         let mut hnsw = HnswIndex::new(64, *size);
         for (id, emb) in &embeddings {
             hnsw.add(*id, emb.clone()).unwrap();
         }
         hnsw.build(Similarity::Cosine).unwrap();
 
         let query: Vec<f32> = (0..64).map(|i| (i as f32) / 64.0).collect();
 
         // Benchmark linear search
-        group.bench_with_input(
-            BenchmarkId::new("linear", size),
-            size,
-            |b, _| {
-                b.iter(|| {
-                    black_box(soft_index.query(&query, 10, Similarity::Cosine).unwrap());
-                });
-            },
-        );
+        group.bench_with_input(BenchmarkId::new("linear", size), size, |b, _| {
+            b.iter(|| {
+                black_box(soft_index.query(&query, 10, Similarity::Cosine).unwrap());
+            });
+        });
 
         // Benchmark HNSW search
-        group.bench_with_input(
-            BenchmarkId::new("hnsw", size),
-            size,
-            |b, _| {
-                b.iter(|| {
-                    black_box(hnsw.search(&query, 10, Similarity::Cosine).unwrap());
-                });
-            },
-        );
+        group.bench_with_input(BenchmarkId::new("hnsw", size), size, |b, _| {
+            b.iter(|| {
+                black_box(hnsw.search(&query, 10, Similarity::Cosine).unwrap());
+            });
+        });
     }
 
     group.finish();
 }
 
 /// Benchmark MMR standard vs fast
 fn bench_mmr_diversity(c: &mut Criterion) {
     let mut group = c.benchmark_group("mmr_diversity");
 
     for k in [10, 20, 50].iter() {
         // Create test candidates
         let mut candidates = vec![];
         for i in 0..100 {
             let tensor = ChromaticTensor::new(4, 4, 3);
             let result = SolverResult {
                 energy: 0.1,
                 coherence: 0.9,
                 violation: 0.0,
                 grad: None,
                 mask: None,
                 meta: json!({}),
             };
             let mut entry = DreamEntry::new(tensor, result);
-            entry.chroma_signature = [
-                (i as f32) / 100.0,
-                1.0 - (i as f32) / 100.0,
-                0.0
-            ];
+            entry.chroma_signature = [(i as f32) / 100.0, 1.0 - (i as f32) / 100.0, 0.0];
             candidates.push(entry);
         }
 
         let query = [1.0, 0.0, 0.0];
 
         // Benchmark standard MMR
-        group.bench_with_input(
-            BenchmarkId::new("standard", k),
-            k,
-            |b, k| {
-                b.iter(|| {
-                    black_box(retrieve_diverse_mmr(&candidates, &query, *k, 0.5, 0.0));
-                });
-            },
-        );
+        group.bench_with_input(BenchmarkId::new("standard", k), k, |b, k| {
+            b.iter(|| {
+                black_box(retrieve_diverse_mmr(&candidates, &query, *k, 0.5, 0.0));
+            });
+        });
 
         // Benchmark fast MMR with sampling
-        group.bench_with_input(
-            BenchmarkId::new("fast", k),
-            k,
-            |b, k| {
-                b.iter(|| {
-                    black_box(retrieve_diverse_mmr_fast(&candidates, &query, *k, 0.5, 0.0, 5));
-                });
-            },
-        );
+        group.bench_with_input(BenchmarkId::new("fast", k), k, |b, k| {
+            b.iter(|| {
+                black_box(retrieve_diverse_mmr_fast(
+                    &candidates,
+                    &query,
+                    *k,
+                    0.5,
+                    0.0,
+                    5,
+                ));
+            });
+        });
     }
 
     group.finish();
 }
 
 /// Benchmark spectral feature extraction
 fn bench_spectral_features(c: &mut Criterion) {
     use chromatic_cognition_core::spectral::{extract_spectral_features, WindowFunction};
 
     c.bench_function("spectral_extraction", |b| {
         let tensor = ChromaticTensor::new(8, 8, 4);
         b.iter(|| {
             black_box(extract_spectral_features(&tensor, WindowFunction::Hann));
         });
     });
 }
 
 /// Benchmark embedding encoding
 fn bench_embedding_encoding(c: &mut Criterion) {
     let mapper = EmbeddingMapper::new(64);
     let tensor = ChromaticTensor::new(4, 4, 3);
     let result = SolverResult {
         energy: 0.1,
         coherence: 0.9,
         violation: 0.0,
diff --git a/examples/demo.rs b/examples/demo.rs
index b292b5227309c02809b11fbb3648abc9a55d95e5..77f8923628b36eefcb8865a0c9fcb7feaae944c4 100644
--- a/examples/demo.rs
+++ b/examples/demo.rs
@@ -1,31 +1,31 @@
 use std::path::PathBuf;
 
 use chromatic_cognition_core::config::ConfigError;
 use chromatic_cognition_core::logging;
 use chromatic_cognition_core::{
-    ChromaticTensor, EngineConfig, GradientLayer, complement, filter, mix, mse_loss, saturate,
+    complement, filter, mix, mse_loss, saturate, ChromaticTensor, EngineConfig, GradientLayer,
 };
 
 fn main() -> Result<(), Box<dyn std::error::Error>> {
     let config = load_config()?;
     println!(
         "Loaded config: rows={} cols={} layers={} seed={}",
         config.rows, config.cols, config.layers, config.seed
     );
 
     let primary = ChromaticTensor::from_seed(config.seed, config.rows, config.cols, config.layers);
     let secondary = ChromaticTensor::from_seed(
         config.seed ^ 0xABCD_EF01,
         config.rows,
         config.cols,
         config.layers,
     );
 
     let mixed = mix(&primary, &secondary);
     let filtered = filter(&mixed, &secondary);
     let complemented = complement(&filtered);
     let saturated = saturate(&complemented, 1.25);
 
     let gradient = GradientLayer::from_tensor(&saturated);
     gradient.to_png(PathBuf::from("out/frame_0001.png"))?;
 
diff --git a/examples/dream_validation.rs b/examples/dream_validation.rs
index 3d1e901b6e38c85a79d9212d835add66ae76b847..1083bedec117300c49a6fb8ba829b2c7c4bc76cb 100644
--- a/examples/dream_validation.rs
+++ b/examples/dream_validation.rs
@@ -1,42 +1,44 @@
 //! Dream Pool Validation Experiment
 //!
 //! This example implements the A/B testing methodology described in
 //! "Validation Experiment Specification: Retrieval Hypothesis"
 //!
 //! It compares two groups:
 //! - Group A (Control): Random noise seeding
 //! - Group B (Test): Retrieval-based seeding from SimpleDreamPool
 //!
 //! Run with:
 //! ```
 //! cargo run --example dream_validation --release
 //! ```
 
 use chromatic_cognition_core::data::DatasetConfig;
+use chromatic_cognition_core::dream::experiment::{
+    ExperimentConfig, ExperimentHarness, SeedingStrategy,
+};
 use chromatic_cognition_core::dream::simple_pool::PoolConfig;
-use chromatic_cognition_core::dream::experiment::{ExperimentConfig, ExperimentHarness, SeedingStrategy};
 use chromatic_cognition_core::ChromaticNativeSolver;
 use std::fs::File;
 use std::io::Write;
 
 fn main() {
     println!("=== Dream Pool Validation Experiment ===\n");
 
     // Shared configuration
     let dataset_config = DatasetConfig {
         tensor_size: (16, 16, 4), // Small tensors for fast iteration
         noise_level: 0.1,
         samples_per_class: 50, // 500 total samples
         seed: 42,
     };
 
     let pool_config = PoolConfig {
         max_size: 200,
         coherence_threshold: 0.7, // Only keep high-coherence dreams
         retrieval_limit: 3,
         use_hnsw: true,
         memory_budget_mb: Some(500),
     };
 
     let num_epochs = 30;
     let dream_iterations = 5;
@@ -72,78 +74,96 @@ fn main() {
         dataset_config,
         seed: 42,
     };
 
     let solver_b = ChromaticNativeSolver::default();
     let mut harness_b = ExperimentHarness::new(config_b, solver_b);
     let result_b = harness_b.run();
 
     println!("  Final Accuracy: {:.4}", result_b.final_accuracy);
     println!("  Convergence Epoch: {:?}", result_b.convergence_epoch);
     println!("  Total Time: {}ms", result_b.total_elapsed_ms);
 
     if let Some(stats) = harness_b.pool_stats() {
         println!("  Pool Size: {}", stats.count);
         println!("  Pool Mean Coherence: {:.4}\n", stats.mean_coherence);
     }
 
     // Compare results
     println!("=== Comparison ===");
     println!("Group A Final Accuracy: {:.4}", result_a.final_accuracy);
     println!("Group B Final Accuracy: {:.4}", result_b.final_accuracy);
 
     let accuracy_improvement = result_b.final_accuracy - result_a.final_accuracy;
     let improvement_pct = (accuracy_improvement / result_a.final_accuracy) * 100.0;
 
-    println!("Accuracy Improvement: {:.4} ({:.2}%)", accuracy_improvement, improvement_pct);
+    println!(
+        "Accuracy Improvement: {:.4} ({:.2}%)",
+        accuracy_improvement, improvement_pct
+    );
 
     if let (Some(conv_a), Some(conv_b)) = (result_a.convergence_epoch, result_b.convergence_epoch) {
         let epoch_reduction = conv_a as i32 - conv_b as i32;
         println!("Group A Convergence: epoch {}", conv_a);
         println!("Group B Convergence: epoch {}", conv_b);
         println!("Epochs Saved: {}\n", epoch_reduction);
     } else {
-        println!("Convergence: A={:?}, B={:?}\n", result_a.convergence_epoch, result_b.convergence_epoch);
+        println!(
+            "Convergence: A={:?}, B={:?}\n",
+            result_a.convergence_epoch, result_b.convergence_epoch
+        );
     }
 
     // Compare mean coherence across epochs
-    let mean_coherence_a: f64 = result_a.epoch_metrics.iter().map(|m| m.mean_coherence).sum::<f64>()
+    let mean_coherence_a: f64 = result_a
+        .epoch_metrics
+        .iter()
+        .map(|m| m.mean_coherence)
+        .sum::<f64>()
         / result_a.epoch_metrics.len() as f64;
-    let mean_coherence_b: f64 = result_b.epoch_metrics.iter().map(|m| m.mean_coherence).sum::<f64>()
+    let mean_coherence_b: f64 = result_b
+        .epoch_metrics
+        .iter()
+        .map(|m| m.mean_coherence)
+        .sum::<f64>()
         / result_b.epoch_metrics.len() as f64;
 
     println!("Mean Coherence (all epochs):");
     println!("  Group A: {:.4}", mean_coherence_a);
     println!("  Group B: {:.4}", mean_coherence_b);
     println!("  Difference: {:.4}\n", mean_coherence_b - mean_coherence_a);
 
     // Save detailed results to JSON
     println!("Saving results to logs/...");
     std::fs::create_dir_all("logs").expect("Failed to create logs directory");
 
     let json_a = serde_json::to_string_pretty(&result_a).expect("Failed to serialize Group A");
     let mut file_a = File::create("logs/experiment_group_a.json").expect("Failed to create file");
-    file_a.write_all(json_a.as_bytes()).expect("Failed to write");
+    file_a
+        .write_all(json_a.as_bytes())
+        .expect("Failed to write");
 
     let json_b = serde_json::to_string_pretty(&result_b).expect("Failed to serialize Group B");
     let mut file_b = File::create("logs/experiment_group_b.json").expect("Failed to create file");
-    file_b.write_all(json_b.as_bytes()).expect("Failed to write");
+    file_b
+        .write_all(json_b.as_bytes())
+        .expect("Failed to write");
 
     println!("Results saved!");
     println!("\n=== Experiment Complete ===");
 
     // Decision gate
     println!("\n=== Decision Gate ===");
     if improvement_pct > 5.0 && accuracy_improvement > 0.01 {
         println!("✅ HYPOTHESIS VALIDATED");
         println!("   Retrieval-based seeding shows meaningful improvement.");
         println!("   Recommendation: Proceed to Phase 2 (Persistence, FFT)");
     } else if improvement_pct > 0.0 {
         println!("⚠️  HYPOTHESIS WEAKLY SUPPORTED");
         println!("   Small improvement observed, but below significance threshold.");
         println!("   Recommendation: Tune parameters or investigate further");
     } else {
         println!("❌ HYPOTHESIS NOT VALIDATED");
         println!("   No improvement from retrieval-based seeding.");
         println!("   Recommendation: Defer Dream Pool implementation");
     }
 }
diff --git a/examples/dream_validation_full.rs b/examples/dream_validation_full.rs
index 09436f459f24dcbd77b0a063e5de1c407a237eac..d96735cd156a3787f8de283c904048fedf8e2058 100644
--- a/examples/dream_validation_full.rs
+++ b/examples/dream_validation_full.rs
@@ -1,77 +1,80 @@
 //! Comprehensive Dream Pool Validation Experiment with Statistical Analysis
 //!
 //! This example runs the full A/B test with detailed statistical analysis
 //! and generates comprehensive reports.
 //!
 //! Run with:
 //! ```
 //! cargo run --example dream_validation_full --release
 //! ```
 
 use chromatic_cognition_core::data::DatasetConfig;
-use chromatic_cognition_core::dream::simple_pool::PoolConfig;
+use chromatic_cognition_core::dream::analysis::{compare_experiments, generate_report};
 use chromatic_cognition_core::dream::experiment::{
     ExperimentConfig, ExperimentHarness, SeedingStrategy,
 };
-use chromatic_cognition_core::dream::analysis::{compare_experiments, generate_report};
+use chromatic_cognition_core::dream::simple_pool::PoolConfig;
 use chromatic_cognition_core::ChromaticNativeSolver;
 use std::fs::File;
 use std::io::Write;
 
 fn main() {
     println!("╔════════════════════════════════════════════════════════════════╗");
     println!("║   Dream Pool Validation Experiment - Full Analysis            ║");
     println!("║   Validation Experiment Specification: Retrieval Hypothesis    ║");
     println!("╚════════════════════════════════════════════════════════════════╝\n");
 
     // Shared configuration
     let dataset_config = DatasetConfig {
         tensor_size: (16, 16, 4),
         noise_level: 0.15,
         samples_per_class: 50,
         seed: 42,
     };
 
     let pool_config = PoolConfig {
         max_size: 500,
         coherence_threshold: 0.65,
         retrieval_limit: 3,
         use_hnsw: true,
         memory_budget_mb: Some(500),
     };
 
     let num_epochs = 40;
     let dream_iterations = 8;
 
     println!("Configuration:");
     println!("  Tensor Size: {:?}", dataset_config.tensor_size);
     println!("  Samples per Class: {}", dataset_config.samples_per_class);
     println!("  Total Samples: {}", dataset_config.samples_per_class * 10);
     println!("  Epochs: {}", num_epochs);
     println!("  Dream Iterations: {}", dream_iterations);
-    println!("  Pool Coherence Threshold: {}\n", pool_config.coherence_threshold);
+    println!(
+        "  Pool Coherence Threshold: {}\n",
+        pool_config.coherence_threshold
+    );
 
     // ========================================================================
     // Group A: Control (Random Noise)
     // ========================================================================
     println!("┌─────────────────────────────────────────────────────────────┐");
     println!("│ Group A: Control (Random Noise Seeding)                    │");
     println!("└─────────────────────────────────────────────────────────────┘");
 
     let config_a = ExperimentConfig {
         strategy: SeedingStrategy::RandomNoise,
         num_epochs,
         batch_size: 10,
         dream_iterations,
         pool_config: pool_config.clone(),
         dataset_config: dataset_config.clone(),
         seed: 42,
     };
 
     let solver_a = ChromaticNativeSolver::default();
     let mut harness_a = ExperimentHarness::new(config_a, solver_a);
 
     print!("Running experiment... ");
     std::io::stdout().flush().unwrap();
     let result_a = harness_a.run();
     println!("Done!");
@@ -136,99 +139,122 @@ fn main() {
             / result_b.epoch_metrics.len() as f64
     );
 
     // ========================================================================
     // Statistical Comparison
     // ========================================================================
     println!("┌─────────────────────────────────────────────────────────────┐");
     println!("│ Statistical Analysis                                        │");
     println!("└─────────────────────────────────────────────────────────────┘");
 
     let comparison = compare_experiments(&result_a, &result_b, 0.01);
     let report = generate_report(&comparison);
     println!("{}", report);
 
     // ========================================================================
     // Save Results
     // ========================================================================
     println!("\n┌─────────────────────────────────────────────────────────────┐");
     println!("│ Saving Results                                              │");
     println!("└─────────────────────────────────────────────────────────────┘");
 
     std::fs::create_dir_all("logs").expect("Failed to create logs directory");
 
     // Save raw results
     let json_a = serde_json::to_string_pretty(&result_a).expect("Failed to serialize Group A");
-    let mut file_a =
-        File::create("logs/validation_group_a.json").expect("Failed to create file");
-    file_a.write_all(json_a.as_bytes()).expect("Failed to write");
+    let mut file_a = File::create("logs/validation_group_a.json").expect("Failed to create file");
+    file_a
+        .write_all(json_a.as_bytes())
+        .expect("Failed to write");
     println!("✓ Group A results: logs/validation_group_a.json");
 
     let json_b = serde_json::to_string_pretty(&result_b).expect("Failed to serialize Group B");
-    let mut file_b =
-        File::create("logs/validation_group_b.json").expect("Failed to create file");
-    file_b.write_all(json_b.as_bytes()).expect("Failed to write");
+    let mut file_b = File::create("logs/validation_group_b.json").expect("Failed to create file");
+    file_b
+        .write_all(json_b.as_bytes())
+        .expect("Failed to write");
     println!("✓ Group B results: logs/validation_group_b.json");
 
     // Save comparison
     let json_comp =
         serde_json::to_string_pretty(&comparison).expect("Failed to serialize comparison");
     let mut file_comp =
         File::create("logs/validation_comparison.json").expect("Failed to create file");
     file_comp
         .write_all(json_comp.as_bytes())
         .expect("Failed to write");
     println!("✓ Comparison: logs/validation_comparison.json");
 
     // Save human-readable report
     let mut report_file =
         File::create("logs/validation_report.txt").expect("Failed to create report file");
     report_file
         .write_all(report.as_bytes())
         .expect("Failed to write report");
     println!("✓ Report: logs/validation_report.txt");
 
     // ========================================================================
     // Generate CSV for plotting
     // ========================================================================
-    let mut csv = String::from("epoch,group_a_accuracy,group_b_accuracy,group_a_coherence,group_b_coherence\n");
-    for (a, b) in result_a.epoch_metrics.iter().zip(result_b.epoch_metrics.iter()) {
+    let mut csv = String::from(
+        "epoch,group_a_accuracy,group_b_accuracy,group_a_coherence,group_b_coherence\n",
+    );
+    for (a, b) in result_a
+        .epoch_metrics
+        .iter()
+        .zip(result_b.epoch_metrics.iter())
+    {
         csv.push_str(&format!(
             "{},{},{},{},{}\n",
-            a.epoch, a.validation_accuracy, b.validation_accuracy, a.mean_coherence, b.mean_coherence
+            a.epoch,
+            a.validation_accuracy,
+            b.validation_accuracy,
+            a.mean_coherence,
+            b.mean_coherence
         ));
     }
 
     let mut csv_file =
         File::create("logs/validation_metrics.csv").expect("Failed to create CSV file");
-    csv_file.write_all(csv.as_bytes()).expect("Failed to write CSV");
+    csv_file
+        .write_all(csv.as_bytes())
+        .expect("Failed to write CSV");
     println!("✓ Metrics CSV: logs/validation_metrics.csv");
 
     println!("\n┌─────────────────────────────────────────────────────────────┐");
     println!("│ Experiment Complete                                         │");
     println!("└─────────────────────────────────────────────────────────────┘");
 
     // Final decision
     if comparison.is_significant {
         println!("\n🎉 SUCCESS: Retrieval hypothesis VALIDATED!");
-        println!("   ✓ Improvement: {:.2}%", comparison.accuracy_improvement_pct);
-        println!("   ✓ Statistically significant (p < {})", comparison.significance_level);
+        println!(
+            "   ✓ Improvement: {:.2}%",
+            comparison.accuracy_improvement_pct
+        );
+        println!(
+            "   ✓ Statistically significant (p < {})",
+            comparison.significance_level
+        );
         println!("\n   → RECOMMENDATION: Proceed to Phase 2");
         println!("     • Implement SQLite persistence");
         println!("     • Add FFT spectral analysis");
         println!("     • Develop full Dream Pool specification");
     } else if comparison.accuracy_improvement > 0.0 {
         println!("\n⚠️  MARGINAL: Hypothesis weakly supported");
-        println!("   • Improvement: {:.2}%", comparison.accuracy_improvement_pct);
+        println!(
+            "   • Improvement: {:.2}%",
+            comparison.accuracy_improvement_pct
+        );
         println!("   • Below significance threshold");
         println!("\n   → RECOMMENDATION: Investigate further");
         println!("     • Tune hyperparameters");
         println!("     • Increase dataset size");
         println!("     • Analyze failure modes");
     } else {
         println!("\n❌ NEGATIVE: Hypothesis not validated");
         println!("   • No improvement observed");
         println!("\n   → RECOMMENDATION: Defer Dream Pool");
         println!("     • Focus on core solver optimization");
         println!("     • Consider alternative approaches");
     }
 }
diff --git a/examples/learner_validation.rs b/examples/learner_validation.rs
index 270afdb2323c6803374133b2f7495132784de5e2..d56d4bc563adf61ee6478f04932e47d124c8eb15 100644
--- a/examples/learner_validation.rs
+++ b/examples/learner_validation.rs
@@ -1,46 +1,46 @@
 //! Learner Validation Experiment
 //!
 //! This example validates the Minimal Viable Learner (MVP) and demonstrates
 //! that real gradient descent training works with the color classification task.
 //!
 //! Validates:
 //! 1. Training can actually learn (reaches >90% accuracy)
 //! 2. Retrieval-based seeding from Dream Pool helps convergence
 //! 3. Proper feedback loop infrastructure for full LEARNER MANIFEST v1.0
 //!
 //! Run with:
 //! ```
 //! cargo run --example learner_validation --release
 //! ```
 
 use chromatic_cognition_core::data::{ColorDataset, DatasetConfig};
 use chromatic_cognition_core::dream::simple_pool::PoolConfig;
 use chromatic_cognition_core::dream::{RetrievalMode, SimpleDreamPool};
 use chromatic_cognition_core::{
-    ChromaticNativeSolver, ClassifierConfig, MLPClassifier, TrainingConfig,
-    train_baseline, train_with_dreams,
+    train_baseline, train_with_dreams, ChromaticNativeSolver, ClassifierConfig, MLPClassifier,
+    TrainingConfig,
 };
 use std::fs::File;
 use std::io::Write;
 
 fn main() {
     println!("╔══════════════════════════════════════════════════════════════╗");
     println!("║  Learner Validation Experiment - Minimal Viable Learner     ║");
     println!("║  Goal: Prove training works & Dream Pool helps convergence  ║");
     println!("╚══════════════════════════════════════════════════════════════╝\n");
 
     // Configuration
     let dataset_config = DatasetConfig {
         tensor_size: (16, 16, 4),
         noise_level: 0.1,
         samples_per_class: 100, // 1000 total samples
         seed: 42,
     };
 
     let classifier_config = ClassifierConfig {
         input_size: 16 * 16 * 4 * 3,
         hidden_size: 256,
         output_size: 10,
         seed: 42,
     };
 
@@ -66,243 +66,288 @@ fn main() {
     println!("  Val: {} samples\n", val_data.len());
 
     // ========================================================================
     // Experiment 1: Baseline Training (No Dream Pool)
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Experiment 1: Baseline Training (No Dream Pool)             │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let baseline_config = TrainingConfig {
         num_epochs: 100,
         batch_size: 32,
         learning_rate: 0.01,
         lr_decay: 0.98,
         use_dream_pool: false,
         num_dreams_retrieve: 0,
         retrieval_mode: RetrievalMode::Hard,
         seed: 42,
     };
 
     let classifier_baseline = MLPClassifier::new(classifier_config.clone());
 
     print!("Training baseline model... ");
     std::io::stdout().flush().unwrap();
 
-    let result_baseline = train_baseline(
-        classifier_baseline,
-        &train_data,
-        &val_data,
-        baseline_config,
-    );
+    let result_baseline =
+        train_baseline(classifier_baseline, &train_data, &val_data, baseline_config);
 
     println!("Done!");
     println!("\nBaseline Results:");
-    println!("  Final Train Accuracy: {:.2}%", result_baseline.final_train_accuracy * 100.0);
-    println!("  Final Val Accuracy: {:.2}%", result_baseline.final_val_accuracy * 100.0);
-    println!("  Converged at Epoch: {:?}", result_baseline.converged_epoch);
+    println!(
+        "  Final Train Accuracy: {:.2}%",
+        result_baseline.final_train_accuracy * 100.0
+    );
+    println!(
+        "  Final Val Accuracy: {:.2}%",
+        result_baseline.final_val_accuracy * 100.0
+    );
+    println!(
+        "  Converged at Epoch: {:?}",
+        result_baseline.converged_epoch
+    );
     println!("  Total Time: {}ms\n", result_baseline.total_elapsed_ms);
 
     // ========================================================================
     // Experiment 2: Training with Dream Pool Retrieval
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Experiment 2: Training with Dream Pool Retrieval            │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let dream_config = TrainingConfig {
         num_epochs: 100,
         batch_size: 32,
         learning_rate: 0.01,
         lr_decay: 0.98,
         use_dream_pool: true,
         num_dreams_retrieve: 3,
         retrieval_mode: RetrievalMode::Hard,
         seed: 42,
     };
 
     let classifier_dream = MLPClassifier::new(classifier_config.clone());
     let mut pool = SimpleDreamPool::new(pool_config);
     let mut solver = ChromaticNativeSolver::default();
 
     print!("Training with Dream Pool... ");
     std::io::stdout().flush().unwrap();
 
     let result_dream = train_with_dreams(
         classifier_dream,
         &train_data,
         &val_data,
         dream_config,
         Some(&mut pool),
         Some(&mut solver),
     );
 
     println!("Done!");
     println!("\nDream Pool Results:");
-    println!("  Final Train Accuracy: {:.2}%", result_dream.final_train_accuracy * 100.0);
-    println!("  Final Val Accuracy: {:.2}%", result_dream.final_val_accuracy * 100.0);
+    println!(
+        "  Final Train Accuracy: {:.2}%",
+        result_dream.final_train_accuracy * 100.0
+    );
+    println!(
+        "  Final Val Accuracy: {:.2}%",
+        result_dream.final_val_accuracy * 100.0
+    );
     println!("  Converged at Epoch: {:?}", result_dream.converged_epoch);
     println!("  Total Time: {}ms", result_dream.total_elapsed_ms);
 
     let pool_stats = pool.stats();
     println!("  Pool Size: {}", pool_stats.count);
     println!("  Pool Mean Coherence: {:.4}\n", pool_stats.mean_coherence);
 
     // ========================================================================
     // Comparison
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Comparison                                                   │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let accuracy_improvement = result_dream.final_val_accuracy - result_baseline.final_val_accuracy;
     let improvement_pct = (accuracy_improvement / result_baseline.final_val_accuracy) * 100.0;
 
     println!("Final Validation Accuracy:");
-    println!("  Baseline: {:.2}%", result_baseline.final_val_accuracy * 100.0);
-    println!("  Dream Pool: {:.2}%", result_dream.final_val_accuracy * 100.0);
-    println!("  Improvement: {:.4} ({:.2}%)\n", accuracy_improvement, improvement_pct);
+    println!(
+        "  Baseline: {:.2}%",
+        result_baseline.final_val_accuracy * 100.0
+    );
+    println!(
+        "  Dream Pool: {:.2}%",
+        result_dream.final_val_accuracy * 100.0
+    );
+    println!(
+        "  Improvement: {:.4} ({:.2}%)\n",
+        accuracy_improvement, improvement_pct
+    );
 
-    let convergence_comparison = match (result_baseline.converged_epoch, result_dream.converged_epoch) {
+    let convergence_comparison = match (
+        result_baseline.converged_epoch,
+        result_dream.converged_epoch,
+    ) {
         (Some(baseline_epoch), Some(dream_epoch)) => {
             println!("Convergence (95% accuracy):");
             println!("  Baseline: epoch {}", baseline_epoch);
             println!("  Dream Pool: epoch {}", dream_epoch);
             if dream_epoch < baseline_epoch {
-                println!("  Dream Pool converged {} epochs FASTER ✓\n", baseline_epoch - dream_epoch);
+                println!(
+                    "  Dream Pool converged {} epochs FASTER ✓\n",
+                    baseline_epoch - dream_epoch
+                );
             } else if dream_epoch > baseline_epoch {
-                println!("  Dream Pool converged {} epochs SLOWER ⚠️\n", dream_epoch - baseline_epoch);
+                println!(
+                    "  Dream Pool converged {} epochs SLOWER ⚠️\n",
+                    dream_epoch - baseline_epoch
+                );
             } else {
                 println!("  Same convergence speed\n");
             }
             Some((baseline_epoch, dream_epoch))
         }
         (Some(baseline_epoch), None) => {
             println!("Convergence:");
             println!("  Baseline: epoch {} ✓", baseline_epoch);
             println!("  Dream Pool: Did not converge ⚠️\n");
             None
         }
         (None, Some(dream_epoch)) => {
             println!("Convergence:");
             println!("  Baseline: Did not converge");
             println!("  Dream Pool: epoch {} ✓\n", dream_epoch);
             None
         }
         (None, None) => {
             println!("Convergence:");
             println!("  Neither model converged to 95% accuracy\n");
             None
         }
     };
 
     // ========================================================================
     // Save Results
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Saving Results                                               │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     std::fs::create_dir_all("logs").expect("Failed to create logs directory");
 
     // Save JSON results
-    let json_baseline = serde_json::to_string_pretty(&result_baseline)
-        .expect("Failed to serialize baseline");
-    let mut file_baseline = File::create("logs/learner_baseline.json")
-        .expect("Failed to create file");
-    file_baseline.write_all(json_baseline.as_bytes()).expect("Failed to write");
+    let json_baseline =
+        serde_json::to_string_pretty(&result_baseline).expect("Failed to serialize baseline");
+    let mut file_baseline =
+        File::create("logs/learner_baseline.json").expect("Failed to create file");
+    file_baseline
+        .write_all(json_baseline.as_bytes())
+        .expect("Failed to write");
     println!("✓ Baseline results: logs/learner_baseline.json");
 
-    let json_dream = serde_json::to_string_pretty(&result_dream)
-        .expect("Failed to serialize dream pool");
-    let mut file_dream = File::create("logs/learner_dream_pool.json")
-        .expect("Failed to create file");
-    file_dream.write_all(json_dream.as_bytes()).expect("Failed to write");
+    let json_dream =
+        serde_json::to_string_pretty(&result_dream).expect("Failed to serialize dream pool");
+    let mut file_dream =
+        File::create("logs/learner_dream_pool.json").expect("Failed to create file");
+    file_dream
+        .write_all(json_dream.as_bytes())
+        .expect("Failed to write");
     println!("✓ Dream Pool results: logs/learner_dream_pool.json");
 
     // Generate CSV for plotting
     let mut csv = String::from("epoch,baseline_train_acc,baseline_val_acc,dream_train_acc,dream_val_acc,baseline_loss,dream_loss\n");
-    for (baseline, dream) in result_baseline.epoch_metrics.iter().zip(result_dream.epoch_metrics.iter()) {
+    for (baseline, dream) in result_baseline
+        .epoch_metrics
+        .iter()
+        .zip(result_dream.epoch_metrics.iter())
+    {
         csv.push_str(&format!(
             "{},{},{},{},{},{},{}\n",
             baseline.epoch,
             baseline.train_accuracy,
             baseline.val_accuracy,
             dream.train_accuracy,
             dream.val_accuracy,
             baseline.train_loss,
             dream.train_loss,
         ));
     }
 
-    let mut csv_file = File::create("logs/learner_comparison.csv")
-        .expect("Failed to create CSV file");
-    csv_file.write_all(csv.as_bytes()).expect("Failed to write CSV");
+    let mut csv_file =
+        File::create("logs/learner_comparison.csv").expect("Failed to create CSV file");
+    csv_file
+        .write_all(csv.as_bytes())
+        .expect("Failed to write CSV");
     println!("✓ Metrics CSV: logs/learner_comparison.csv");
 
     // ========================================================================
     // Validation Assessment
     // ========================================================================
     println!("\n┌──────────────────────────────────────────────────────────────┐");
     println!("│ Validation Assessment                                        │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let training_works = result_baseline.final_val_accuracy >= 0.90;
     let retrieval_helps = result_dream.final_val_accuracy > result_baseline.final_val_accuracy;
     let converged_faster = convergence_comparison
         .map(|(baseline, dream)| dream < baseline)
         .unwrap_or(false);
 
     println!("\n✓ VALIDATION CHECKLIST:\n");
 
     print!("  [");
     if training_works {
         print!("✓");
     } else {
         print!("✗");
     }
     println!("] Training achieves >90% accuracy");
-    println!("      Baseline: {:.2}%", result_baseline.final_val_accuracy * 100.0);
+    println!(
+        "      Baseline: {:.2}%",
+        result_baseline.final_val_accuracy * 100.0
+    );
 
     print!("  [");
     if retrieval_helps {
         print!("✓");
     } else {
         print!("✗");
     }
     println!("] Dream Pool improves final accuracy");
     println!("      Improvement: {:.2}%", improvement_pct);
 
     print!("  [");
     if converged_faster {
         print!("✓");
     } else {
         print!("✗");
     }
     println!("] Dream Pool accelerates convergence");
     if let Some((baseline, dream)) = convergence_comparison {
-        println!("      Baseline: {} epochs, Dream Pool: {} epochs", baseline, dream);
+        println!(
+            "      Baseline: {} epochs, Dream Pool: {} epochs",
+            baseline, dream
+        );
     }
 
     println!("\n╔══════════════════════════════════════════════════════════════╗");
     if training_works && (retrieval_helps || converged_faster) {
         println!("║  ✓ SUCCESS: Learner validated & Dream Pool hypothesis holds ║");
         println!("╚══════════════════════════════════════════════════════════════╝");
         println!("\n🎉 The Minimal Viable Learner is working!");
         println!("\nNext Steps:");
         println!("  1. ✓ Training algorithm works (gradient descent + cross-entropy)");
         println!("  2. ✓ Dream Pool retrieval integration functional");
         println!("  3. → Ready for full LEARNER MANIFEST v1.0 implementation:");
         println!("       - FFT feature extraction");
         println!("       - Feedback collection (Δloss tracking)");
         println!("       - Bias profile synthesis");
         println!("       - Advanced retrieval modes (euclidean, mixed)");
     } else if training_works && !retrieval_helps {
         println!("║  ⚠️  PARTIAL: Training works, but Dream Pool shows no benefit║");
         println!("╚══════════════════════════════════════════════════════════════╝");
         println!("\nTraining algorithm is functional, but retrieval hypothesis not validated.");
         println!("\nPossible causes:");
         println!("  - Coherence threshold too high (pool too sparse)");
         println!("  - Dataset too simple (training converges quickly anyway)");
         println!("  - Retrieval strategy needs tuning");
         println!("\nRecommendation: Investigate Dream Pool parameters or try harder task.");
     } else {
diff --git a/examples/phase_3b_validation.rs b/examples/phase_3b_validation.rs
index 90029e6642187f7d0ba43c9ced810a89b0c67af8..5402073f7e45df9e89847efe3d589ff21ea9e51c 100644
--- a/examples/phase_3b_validation.rs
+++ b/examples/phase_3b_validation.rs
@@ -1,47 +1,49 @@
 //! Phase 3B Validation - Test refined Dream Pool improvements
 //!
 //! This experiment validates whether Phase 3B refinements improve training:
 //! - Class-aware retrieval
 //! - Diversity enforcement (MMR)
 //! - Spectral feature extraction
 //! - ΔLoss utility scoring
 //! - Bias profile synthesis
 //!
 //! **3-Way Comparison:**
 //! 1. Baseline: No Dream Pool
 //! 2. Phase 3A: Original Dream Pool (cosine similarity only)
 //! 3. Phase 3B: Refined Dream Pool (all enhancements)
 
 use chromatic_cognition_core::data::{ColorClass, ColorDataset, ColorSample, DatasetConfig};
 use chromatic_cognition_core::dream::simple_pool::PoolConfig;
 use chromatic_cognition_core::dream::{BiasProfile, RetrievalMode, SimpleDreamPool};
 use chromatic_cognition_core::learner::feedback::{FeedbackRecord, UtilityAggregator};
-use chromatic_cognition_core::learner::training::{EpochMetrics, TrainingConfig, TrainingResult, train_with_dreams};
+use chromatic_cognition_core::learner::training::{
+    train_with_dreams, EpochMetrics, TrainingConfig, TrainingResult,
+};
 use chromatic_cognition_core::learner::{ClassifierConfig, MLPClassifier};
-use chromatic_cognition_core::tensor::operations::mix;
 use chromatic_cognition_core::solver::native::ChromaticNativeSolver;
+use chromatic_cognition_core::tensor::operations::mix;
 use chromatic_cognition_core::Solver;
 
 fn main() {
     println!("╔══════════════════════════════════════════════════════════════╗");
     println!("║  Phase 3B Validation - Refined Dream Pool Evaluation       ║");
     println!("║  Testing: Class-aware + Diversity + Utility + Bias         ║");
     println!("╚══════════════════════════════════════════════════════════════╝\n");
 
     // Configuration
     let dataset_config = DatasetConfig {
         samples_per_class: 100,
         tensor_size: (16, 16, 4),
         noise_level: 0.1,
         seed: 42,
     };
 
     let base_training = TrainingConfig {
         num_epochs: 100,
         batch_size: 32,
         learning_rate: 0.01,
         lr_decay: 0.98,
         use_dream_pool: false,
         num_dreams_retrieve: 3,
         retrieval_mode: RetrievalMode::Hard,
         seed: 42,
@@ -76,209 +78,233 @@ fn main() {
     // ========================================================================
     // Experiment 1: Baseline (No Dream Pool)
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Experiment 1: Baseline (No Dream Pool)                     │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let baseline_config = TrainingConfig {
         use_dream_pool: false,
         num_dreams_retrieve: 0,
         retrieval_mode: RetrievalMode::Hard,
         ..base_training.clone()
     };
 
     let classifier = MLPClassifier::new(classifier_config.clone());
     let result_baseline = train_with_dreams(
         classifier,
         &train_samples,
         &val_samples,
         baseline_config.clone(),
         None::<&mut SimpleDreamPool>,
         None::<&mut ChromaticNativeSolver>,
     );
 
     println!("\nBaseline Results:");
-    println!("  Final Train Accuracy: {:.2}%", result_baseline.final_train_accuracy * 100.0);
-    println!("  Final Val Accuracy: {:.2}%", result_baseline.final_val_accuracy * 100.0);
-    println!("  Converged at Epoch: {:?}", result_baseline.converged_epoch);
+    println!(
+        "  Final Train Accuracy: {:.2}%",
+        result_baseline.final_train_accuracy * 100.0
+    );
+    println!(
+        "  Final Val Accuracy: {:.2}%",
+        result_baseline.final_val_accuracy * 100.0
+    );
+    println!(
+        "  Converged at Epoch: {:?}",
+        result_baseline.converged_epoch
+    );
     println!("  Total Time: {}ms\n", result_baseline.total_elapsed_ms);
 
     // ========================================================================
     // Experiment 2: Phase 3A (Original Dream Pool)
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Experiment 2: Phase 3A (Original Dream Pool)               │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let mut pool_3a = SimpleDreamPool::new(pool_config.clone());
     let mut solver_3a = ChromaticNativeSolver::default();
 
     println!("Populating Dream Pool...");
     for sample in &train_samples {
         if let Ok(result) = solver_3a.evaluate(&sample.tensor, false) {
             pool_3a.add_if_coherent(sample.tensor.clone(), result);
         }
     }
     println!("  Pool size: {}\n", pool_3a.len());
 
     let phase3a_config = TrainingConfig {
         use_dream_pool: true,
         retrieval_mode: RetrievalMode::Hard,
         ..base_training.clone()
     };
 
     let classifier = MLPClassifier::new(classifier_config.clone());
     let result_3a = train_with_dreams(
         classifier,
         &train_samples,
         &val_samples,
         phase3a_config.clone(),
         Some(&mut pool_3a),
         Some(&mut solver_3a),
     );
 
     println!("\nPhase 3A Results:");
-    println!("  Final Train Accuracy: {:.2}%", result_3a.final_train_accuracy * 100.0);
-    println!("  Final Val Accuracy: {:.2}%", result_3a.final_val_accuracy * 100.0);
+    println!(
+        "  Final Train Accuracy: {:.2}%",
+        result_3a.final_train_accuracy * 100.0
+    );
+    println!(
+        "  Final Val Accuracy: {:.2}%",
+        result_3a.final_val_accuracy * 100.0
+    );
     println!("  Converged at Epoch: {:?}", result_3a.converged_epoch);
     println!("  Total Time: {}ms\n", result_3a.total_elapsed_ms);
 
     // ========================================================================
     // Experiment 3: Phase 3B (Refined Dream Pool)
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Experiment 3: Phase 3B (Refined Dream Pool)                │");
     println!("│ Features: Class-aware + Diversity + Utility + Bias         │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let mut pool_3b = SimpleDreamPool::new(pool_config.clone());
     let mut solver_3b = ChromaticNativeSolver::default();
     let mut aggregator = UtilityAggregator::new();
 
     println!("Populating Dream Pool with class labels...");
     for sample in &train_samples {
         if let Ok(result) = solver_3b.evaluate(&sample.tensor, false) {
             pool_3b.add_with_class(sample.tensor.clone(), result, sample.label);
         }
     }
     println!("  Pool size: {}\n", pool_3b.len());
 
     let phase3b_config = TrainingConfig {
         use_dream_pool: true,
         retrieval_mode: RetrievalMode::Hybrid,
         ..base_training.clone()
     };
 
     let classifier = MLPClassifier::new(classifier_config.clone());
 
     println!("Training with Phase 3B enhancements...");
     let result_3b = train_with_phase_3b(
         classifier,
         &train_samples,
         &val_samples,
         phase3b_config.clone(),
         &mut pool_3b,
         &mut solver_3b,
         &mut aggregator,
     );
 
     println!("\nPhase 3B Results:");
-    println!("  Final Train Accuracy: {:.2}%", result_3b.final_train_accuracy * 100.0);
-    println!("  Final Val Accuracy: {:.2}%", result_3b.final_val_accuracy * 100.0);
+    println!(
+        "  Final Train Accuracy: {:.2}%",
+        result_3b.final_train_accuracy * 100.0
+    );
+    println!(
+        "  Final Val Accuracy: {:.2}%",
+        result_3b.final_val_accuracy * 100.0
+    );
     println!("  Converged at Epoch: {:?}", result_3b.converged_epoch);
     println!("  Total Time: {}ms", result_3b.total_elapsed_ms);
     println!("  Feedback Records: {}", aggregator.len());
     println!("  Mean Utility: {:.3}\n", aggregator.mean_utility());
 
     // ========================================================================
     // Synthesize and Save Bias Profile
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Bias Profile Synthesis                                      │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let bias_profile = BiasProfile::from_aggregator(&aggregator, 0.1);
 
     println!("\nClass Biases:");
     for class in [ColorClass::Red, ColorClass::Green, ColorClass::Blue] {
         if let Some(stats) = aggregator.class_stats(class) {
             println!(
                 "  {:?}: mean_utility={:.3}, helpful={}, harmful={}",
-                class,
-                stats.mean_utility,
-                stats.helpful_count,
-                stats.harmful_count
+                class, stats.mean_utility, stats.helpful_count, stats.harmful_count
             );
         }
     }
 
     println!("\nPreferred Classes:");
     for class_name in bias_profile.preferred_classes() {
         println!("  • {}", class_name);
     }
 
     std::fs::create_dir_all("logs").expect("Failed to create logs directory");
     bias_profile
         .save_to_json("logs/phase_3b_bias_profile.json")
         .expect("Failed to save bias profile");
     println!("\n✓ Bias profile saved to logs/phase_3b_bias_profile.json");
 
     // ========================================================================
     // Final Comparison
     // ========================================================================
     println!("\n┌──────────────────────────────────────────────────────────────┐");
     println!("│ Final Comparison                                             │");
     println!("└──────────────────────────────────────────────────────────────┘\n");
 
     println!("| Metric              | Baseline | Phase 3A | Phase 3B | Winner   |");
     println!("|---------------------|----------|----------|----------|----------|");
 
     let val_accs = [
         result_baseline.final_val_accuracy,
         result_3a.final_val_accuracy,
         result_3b.final_val_accuracy,
     ];
     let best_acc_idx = val_accs
         .iter()
         .enumerate()
         .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
         .map(|(i, _)| i)
         .unwrap();
 
     println!(
         "| Val Accuracy        | {:.2}%   | {:.2}%   | {:.2}%   | {}      |",
         val_accs[0] * 100.0,
         val_accs[1] * 100.0,
         val_accs[2] * 100.0,
         ["Baseline", "Phase 3A", "Phase 3B"][best_acc_idx]
     );
 
     let epochs = [
-        result_baseline.converged_epoch.unwrap_or(base_training.num_epochs),
-        result_3a.converged_epoch.unwrap_or(base_training.num_epochs),
-        result_3b.converged_epoch.unwrap_or(base_training.num_epochs),
+        result_baseline
+            .converged_epoch
+            .unwrap_or(base_training.num_epochs),
+        result_3a
+            .converged_epoch
+            .unwrap_or(base_training.num_epochs),
+        result_3b
+            .converged_epoch
+            .unwrap_or(base_training.num_epochs),
     ];
     let best_epoch_idx = epochs
         .iter()
         .enumerate()
         .min_by(|(_, a), (_, b)| a.cmp(b))
         .map(|(i, _)| i)
         .unwrap();
 
     println!(
         "| Convergence Epoch   | {}       | {}       | {}       | {}      |",
         epochs[0],
         epochs[1],
         epochs[2],
         ["Baseline", "Phase 3A", "Phase 3B"][best_epoch_idx]
     );
 
     println!("\n┌──────────────────────────────────────────────────────────────┐");
     println!("│ Conclusion                                                   │");
     println!("└──────────────────────────────────────────────────────────────┘\n");
 
     if best_acc_idx == 2 && best_epoch_idx == 2 {
         println!("✅ SUCCESS: Phase 3B outperforms both Baseline and Phase 3A!");
         println!("   Refinements (class-aware + diversity + utility) are effective.\n");
     } else if best_acc_idx == 2 || best_epoch_idx == 2 {
         println!("⚠️  PARTIAL: Phase 3B shows improvements in some metrics.");
@@ -306,51 +332,55 @@ fn train_with_phase_3b(
     let mut current_lr = config.learning_rate;
     let start = Instant::now();
 
     let mut epoch_metrics = Vec::new();
     let mut best_val_accuracy = 0.0;
     let mut converged_epoch = None;
     let mut final_train_accuracy = 0.0;
 
     for epoch in 0..config.num_epochs {
         let epoch_start = Instant::now();
         let mut epoch_loss = 0.0f32;
         let mut batch_count = 0usize;
         let mut epoch_feedback = 0usize;
         let mut dreams_used = 0usize;
 
         for batch_start in (0..train_data.len()).step_by(config.batch_size.max(1)) {
             let batch_end = (batch_start + config.batch_size).min(train_data.len());
             let batch = &train_data[batch_start..batch_end];
 
             let mut batch_tensors = Vec::with_capacity(batch.len());
             let mut batch_labels = Vec::with_capacity(batch.len());
 
             for sample in batch {
                 let mut tensor = sample.tensor.clone();
                 let signature = tensor.mean_rgb();
-                let retrieved = pool.retrieve_similar_class(&signature, sample.label, config.num_dreams_retrieve);
+                let retrieved = pool.retrieve_similar_class(
+                    &signature,
+                    sample.label,
+                    config.num_dreams_retrieve,
+                );
 
                 if let Some(entry) = retrieved.first() {
                     tensor = mix(&tensor, &entry.tensor);
                     dreams_used += 1;
                 }
 
                 batch_tensors.push(tensor);
                 batch_labels.push(sample.label);
             }
 
             let loss_before = classifier.compute_loss(&batch_tensors, &batch_labels).0;
             let (loss, gradients) = classifier.compute_loss(&batch_tensors, &batch_labels);
             classifier.update_weights(&gradients, current_lr);
             let loss_after = classifier.compute_loss(&batch_tensors, &batch_labels).0;
 
             epoch_loss += loss;
             batch_count += 1;
 
             if epoch > 0 && batch_start % 256 == 0 {
                 if let Some(sample) = batch.first() {
                     let record = FeedbackRecord::new(
                         sample.tensor.mean_rgb(),
                         Some(sample.label),
                         loss_before,
                         loss_after,
@@ -427,31 +457,27 @@ fn train_with_phase_3b(
                 if let Ok(result) = solver.evaluate(&sample.tensor, false) {
                     pool.add_with_class(sample.tensor.clone(), result, sample.label);
                 }
             }
         }
     }
 
     TrainingResult {
         config,
         epoch_metrics,
         final_train_accuracy,
         final_val_accuracy: best_val_accuracy,
         total_elapsed_ms: start.elapsed().as_millis(),
         converged_epoch,
     }
 }
 
 /// Helper: deterministic subset selection for periodic pool refresh.
 fn batch_subset<'a>(samples: &'a [ColorSample], epoch: usize) -> Vec<&'a ColorSample> {
     if samples.is_empty() {
         return Vec::new();
     }
 
     let step = (samples.len() / 16).max(1);
     let offset = epoch % step;
-    samples
-        .iter()
-        .skip(offset)
-        .step_by(step)
-        .collect()
+    samples.iter().skip(offset).step_by(step).collect()
 }
diff --git a/examples/solver_demo.rs b/examples/solver_demo.rs
index 2884da1f2a7039f8da2d38ad2ef0b93d7270751e..20fe15c80bb17cd13fe8e9530bcd78cdd40787a2 100644
--- a/examples/solver_demo.rs
+++ b/examples/solver_demo.rs
@@ -1,108 +1,140 @@
 /// Demonstration of the chromatic field solver
 ///
 /// This example shows how to:
 /// 1. Create a chromatic tensor
 /// 2. Evaluate it with the native solver
 /// 3. Interpret energy, coherence, and violation metrics
 /// 4. Compute gradients for optimization
-
-use chromatic_cognition_core::{ChromaticTensor, ChromaticNativeSolver, Solver};
+use chromatic_cognition_core::{ChromaticNativeSolver, ChromaticTensor, Solver};
 
 fn main() {
     println!("=== Chromatic Field Solver Demo ===\n");
 
     // Create a solver with default parameters
     let mut solver = ChromaticNativeSolver::new();
     println!("Solver: {}", solver.name());
     println!("Parameters:");
     println!("  lambda_tv (total variation): {}", solver.lambda_tv);
     println!("  lambda_sat (saturation penalty): {}", solver.lambda_sat);
     println!("  target_saturation: {}", solver.target_saturation);
-    println!("  discontinuity_threshold: {}\n", solver.discontinuity_threshold);
+    println!(
+        "  discontinuity_threshold: {}\n",
+        solver.discontinuity_threshold
+    );
 
     // Example 1: Smooth random field
     println!("--- Example 1: Smooth Random Field ---");
     let smooth_field = ChromaticTensor::from_seed(42, 8, 8, 2);
     let result = solver.evaluate(&smooth_field, false).expect("eval failed");
-    println!("Energy: {:.4} (total variation + saturation penalty)", result.energy);
-    println!("Coherence: {:.4} (0-1, higher = more harmonious)", result.coherence);
-    println!("Violation: {:.4} (0-1, lower = fewer constraint violations)\n", result.violation);
+    println!(
+        "Energy: {:.4} (total variation + saturation penalty)",
+        result.energy
+    );
+    println!(
+        "Coherence: {:.4} (0-1, higher = more harmonious)",
+        result.coherence
+    );
+    println!(
+        "Violation: {:.4} (0-1, lower = fewer constraint violations)\n",
+        result.violation
+    );
 
     // Example 2: High contrast field (checkerboard pattern)
     println!("--- Example 2: High Contrast Field ---");
     let mut colors = ndarray::Array4::zeros((4, 4, 1, 3));
     for r in 0..4 {
         for c in 0..4 {
             let is_black_square = (r + c) % 2 == 0;
             if is_black_square {
                 colors[[r, c, 0, 0]] = 0.0; // Black
                 colors[[r, c, 0, 1]] = 0.0;
                 colors[[r, c, 0, 2]] = 0.0;
             } else {
                 colors[[r, c, 0, 0]] = 1.0; // White
                 colors[[r, c, 0, 1]] = 1.0;
                 colors[[r, c, 0, 2]] = 1.0;
             }
         }
     }
     let certainty = ndarray::Array3::ones((4, 4, 1));
     let checkerboard = ChromaticTensor::from_arrays(colors, certainty);
 
     let result = solver.evaluate(&checkerboard, false).expect("eval failed");
-    println!("Energy: {:.4} (high due to sharp transitions)", result.energy);
-    println!("Coherence: {:.4} (low due to extreme black/white)", result.coherence);
-    println!("Violation: {:.4} (low, colors in gamut)\n", result.violation);
+    println!(
+        "Energy: {:.4} (high due to sharp transitions)",
+        result.energy
+    );
+    println!(
+        "Coherence: {:.4} (low due to extreme black/white)",
+        result.coherence
+    );
+    println!(
+        "Violation: {:.4} (low, colors in gamut)\n",
+        result.violation
+    );
 
     // Example 3: Pure colors (high saturation)
     println!("--- Example 3: Pure RGB Colors ---");
     let mut pure_colors = ndarray::Array4::zeros((3, 1, 1, 3));
     pure_colors[[0, 0, 0, 0]] = 1.0; // Pure red
     pure_colors[[1, 0, 0, 1]] = 1.0; // Pure green
     pure_colors[[2, 0, 0, 2]] = 1.0; // Pure blue
     let certainty = ndarray::Array3::ones((3, 1, 1));
     let pure_field = ChromaticTensor::from_arrays(pure_colors, certainty);
 
     let result = solver.evaluate(&pure_field, false).expect("eval failed");
-    println!("Energy: {:.4} (high saturation deviation from target)", result.energy);
-    println!("Coherence: {:.4} (complementary colors present)", result.coherence);
-    println!("Violation: {:.4} (high saturation flagged)\n", result.violation);
+    println!(
+        "Energy: {:.4} (high saturation deviation from target)",
+        result.energy
+    );
+    println!(
+        "Coherence: {:.4} (complementary colors present)",
+        result.coherence
+    );
+    println!(
+        "Violation: {:.4} (high saturation flagged)\n",
+        result.violation
+    );
 
     // Example 4: Out-of-gamut colors (constraint violation)
     println!("--- Example 4: Out-of-Gamut Colors ---");
     let mut bad_colors = ndarray::Array4::zeros((2, 2, 1, 3));
     bad_colors[[0, 0, 0, 0]] = 1.5; // Out of gamut (> 1.0)
     bad_colors[[0, 1, 0, 1]] = -0.2; // Out of gamut (< 0.0)
     bad_colors[[1, 0, 0, 2]] = 0.5; // Valid
     bad_colors[[1, 1, 0, 0]] = 0.8; // Valid
     let certainty = ndarray::Array3::ones((2, 2, 1));
     let bad_field = ChromaticTensor::from_arrays(bad_colors, certainty);
 
     let result = solver.evaluate(&bad_field, false).expect("eval failed");
     println!("Energy: {:.4}", result.energy);
     println!("Coherence: {:.4}", result.coherence);
-    println!("Violation: {:.4} (HIGH - out-of-gamut colors detected)\n", result.violation);
+    println!(
+        "Violation: {:.4} (HIGH - out-of-gamut colors detected)\n",
+        result.violation
+    );
 
     // Example 5: Gradient computation
     println!("--- Example 5: Gradient Computation ---");
     let test_field = ChromaticTensor::from_seed(123, 4, 4, 1);
     let result_with_grad = solver.evaluate(&test_field, true).expect("eval failed");
 
     if let Some(grad) = &result_with_grad.grad {
         println!("Gradient computed: {} values (4×4×1×3)", grad.len());
         println!("First 9 gradient components:");
         for i in 0..9.min(grad.len()) {
             println!("  grad[{}] = {:.4}", i, grad[i]);
         }
         println!("\nGradient can be used for optimization (gradient descent)");
     }
 
     println!("\n=== Interpretation Guide ===");
     println!("Energy: Total field \"cost\" (lower is smoother/better)");
     println!("  - Combines spatial smoothness (total variation)");
     println!("  - And deviation from target saturation");
     println!();
     println!("Coherence: Color harmony score (0-1, higher is better)");
     println!("  - Measures complementary balance (red-cyan, green-magenta, etc.)");
     println!("  - Measures hue consistency (similar hues = more coherent)");
     println!();
     println!("Violation: Constraint violation score (0-1, lower is better)");
diff --git a/examples/train_color_classifier.rs b/examples/train_color_classifier.rs
index 92028a383f2ffa2a65bc27b4056fe73e6c5b0833..0c02027bfd0c85ee65d8a103bba9cc7be3434f80 100644
--- a/examples/train_color_classifier.rs
+++ b/examples/train_color_classifier.rs
@@ -1,31 +1,33 @@
 //! Training example for chromatic neural network on color classification.
 //!
 //! This example trains a chromatic neural network to classify patterns
 //! into primary colors (red, green, blue).
 
-use chromatic_cognition_core::data::{generate_primary_color_dataset, shuffle_dataset, split_dataset};
+use chromatic_cognition_core::data::{
+    generate_primary_color_dataset, shuffle_dataset, split_dataset,
+};
 use chromatic_cognition_core::neural::{ChromaticNetwork, SGDOptimizer};
 use chromatic_cognition_core::tensor::gradient::GradientLayer;
 use std::path::PathBuf;
 
 fn main() -> Result<(), Box<dyn std::error::Error>> {
     println!("🎨 Chromatic Neural Network - Color Classification");
     println!("==================================================\n");
 
     // Hyperparameters
     let samples_per_class = 50;
     let tensor_size = (16, 16, 4); // Small for fast training
     let num_classes = 3; // Red, Green, Blue
     let epochs = 20;
     let learning_rate = 0.05;
     let momentum = 0.9;
     let weight_decay = 0.0001;
 
     println!("Configuration:");
     println!("  Samples per class: {}", samples_per_class);
     println!("  Tensor size: {:?}", tensor_size);
     println!("  Epochs: {}", epochs);
     println!("  Learning rate: {}", learning_rate);
     println!("  Momentum: {}", momentum);
     println!("  Weight decay: {}", weight_decay);
     println!();
@@ -84,69 +86,74 @@ fn main() -> Result<(), Box<dyn std::error::Error>> {
             epochs,
             train_loss,
             train_acc * 100.0,
             val_loss,
             val_acc * 100.0
         );
     }
 
     println!("\n✅ Training complete!");
     println!();
 
     // Final evaluation
     println!("📈 Final Evaluation:");
     let val_inputs: Vec<_> = val_data.iter().map(|p| p.tensor.clone()).collect();
     let val_labels: Vec<_> = val_data.iter().map(|p| p.label).collect();
     let (final_loss, final_acc) = network.evaluate(&val_inputs, &val_labels);
 
     println!("  Validation Loss: {:.4}", final_loss);
     println!("  Validation Accuracy: {:.2}%", final_acc * 100.0);
     println!();
 
     // Per-class accuracy
     println!("📊 Per-Class Performance:");
     let class_names = ["Red", "Green", "Blue"];
     for class in 0..num_classes {
-        let class_samples: Vec<_> = val_data
-            .iter()
-            .filter(|p| p.label == class)
-            .collect();
+        let class_samples: Vec<_> = val_data.iter().filter(|p| p.label == class).collect();
 
         if !class_samples.is_empty() {
             let class_inputs: Vec<_> = class_samples.iter().map(|p| p.tensor.clone()).collect();
             let class_labels: Vec<_> = class_samples.iter().map(|p| p.label).collect();
             let (_loss, acc) = network.evaluate(&class_inputs, &class_labels);
 
             println!("  {}: {:.2}%", class_names[class], acc * 100.0);
         }
     }
     println!();
 
     // Visualize some predictions
     println!("🖼️  Visualizing sample predictions...");
     std::fs::create_dir_all("out/predictions")?;
 
     for (i, pattern) in val_data.iter().take(9).enumerate() {
         let output = network.forward(&pattern.tensor);
         let gradient = GradientLayer::from_tensor(&output);
 
         let filename = format!("out/predictions/sample_{}_label_{}.png", i, pattern.label);
         gradient.to_png(PathBuf::from(&filename))?;
 
         let stats = output.statistics();
-        let predicted_class = if stats.mean_rgb[0] > stats.mean_rgb[1] && stats.mean_rgb[0] > stats.mean_rgb[2] {
-            0
-        } else if stats.mean_rgb[1] > stats.mean_rgb[2] {
-            1
+        let predicted_class =
+            if stats.mean_rgb[0] > stats.mean_rgb[1] && stats.mean_rgb[0] > stats.mean_rgb[2] {
+                0
+            } else if stats.mean_rgb[1] > stats.mean_rgb[2] {
+                1
+            } else {
+                2
+            };
+
+        let correct = if predicted_class == pattern.label {
+            "✓"
         } else {
-            2
+            "✗"
         };
-
-        let correct = if predicted_class == pattern.label { "✓" } else { "✗" };
-        println!("  {} Sample {}: True={}, Pred={}", correct, i, class_names[pattern.label], class_names[predicted_class]);
+        println!(
+            "  {} Sample {}: True={}, Pred={}",
+            correct, i, class_names[pattern.label], class_names[predicted_class]
+        );
     }
     println!();
 
     println!("✨ Done! Check out/predictions/ for visualizations.");
 
     Ok(())
 }
diff --git a/src/data/mod.rs b/src/data/mod.rs
index fb695a118ba91f9dafa390c13be5777cc0b3b052..8c16a3ef3d826401dfe1cd6bf73066a765643933 100644
--- a/src/data/mod.rs
+++ b/src/data/mod.rs
@@ -1,7 +1,7 @@
 //! Dataset and pattern generation for chromatic neural networks.
 
 pub mod color_dataset;
 pub mod pattern;
 
 pub use color_dataset::{ColorClass, ColorDataset, ColorSample, DatasetConfig};
-pub use pattern::{ColorPattern, generate_primary_color_dataset, shuffle_dataset, split_dataset};
+pub use pattern::{generate_primary_color_dataset, shuffle_dataset, split_dataset, ColorPattern};
diff --git a/src/data/pattern.rs b/src/data/pattern.rs
index ac2661f1088c3e3ce46870f18a519e45a11732a1..a816dc4218963a3451bce536a436ce89ab0609b2 100644
--- a/src/data/pattern.rs
+++ b/src/data/pattern.rs
@@ -35,89 +35,91 @@ pub struct ColorPattern {
 /// ```
 /// use chromatic_cognition_core::data::generate_primary_color_dataset;
 ///
 /// let dataset = generate_primary_color_dataset(100, 16, 16, 4, 42);
 /// assert_eq!(dataset.len(), 300); // 100 per class × 3 classes
 /// ```
 pub fn generate_primary_color_dataset(
     samples_per_class: usize,
     rows: usize,
     cols: usize,
     layers: usize,
     seed: u64,
 ) -> Vec<ColorPattern> {
     let mut dataset = Vec::new();
     let mut rng = rand::rngs::StdRng::seed_from_u64(seed);
 
     let class_names = ["red", "green", "blue"];
     let base_colors = [
         [0.9, 0.1, 0.1], // Red
         [0.1, 0.9, 0.1], // Green
         [0.1, 0.1, 0.9], // Blue
     ];
 
     for class in 0..3 {
         for sample_idx in 0..samples_per_class {
-            let sample_seed = seed
-                .wrapping_add((class * samples_per_class + sample_idx) as u64);
+            let sample_seed = seed.wrapping_add((class * samples_per_class + sample_idx) as u64);
 
             // Start with random tensor
             let mut tensor = ChromaticTensor::from_seed(sample_seed, rows, cols, layers);
 
             // Bias toward class color
             let base_color = base_colors[class];
             let intensity = 0.7 + rng.gen::<f32>() * 0.3; // 0.7 to 1.0
 
             for row in 0..rows {
                 for col in 0..cols {
                     for layer in 0..layers {
                         // Mix with base color
                         let r = tensor.colors[[row, col, layer, 0]];
                         let g = tensor.colors[[row, col, layer, 1]];
                         let b = tensor.colors[[row, col, layer, 2]];
 
                         tensor.colors[[row, col, layer, 0]] =
                             (r * (1.0 - intensity) + base_color[0] * intensity).clamp(0.0, 1.0);
                         tensor.colors[[row, col, layer, 1]] =
                             (g * (1.0 - intensity) + base_color[1] * intensity).clamp(0.0, 1.0);
                         tensor.colors[[row, col, layer, 2]] =
                             (b * (1.0 - intensity) + base_color[2] * intensity).clamp(0.0, 1.0);
 
                         // Add some certainty variation
                         tensor.certainty[[row, col, layer]] = 0.5 + rng.gen::<f32>() * 0.5;
                     }
                 }
             }
 
             // Add noise
             let noise_level = 0.1;
             for row in 0..rows {
                 for col in 0..cols {
                     for layer in 0..layers {
-                        tensor.colors[[row, col, layer, 0]] += (rng.gen::<f32>() - 0.5) * noise_level;
-                        tensor.colors[[row, col, layer, 1]] += (rng.gen::<f32>() - 0.5) * noise_level;
-                        tensor.colors[[row, col, layer, 2]] += (rng.gen::<f32>() - 0.5) * noise_level;
+                        tensor.colors[[row, col, layer, 0]] +=
+                            (rng.gen::<f32>() - 0.5) * noise_level;
+                        tensor.colors[[row, col, layer, 1]] +=
+                            (rng.gen::<f32>() - 0.5) * noise_level;
+                        tensor.colors[[row, col, layer, 2]] +=
+                            (rng.gen::<f32>() - 0.5) * noise_level;
                     }
                 }
             }
 
             // Clamp to valid range
             tensor = tensor.clamp(0.0, 1.0);
 
             dataset.push(ColorPattern {
                 tensor,
                 label: class,
                 description: format!("{} pattern #{}", class_names[class], sample_idx),
             });
         }
     }
 
     dataset
 }
 
 /// Splits a dataset into training and validation sets.
 ///
 /// # Arguments
 ///
 /// * `dataset` - Full dataset
 /// * `train_fraction` - Fraction to use for training (e.g., 0.8)
 ///
@@ -174,28 +176,28 @@ mod tests {
             assert!(stats.mean_rgb[0] > stats.mean_rgb[1]); // R > G
             assert!(stats.mean_rgb[0] > stats.mean_rgb[2]); // R > B
         }
 
         // Green class should have higher green values
         for pattern in dataset.iter().filter(|p| p.label == 1) {
             let stats = pattern.tensor.statistics();
             assert!(stats.mean_rgb[1] > stats.mean_rgb[0]); // G > R
             assert!(stats.mean_rgb[1] > stats.mean_rgb[2]); // G > B
         }
 
         // Blue class should have higher blue values
         for pattern in dataset.iter().filter(|p| p.label == 2) {
             let stats = pattern.tensor.statistics();
             assert!(stats.mean_rgb[2] > stats.mean_rgb[0]); // B > R
             assert!(stats.mean_rgb[2] > stats.mean_rgb[1]); // B > G
         }
     }
 
     #[test]
     fn test_split_dataset() {
         let dataset = generate_primary_color_dataset(10, 4, 4, 2, 42);
         let (train, val) = split_dataset(dataset, 0.8);
 
         assert_eq!(train.len(), 24); // 0.8 * 30
-        assert_eq!(val.len(), 6);    // 0.2 * 30
+        assert_eq!(val.len(), 6); // 0.2 * 30
     }
 }
diff --git a/src/dream/analysis.rs b/src/dream/analysis.rs
index d23da6f83256289ac6fd86d594b920fdd6376d60..980f7e9d3452e36499ff1f1872c689827ac56acb 100644
--- a/src/dream/analysis.rs
+++ b/src/dream/analysis.rs
@@ -1,32 +1,32 @@
 //! Statistical analysis utilities for experiment results
 //!
 //! Provides tools for analyzing and comparing A/B test results,
 //! including statistical significance testing.
 
-use crate::dream::experiment::{ExperimentResult, EpochMetrics};
-use serde::{Serialize, Deserialize};
+use crate::dream::experiment::{EpochMetrics, ExperimentResult};
+use serde::{Deserialize, Serialize};
 
 /// Comparison of two experiment results
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct ExperimentComparison {
     pub control_strategy: String,
     pub test_strategy: String,
     pub accuracy_improvement: f64,
     pub accuracy_improvement_pct: f64,
     pub convergence_epoch_delta: Option<i32>,
     pub mean_coherence_improvement: f64,
     pub mean_energy_improvement: f64,
     pub is_significant: bool,
     pub significance_level: f64,
 }
 
 /// Compute basic statistics for a sequence of values
 #[derive(Debug, Clone, Serialize)]
 pub struct Statistics {
     pub mean: f64,
     pub variance: f64,
     pub std_dev: f64,
     pub min: f64,
     pub max: f64,
     pub count: usize,
 }
@@ -76,102 +76,107 @@ impl Statistics {
 pub fn compare_experiments(
     control: &ExperimentResult,
     test: &ExperimentResult,
     significance_threshold: f64,
 ) -> ExperimentComparison {
     // Accuracy comparison
     let accuracy_improvement = test.final_accuracy - control.final_accuracy;
     let accuracy_improvement_pct = if control.final_accuracy > 0.0 {
         (accuracy_improvement / control.final_accuracy) * 100.0
     } else {
         0.0
     };
 
     // Convergence epoch comparison
     let convergence_epoch_delta = match (control.convergence_epoch, test.convergence_epoch) {
         (Some(a), Some(b)) => Some(a as i32 - b as i32),
         _ => None,
     };
 
     // Mean coherence comparison
     let control_coherence: Vec<f64> = control
         .epoch_metrics
         .iter()
         .map(|m| m.mean_coherence)
         .collect();
-    let test_coherence: Vec<f64> = test.epoch_metrics.iter().map(|m| m.mean_coherence).collect();
+    let test_coherence: Vec<f64> = test
+        .epoch_metrics
+        .iter()
+        .map(|m| m.mean_coherence)
+        .collect();
 
     let mean_coherence_control = Statistics::from_slice(&control_coherence).mean;
     let mean_coherence_test = Statistics::from_slice(&test_coherence).mean;
     let mean_coherence_improvement = mean_coherence_test - mean_coherence_control;
 
     // Mean energy comparison (lower is better)
-    let control_energy: Vec<f64> = control.epoch_metrics.iter().map(|m| m.mean_energy).collect();
+    let control_energy: Vec<f64> = control
+        .epoch_metrics
+        .iter()
+        .map(|m| m.mean_energy)
+        .collect();
     let test_energy: Vec<f64> = test.epoch_metrics.iter().map(|m| m.mean_energy).collect();
 
     let mean_energy_control = Statistics::from_slice(&control_energy).mean;
     let mean_energy_test = Statistics::from_slice(&test_energy).mean;
     let mean_energy_improvement = mean_energy_control - mean_energy_test; // Note: reversed (lower is better)
 
     // Simple significance test: improvement must be > threshold and positive
-    let is_significant = accuracy_improvement > significance_threshold && accuracy_improvement > 0.0;
+    let is_significant =
+        accuracy_improvement > significance_threshold && accuracy_improvement > 0.0;
 
     ExperimentComparison {
         control_strategy: control.strategy.clone(),
         test_strategy: test.strategy.clone(),
         accuracy_improvement,
         accuracy_improvement_pct,
         convergence_epoch_delta,
         mean_coherence_improvement,
         mean_energy_improvement,
         is_significant,
         significance_level: significance_threshold,
     }
 }
 
 /// Compute t-test statistic for two independent samples
 ///
 /// Returns (t_statistic, degrees_of_freedom)
 /// Note: This is Welch's t-test (unequal variances)
 pub fn welch_t_test(sample1: &[f64], sample2: &[f64]) -> (f64, f64) {
     let stats1 = Statistics::from_slice(sample1);
     let stats2 = Statistics::from_slice(sample2);
 
     if stats1.count == 0 || stats2.count == 0 {
         return (0.0, 0.0);
     }
 
     let mean_diff = stats1.mean - stats2.mean;
     let se1 = stats1.variance / stats1.count as f64;
     let se2 = stats2.variance / stats2.count as f64;
     let se = (se1 + se2).sqrt();
 
-    let t_stat = if se > 0.0 {
-        mean_diff / se
-    } else {
-        0.0
-    };
+    let t_stat = if se > 0.0 { mean_diff / se } else { 0.0 };
 
     // Welch-Satterthwaite degrees of freedom
     let df = if se1 > 0.0 && se2 > 0.0 {
         let numerator = (se1 + se2).powi(2);
         let denom1 = se1.powi(2) / (stats1.count - 1) as f64;
         let denom2 = se2.powi(2) / (stats2.count - 1) as f64;
         numerator / (denom1 + denom2)
     } else {
         0.0
     };
 
     (t_stat, df)
 }
 
 /// Generate a summary report comparing two experiments
 pub fn generate_report(comparison: &ExperimentComparison) -> String {
     let mut report = String::new();
 
     report.push_str("=== Experiment Comparison Report ===\n\n");
 
     report.push_str(&format!(
         "Control Strategy: {}\n",
         comparison.control_strategy
     ));
     report.push_str(&format!("Test Strategy: {}\n\n", comparison.test_strategy));
@@ -205,73 +210,72 @@ pub fn generate_report(comparison: &ExperimentComparison) -> String {
     ));
     report.push_str(&format!(
         "Mean Energy Improvement: {:.4}\n",
         comparison.mean_energy_improvement
     ));
 
     report.push_str("\n--- Statistical Significance ---\n");
     report.push_str(&format!(
         "Significance Threshold: {:.4}\n",
         comparison.significance_level
     ));
     report.push_str(&format!(
         "Is Significant: {}\n",
         if comparison.is_significant {
             "YES ✓"
         } else {
             "NO"
         }
     ));
 
     report.push_str("\n--- Conclusion ---\n");
     if comparison.is_significant {
         report.push_str("✅ The test strategy shows statistically significant improvement.\n");
         report.push_str("   Recommendation: PROCEED with Dream Pool implementation.\n");
     } else if comparison.accuracy_improvement > 0.0 {
-        report.push_str("⚠️  The test strategy shows improvement, but below significance threshold.\n");
+        report.push_str(
+            "⚠️  The test strategy shows improvement, but below significance threshold.\n",
+        );
         report.push_str("   Recommendation: INVESTIGATE further or tune parameters.\n");
     } else {
         report.push_str("❌ The test strategy does not show improvement.\n");
         report.push_str("   Recommendation: DEFER Dream Pool implementation.\n");
     }
 
     report
 }
 
 /// Analyze learning curves (accuracy over epochs)
 pub fn analyze_learning_curves(
     control_metrics: &[EpochMetrics],
     test_metrics: &[EpochMetrics],
 ) -> LearningCurveAnalysis {
     let control_accuracies: Vec<f64> = control_metrics
         .iter()
         .map(|m| m.validation_accuracy)
         .collect();
-    let test_accuracies: Vec<f64> = test_metrics
-        .iter()
-        .map(|m| m.validation_accuracy)
-        .collect();
+    let test_accuracies: Vec<f64> = test_metrics.iter().map(|m| m.validation_accuracy).collect();
 
     let control_stats = Statistics::from_slice(&control_accuracies);
     let test_stats = Statistics::from_slice(&test_accuracies);
 
     // Find epoch where test first exceeds control
     let mut first_improvement_epoch = None;
     for (i, (control, test)) in control_metrics.iter().zip(test_metrics.iter()).enumerate() {
         if test.validation_accuracy > control.validation_accuracy {
             first_improvement_epoch = Some(i);
             break;
         }
     }
 
     LearningCurveAnalysis {
         control_stats,
         test_stats,
         first_improvement_epoch,
     }
 }
 
 /// Learning curve analysis result
 #[derive(Debug, Clone, Serialize)]
 pub struct LearningCurveAnalysis {
     pub control_stats: Statistics,
     pub test_stats: Statistics,
diff --git a/src/dream/bias.rs b/src/dream/bias.rs
index a47ba410a4c424af20f4d50d67b907eb3c0dc5e8..714af74fc5d6a01469b37c79c767e32cc3ff53af 100644
--- a/src/dream/bias.rs
+++ b/src/dream/bias.rs
@@ -272,54 +272,51 @@ fn synthesize_spectral_biases(aggregator: &UtilityAggregator) -> SpectralBias {
         .iter()
         .filter_map(|r| r.spectral_features.as_ref().map(|f| f.high_freq_energy))
         .collect();
 
     let low_freq_threshold = if !low_freqs.is_empty() {
         Some(low_freqs.iter().sum::<f32>() / low_freqs.len() as f32)
     } else {
         None
     };
 
     let high_freq_threshold = if !high_freqs.is_empty() {
         Some(high_freqs.iter().sum::<f32>() / high_freqs.len() as f32)
     } else {
         None
     };
 
     SpectralBias {
         entropy_range,
         entropy_utility_correlation,
         low_freq_threshold,
         high_freq_threshold,
     }
 }
 
 /// Synthesize chromatic signature biases from feedback.
-fn synthesize_chroma_biases(
-    aggregator: &UtilityAggregator,
-    utility_threshold: f32,
-) -> ChromaBias {
+fn synthesize_chroma_biases(aggregator: &UtilityAggregator, utility_threshold: f32) -> ChromaBias {
     // Get helpful dreams
     let helpful = aggregator.filter_by_utility(utility_threshold);
 
     if helpful.is_empty() {
         return ChromaBias {
             red_range: None,
             green_range: None,
             blue_range: None,
         };
     }
 
     // Extract RGB ranges from helpful dreams
     let reds: Vec<f32> = helpful.iter().map(|r| r.chroma_signature[0]).collect();
     let greens: Vec<f32> = helpful.iter().map(|r| r.chroma_signature[1]).collect();
     let blues: Vec<f32> = helpful.iter().map(|r| r.chroma_signature[2]).collect();
 
     let red_range = compute_range(&reds);
     let green_range = compute_range(&greens);
     let blue_range = compute_range(&blues);
 
     ChromaBias {
         red_range,
         green_range,
         blue_range,
     }
diff --git a/src/dream/diversity.rs b/src/dream/diversity.rs
index 1663f84f81045a315fafe63370e6f89e8d8c6b88..bd66dabba84e333dcc2511fc40392694778d1931 100644
--- a/src/dream/diversity.rs
+++ b/src/dream/diversity.rs
@@ -1,36 +1,35 @@
 /// Diversity enforcement for Dream Pool retrieval.
 ///
 /// This module implements diversity metrics and Maximum Marginal Relevance (MMR)
 /// to ensure retrieved dreams have sufficient chromatic variation.
 ///
 /// **Problem:** Cosine similarity can return near-duplicates, reducing effective
 /// batch diversity and limiting training data variation.
 ///
 /// **Solution:** MMR balances relevance (similarity to query) with diversity
 /// (dissimilarity to already-selected dreams).
-
 use crate::dream::simple_pool::DreamEntry;
 
 /// Compute pairwise chromatic dispersion between dream entries.
 ///
 /// **Dispersion** = mean pairwise Euclidean distance in RGB space.
 ///
 /// # Arguments
 /// * `dreams` - Set of dream entries to analyze
 ///
 /// # Returns
 /// * Mean pairwise L2 distance across all RGB channels
 /// * Returns 0.0 for empty or single-element sets
 ///
 /// # Example
 /// ```
 /// # use chromatic_cognition_core::dream::diversity::chroma_dispersion;
 /// # use chromatic_cognition_core::dream::simple_pool::DreamEntry;
 /// # use chromatic_cognition_core::tensor::ChromaticTensor;
 /// # use chromatic_cognition_core::solver::SolverResult;
 /// # use serde_json::json;
 /// # let tensor = ChromaticTensor::new(2, 2, 4);
 /// # let result = SolverResult { energy: 0.1, coherence: 0.8, violation: 0.0, grad: None, mask: None, meta: json!({}) };
 /// let mut dreams = vec![
 ///     DreamEntry::new(tensor.clone(), result.clone()),
 ///     DreamEntry::new(tensor.clone(), result.clone()),
@@ -206,51 +205,53 @@ pub fn retrieve_diverse_mmr(
     lambda: f32,
     min_dispersion: f32,
 ) -> Vec<DreamEntry> {
     if candidates.is_empty() || k == 0 {
         return Vec::new();
     }
 
     let mut selected = Vec::with_capacity(k);
     let mut remaining: Vec<&DreamEntry> = candidates.iter().collect();
 
     // Greedy selection: pick best MMR score at each step
     for _ in 0..k {
         if remaining.is_empty() {
             break;
         }
 
         // Find candidate with highest MMR score
         let (best_idx, _) = remaining
             .iter()
             .enumerate()
             .map(|(idx, candidate)| {
                 let score = mmr_score(candidate, query_sig, &selected, lambda);
                 (idx, score)
             })
             .max_by(|(_, score_a), (_, score_b)| {
-                score_a.partial_cmp(score_b).unwrap_or(std::cmp::Ordering::Equal)
+                score_a
+                    .partial_cmp(score_b)
+                    .unwrap_or(std::cmp::Ordering::Equal)
             })
             .unwrap();
 
         // Add to selected set
         let chosen = remaining.remove(best_idx).clone();
         selected.push(chosen);
 
         // Check dispersion constraint
         if min_dispersion > 0.0 && selected.len() >= 2 {
             let dispersion = chroma_dispersion(&selected);
             if dispersion < min_dispersion {
                 // Constraint violated, stop early
                 break;
             }
         }
     }
 
     selected
 }
 
 /// Fast approximate MMR selection using early termination and sampling.
 ///
 /// **Optimizations:**
 /// 1. Early termination: Skip candidates with low query similarity
 /// 2. Sampling: Approximate max similarity to selected set (sample instead of full scan)
@@ -294,58 +295,62 @@ pub fn retrieve_diverse_mmr_fast(
 
     for _ in 0..k {
         if remaining.is_empty() {
             break;
         }
 
         let mut best_idx = 0;
         let mut best_score = f32::NEG_INFINITY;
 
         for (idx, candidate) in remaining.iter().enumerate() {
             // Relevance to query
             let relevance = cosine_similarity(&candidate.chroma_signature, query_sig);
 
             // Early termination: skip if relevance too low
             if relevance < min_similarity {
                 continue;
             }
 
             // Diversity: approximate max similarity using sampling
             let max_similarity = if selected.is_empty() {
                 0.0
             } else if sample_size == 0 || selected.len() <= sample_size {
                 // Small selected set or no sampling: compute exact
                 selected
                     .iter()
-                    .map(|s: &DreamEntry| cosine_similarity(&candidate.chroma_signature, &s.chroma_signature))
+                    .map(|s: &DreamEntry| {
+                        cosine_similarity(&candidate.chroma_signature, &s.chroma_signature)
+                    })
                     .fold(f32::NEG_INFINITY, f32::max)
             } else {
                 // Large selected set: sample
                 selected
                     .iter()
                     .step_by(selected.len() / sample_size)
-                    .map(|s: &DreamEntry| cosine_similarity(&candidate.chroma_signature, &s.chroma_signature))
+                    .map(|s: &DreamEntry| {
+                        cosine_similarity(&candidate.chroma_signature, &s.chroma_signature)
+                    })
                     .fold(f32::NEG_INFINITY, f32::max)
             };
 
             // MMR score
             let score = lambda * relevance - (1.0 - lambda) * max_similarity;
 
             if score > best_score {
                 best_score = score;
                 best_idx = idx;
             }
         }
 
         // If no candidate passed threshold, stop
         if best_score == f32::NEG_INFINITY {
             break;
         }
 
         let chosen = remaining.remove(best_idx).clone();
         selected.push(chosen);
     }
 
     selected
 }
 
 /// Statistics about dream set diversity.
@@ -362,54 +367,52 @@ pub struct DiversityStats {
 }
 
 impl DiversityStats {
     /// Compute diversity statistics for a set of dreams.
     ///
     /// # Arguments
     /// * `dreams` - Set of dream entries to analyze
     ///
     /// # Returns
     /// * Diversity statistics struct
     pub fn compute(dreams: &[DreamEntry]) -> Self {
         if dreams.len() <= 1 {
             return Self {
                 mean_dispersion: 0.0,
                 min_distance: 0.0,
                 max_distance: 0.0,
                 count: dreams.len(),
             };
         }
 
         let n = dreams.len();
         let mut distances = Vec::new();
 
         for i in 0..n {
             for j in (i + 1)..n {
-                let dist = euclidean_distance(
-                    &dreams[i].chroma_signature,
-                    &dreams[j].chroma_signature,
-                );
+                let dist =
+                    euclidean_distance(&dreams[i].chroma_signature, &dreams[j].chroma_signature);
                 distances.push(dist);
             }
         }
 
         let mean = distances.iter().sum::<f32>() / distances.len() as f32;
         let min = distances.iter().cloned().fold(f32::INFINITY, f32::min);
         let max = distances.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
 
         Self {
             mean_dispersion: mean,
             min_distance: min,
             max_distance: max,
             count: dreams.len(),
         }
     }
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
     use crate::solver::SolverResult;
     use crate::tensor::ChromaticTensor;
     use serde_json::json;
 
     fn make_dream(sig: [f32; 3]) -> DreamEntry {
@@ -491,52 +494,55 @@ mod tests {
         let score_pure_relevance = mmr_score(&candidate, &query, &selected, 1.0);
         assert!(score_pure_relevance > 0.8); // Still high relevance
     }
 
     #[test]
     fn test_retrieve_diverse_mmr() {
         let candidates = vec![
             make_dream([1.0, 0.0, 0.0]),   // Red (most relevant)
             make_dream([0.95, 0.05, 0.0]), // Red-ish (similar to red)
             make_dream([0.0, 1.0, 0.0]),   // Green (diverse but less relevant)
             make_dream([0.0, 0.0, 1.0]),   // Blue (diverse but less relevant)
         ];
         let query = [1.0, 0.0, 0.0]; // Red query
 
         // With lambda=0.5 (equal balance relevance/diversity), should select red + diverse color
         let selected = retrieve_diverse_mmr(&candidates, &query, 2, 0.5, 0.0);
         assert_eq!(selected.len(), 2);
 
         // First should be pure red (most relevant)
         let sig0 = selected[0].chroma_signature;
         assert!((sig0[0] - 1.0).abs() < 0.1);
 
         // Second should be diverse (green or blue, not red-ish)
         let sig1 = selected[1].chroma_signature;
         // Either green (sig1[1] > 0.5) or blue (sig1[2] > 0.5)
-        assert!(sig1[1] > 0.5 || sig1[2] > 0.5,
-                "Second selection should be diverse (green or blue), got {:?}", sig1);
+        assert!(
+            sig1[1] > 0.5 || sig1[2] > 0.5,
+            "Second selection should be diverse (green or blue), got {:?}",
+            sig1
+        );
     }
 
     #[test]
     fn test_retrieve_diverse_mmr_with_min_dispersion() {
         let candidates = vec![
             make_dream([1.0, 0.0, 0.0]),   // Red
             make_dream([0.99, 0.01, 0.0]), // Almost identical red
             make_dream([0.98, 0.02, 0.0]), // Almost identical red
         ];
         let query = [1.0, 0.0, 0.0];
 
         // With high min_dispersion, should stop early due to constraint
         let selected = retrieve_diverse_mmr(&candidates, &query, 3, 0.7, 0.5);
         assert!(selected.len() < 3); // Stopped due to dispersion constraint
     }
 
     #[test]
     fn test_diversity_stats() {
         let dreams = vec![
             make_dream([1.0, 0.0, 0.0]),
             make_dream([0.0, 1.0, 0.0]),
             make_dream([0.0, 0.0, 1.0]),
         ];
 
         let stats = DiversityStats::compute(&dreams);
@@ -556,54 +562,54 @@ mod tests {
 
     #[test]
     fn test_retrieve_diverse_mmr_fast_basic() {
         // Test that fast MMR produces reasonable results
         let candidates = vec![
             make_dream([1.0, 0.0, 0.0]),   // Red (most relevant)
             make_dream([0.95, 0.05, 0.0]), // Red-ish (similar to red)
             make_dream([0.0, 1.0, 0.0]),   // Green (diverse)
             make_dream([0.0, 0.0, 1.0]),   // Blue (diverse)
         ];
         let query = [1.0, 0.0, 0.0]; // Red query
 
         // No early termination, no sampling (should match exact MMR)
         let selected = retrieve_diverse_mmr_fast(&candidates, &query, 2, 0.5, 0.0, 0);
         assert_eq!(selected.len(), 2);
 
         // First should be red (most relevant)
         let sig0 = selected[0].chroma_signature;
         assert!((sig0[0] - 1.0).abs() < 0.1);
     }
 
     #[test]
     fn test_retrieve_diverse_mmr_fast_early_termination() {
         // Test that low-similarity candidates are skipped
         let candidates = vec![
-            make_dream([1.0, 0.0, 0.0]),   // Red (relevant)
-            make_dream([0.9, 0.1, 0.0]),   // Red-ish (relevant)
-            make_dream([0.0, 1.0, 0.0]),   // Green (low similarity to red)
-            make_dream([0.0, 0.0, 1.0]),   // Blue (low similarity to red)
+            make_dream([1.0, 0.0, 0.0]), // Red (relevant)
+            make_dream([0.9, 0.1, 0.0]), // Red-ish (relevant)
+            make_dream([0.0, 1.0, 0.0]), // Green (low similarity to red)
+            make_dream([0.0, 0.0, 1.0]), // Blue (low similarity to red)
         ];
         let query = [1.0, 0.0, 0.0]; // Red query
 
         // Set min_similarity = 0.8 (should skip green and blue)
         let selected = retrieve_diverse_mmr_fast(&candidates, &query, 4, 0.5, 0.8, 0);
 
         // Should only select red and red-ish (green and blue terminated early)
         assert_eq!(selected.len(), 2);
 
         // Both should be red-ish
         for entry in &selected {
             let sig = entry.chroma_signature;
             assert!(sig[0] > 0.8, "Expected red-dominant, got {:?}", sig);
         }
     }
 
     #[test]
     fn test_retrieve_diverse_mmr_fast_sampling() {
         // Test sampling approximation with large selected set
         let mut candidates = vec![];
 
         // Create many candidates with varying colors
         for i in 0..20 {
             let r = (i as f32) / 20.0;
             let g = 1.0 - r;
@@ -676,36 +682,33 @@ mod tests {
 
         // Early termination at 0.8, sampling at size 2
         let selected = retrieve_diverse_mmr_fast(&candidates, &query, 5, 0.5, 0.8, 2);
 
         // Should only select from red variants (green terminated)
         assert!(selected.len() <= 10);
 
         // All should be red-dominant
         for entry in &selected {
             let sig = entry.chroma_signature;
             assert!(sig[0] > 0.8, "Expected red-dominant, got {:?}", sig);
         }
     }
 
     #[test]
     fn test_retrieve_diverse_mmr_fast_empty_candidates() {
         let candidates = vec![];
         let query = [1.0, 0.0, 0.0];
 
         let selected = retrieve_diverse_mmr_fast(&candidates, &query, 5, 0.5, 0.0, 0);
         assert_eq!(selected.len(), 0);
     }
 
     #[test]
     fn test_retrieve_diverse_mmr_fast_fewer_candidates_than_k() {
-        let candidates = vec![
-            make_dream([1.0, 0.0, 0.0]),
-            make_dream([0.0, 1.0, 0.0]),
-        ];
+        let candidates = vec![make_dream([1.0, 0.0, 0.0]), make_dream([0.0, 1.0, 0.0])];
         let query = [1.0, 0.0, 0.0];
 
         // Request more than available
         let selected = retrieve_diverse_mmr_fast(&candidates, &query, 10, 0.5, 0.0, 0);
         assert_eq!(selected.len(), 2); // Should return all available
     }
 }
diff --git a/src/dream/embedding.rs b/src/dream/embedding.rs
index 4076126d138a791e1edcedb2ed525eda1be1b8aa..ec5d62483decad2129387cb0c052787fdd51595c 100644
--- a/src/dream/embedding.rs
+++ b/src/dream/embedding.rs
@@ -99,51 +99,52 @@ impl EmbeddingMapper {
     ///
     /// # Arguments
     /// * `entry` - Dream entry to encode
     /// * `bias` - Optional bias profile for conditioning
     ///
     /// # Returns
     /// * D-dimensional embedding vector
     pub fn encode_entry(&self, entry: &DreamEntry, bias: Option<&BiasProfile>) -> Vec<f32> {
         let mut features = Vec::new();
 
         // 1. Chromatic signature (3 dims)
         features.extend_from_slice(&entry.chroma_signature);
 
         // 2. Spectral features (6 dims) - Always present since Phase 4 optimization
         if self.include_spectral {
             let spectral = &entry.spectral_features;
             features.push(spectral.entropy);
             features.push(spectral.low_freq_energy);
             features.push(spectral.mid_freq_energy);
             features.push(spectral.high_freq_energy);
             features.push(spectral.mean_psd);
 
             // Dominant frequency (averaged across RGB)
             let dom_freq_mean = (spectral.dominant_frequencies[0] as f32
                 + spectral.dominant_frequencies[1] as f32
-                + spectral.dominant_frequencies[2] as f32) / 3.0;
+                + spectral.dominant_frequencies[2] as f32)
+                / 3.0;
             features.push(dom_freq_mean);
         }
 
         // 3. Class one-hot (10 dims)
         if self.include_class {
             if let Some(class) = entry.class_label {
                 let onehot = self.class_to_onehot(class);
                 features.extend_from_slice(&onehot);
             } else {
                 // Unknown class - use uniform distribution
                 features.extend_from_slice(&[0.1; 10]);
             }
         }
 
         // 4. Utility features (2 dims: raw utility + bias weight)
         if self.include_utility {
             let utility = entry.utility.unwrap_or(0.0);
             features.push(utility);
 
             // Bias weight if available
             if let Some(bias_profile) = bias {
                 if let Some(class) = entry.class_label {
                     let weight = bias_profile.class_weight(class);
                     features.push(weight);
                 } else {
@@ -161,51 +162,52 @@ impl EmbeddingMapper {
     /// Encode a query signature into an embedding vector.
     ///
     /// # Arguments
     /// * `query` - Query signature
     /// * `bias` - Optional bias profile for conditioning
     ///
     /// # Returns
     /// * D-dimensional embedding vector
     pub fn encode_query(&self, query: &QuerySignature, bias: Option<&BiasProfile>) -> Vec<f32> {
         let mut features = Vec::new();
 
         // 1. Chromatic signature (3 dims)
         features.extend_from_slice(&query.chroma);
 
         // 2. Spectral features (6 dims)
         if self.include_spectral {
             if let Some(ref spectral) = query.spectral {
                 features.push(spectral.entropy);
                 features.push(spectral.low_freq_energy);
                 features.push(spectral.mid_freq_energy);
                 features.push(spectral.high_freq_energy);
                 features.push(spectral.mean_psd);
 
                 let dom_freq_mean = (spectral.dominant_frequencies[0] as f32
                     + spectral.dominant_frequencies[1] as f32
-                    + spectral.dominant_frequencies[2] as f32) / 3.0;
+                    + spectral.dominant_frequencies[2] as f32)
+                    / 3.0;
                 features.push(dom_freq_mean);
             } else {
                 features.extend_from_slice(&[0.0; 6]);
             }
         }
 
         // 3. Class one-hot (10 dims)
         if self.include_class {
             if let Some(class) = query.class_hint {
                 let onehot = self.class_to_onehot(class);
                 features.extend_from_slice(&onehot);
             } else {
                 // No class hint - use uniform
                 features.extend_from_slice(&[0.1; 10]);
             }
         }
 
         // 4. Utility features (2 dims)
         if self.include_utility {
             let utility_prior = query.utility_prior.unwrap_or(0.0);
             features.push(utility_prior);
 
             // Bias weight
             if let Some(bias_profile) = bias {
                 if let Some(class) = query.class_hint {
@@ -229,52 +231,52 @@ impl EmbeddingMapper {
         onehot[idx] = 1.0;
         onehot
     }
 
     /// Project features to target dimension and apply layer normalization.
     ///
     /// Uses a simple deterministic projection (no learned weights).
     fn project_and_normalize(&self, features: Vec<f32>) -> Vec<f32> {
         let feature_dim = features.len();
 
         // Simple linear projection: repeat/truncate to target dim
         let mut projected = Vec::with_capacity(self.dim);
 
         if self.dim <= feature_dim {
             // Truncate
             projected.extend_from_slice(&features[..self.dim]);
         } else {
             // Repeat features cyclically to fill dimension
             for i in 0..self.dim {
                 projected.push(features[i % feature_dim]);
             }
         }
 
         // Layer normalization: (x - mean) / std
         let mean = projected.iter().sum::<f32>() / projected.len() as f32;
-        let variance = projected.iter().map(|&x| (x - mean).powi(2)).sum::<f32>()
-            / projected.len() as f32;
+        let variance =
+            projected.iter().map(|&x| (x - mean).powi(2)).sum::<f32>() / projected.len() as f32;
         let std = (variance + 1e-8).sqrt(); // Add epsilon for numerical stability
 
         projected.iter().map(|&x| (x - mean) / std).collect()
     }
 
     /// Get expected feature dimension (before projection).
     pub fn feature_dim(&self) -> usize {
         let mut dim = 3; // RGB
 
         if self.include_spectral {
             dim += 6; // entropy + 3 band energies + mean_psd + dom_freq
         }
 
         if self.include_class {
             dim += 10; // one-hot
         }
 
         if self.include_utility {
             dim += 2; // utility + bias_weight
         }
 
         dim
     }
 }
 
@@ -345,52 +347,52 @@ mod tests {
         let query = QuerySignature::from_chroma([1.0, 0.0, 0.0]);
 
         let embedding = mapper.encode_query(&query, None);
         assert_eq!(embedding.len(), 64);
     }
 
     #[test]
     fn test_class_to_onehot() {
         let mapper = EmbeddingMapper::new(64);
         let onehot = mapper.class_to_onehot(ColorClass::Red);
 
         assert_eq!(onehot[0], 1.0); // Red is index 0
         assert_eq!(onehot[1], 0.0);
         assert_eq!(onehot.iter().sum::<f32>(), 1.0);
     }
 
     #[test]
     fn test_layer_normalization() {
         let mapper = EmbeddingMapper::new(64);
         let entry = make_dream_entry();
 
         let embedding = mapper.encode_entry(&entry, None);
 
         // Check layer norm properties: mean ≈ 0, std ≈ 1
         let mean = embedding.iter().sum::<f32>() / embedding.len() as f32;
-        let variance = embedding.iter().map(|&x| (x - mean).powi(2)).sum::<f32>()
-            / embedding.len() as f32;
+        let variance =
+            embedding.iter().map(|&x| (x - mean).powi(2)).sum::<f32>() / embedding.len() as f32;
         let std = variance.sqrt();
 
         assert!(mean.abs() < 0.01); // Mean should be ~0
         assert!((std - 1.0).abs() < 0.01); // Std should be ~1
     }
 
     #[test]
     fn test_different_classes_different_embeddings() {
         let mapper = EmbeddingMapper::new(64);
 
         let mut entry1 = make_dream_entry();
         entry1.class_label = Some(ColorClass::Red);
 
         let mut entry2 = make_dream_entry();
         entry2.class_label = Some(ColorClass::Blue);
 
         let embed1 = mapper.encode_entry(&entry1, None);
         let embed2 = mapper.encode_entry(&entry2, None);
 
         assert_ne!(embed1, embed2);
     }
 
     #[test]
     fn test_query_with_class_hint() {
         let mapper = EmbeddingMapper::new(64);
diff --git a/src/dream/error.rs b/src/dream/error.rs
index 85fe143fc28d7368921762e89c011053615cc4e7..5e3a490abf68dc6338dee82b949a1700e1553fd1 100644
--- a/src/dream/error.rs
+++ b/src/dream/error.rs
@@ -1,138 +1,138 @@
 //! Error types for dream pool operations
 //!
 //! This module provides comprehensive error handling for all dream pool
 //! operations, replacing panics with Result types for better library ergonomics.
 
 use std::fmt;
 
 /// Result type alias for dream pool operations
 pub type DreamResult<T> = Result<T, DreamError>;
 
 /// Comprehensive error type for dream pool operations
 #[derive(Debug, Clone, PartialEq)]
 pub enum DreamError {
     /// Dimension mismatch between query and index
     DimensionMismatch {
         expected: usize,
         got: usize,
         context: String,
     },
 
     /// Index has not been built yet
-    IndexNotBuilt {
-        operation: String,
-    },
+    IndexNotBuilt { operation: String },
 
     /// Pool has reached capacity
-    CapacityExceeded {
-        current: usize,
-        max: usize,
-    },
+    CapacityExceeded { current: usize, max: usize },
 
     /// Invalid configuration parameter
     InvalidConfiguration {
         parameter: String,
         value: String,
         reason: String,
     },
 
     /// Empty pool or candidate set
-    EmptyCollection {
-        collection: String,
-    },
+    EmptyCollection { collection: String },
 
     /// Invalid parameter value
     InvalidParameter {
         parameter: String,
         value: String,
         constraint: String,
     },
 
     /// Index corruption detected
-    IndexCorrupted {
-        details: String,
-    },
+    IndexCorrupted { details: String },
 
     /// Memory budget exceeded
-    MemoryExceeded {
-        requested: usize,
-        available: usize,
-    },
+    MemoryExceeded { requested: usize, available: usize },
 
     /// Operation requires a feature that is not available
-    FeatureUnavailable {
-        feature: String,
-        reason: String,
-    },
+    FeatureUnavailable { feature: String, reason: String },
 }
 
 impl fmt::Display for DreamError {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
         match self {
-            DreamError::DimensionMismatch { expected, got, context } => {
+            DreamError::DimensionMismatch {
+                expected,
+                got,
+                context,
+            } => {
                 write!(
                     f,
                     "Dimension mismatch in {}: expected {} dimensions, got {}",
                     context, expected, got
                 )
             }
             DreamError::IndexNotBuilt { operation } => {
                 write!(
                     f,
                     "Index not built: operation '{}' requires index to be built first. Call build() before querying.",
                     operation
                 )
             }
             DreamError::CapacityExceeded { current, max } => {
                 write!(
                     f,
                     "Pool capacity exceeded: current size {} exceeds maximum {}",
                     current, max
                 )
             }
-            DreamError::InvalidConfiguration { parameter, value, reason } => {
+            DreamError::InvalidConfiguration {
+                parameter,
+                value,
+                reason,
+            } => {
                 write!(
                     f,
                     "Invalid configuration for parameter '{}' with value '{}': {}",
                     parameter, value, reason
                 )
             }
             DreamError::EmptyCollection { collection } => {
                 write!(f, "Empty collection: {}", collection)
             }
-            DreamError::InvalidParameter { parameter, value, constraint } => {
+            DreamError::InvalidParameter {
+                parameter,
+                value,
+                constraint,
+            } => {
                 write!(
                     f,
                     "Invalid parameter '{}' = '{}': must satisfy {}",
                     parameter, value, constraint
                 )
             }
             DreamError::IndexCorrupted { details } => {
                 write!(f, "Index corrupted: {}", details)
             }
-            DreamError::MemoryExceeded { requested, available } => {
+            DreamError::MemoryExceeded {
+                requested,
+                available,
+            } => {
                 write!(
                     f,
                     "Memory budget exceeded: requested {} bytes, only {} available",
                     requested, available
                 )
             }
             DreamError::FeatureUnavailable { feature, reason } => {
                 write!(f, "Feature '{}' unavailable: {}", feature, reason)
             }
         }
     }
 }
 
 impl std::error::Error for DreamError {}
 
 // Convenience constructors for common error patterns
 impl DreamError {
     /// Create a dimension mismatch error
     pub fn dimension_mismatch(expected: usize, got: usize, context: impl Into<String>) -> Self {
         DreamError::DimensionMismatch {
             expected,
             got,
             context: context.into(),
         }
     }
diff --git a/src/dream/experiment.rs b/src/dream/experiment.rs
index 05d22aeac4f8b7a29990d7634d30300edb62a857..7547ca686bda90bc5eb42e2cbc57d501bf3104be 100644
--- a/src/dream/experiment.rs
+++ b/src/dream/experiment.rs
@@ -1,36 +1,36 @@
 //! A/B testing harness for Dream Pool validation experiments
 //!
 //! This module implements the validation experiment specified in
 //! "Validation Experiment Specification: Retrieval Hypothesis"
 
 use crate::data::{ColorDataset, ColorSample, DatasetConfig};
-use crate::dream::SimpleDreamPool;
 use crate::dream::simple_pool::{PoolConfig, PoolStats};
+use crate::dream::SimpleDreamPool;
 use crate::solver::Solver;
-use crate::tensor::{ChromaticTensor, operations::mix};
-use serde::{Serialize, Deserialize};
+use crate::tensor::{operations::mix, ChromaticTensor};
+use serde::{Deserialize, Serialize};
 use std::time::Instant;
 
 /// Seeding strategy for the experiment
 #[derive(Debug, Clone, Copy, PartialEq, Eq)]
 pub enum SeedingStrategy {
     /// Group A: Random noise or zero tensor (control)
     RandomNoise,
     /// Group B: Retrieval-based seeding from dream pool (test)
     RetrievalBased,
 }
 
 /// Configuration for a single experimental run
 #[derive(Debug, Clone)]
 pub struct ExperimentConfig {
     /// Seeding strategy to use
     pub strategy: SeedingStrategy,
     /// Number of training epochs
     pub num_epochs: usize,
     /// Batch size for training
     pub batch_size: usize,
     /// Number of solver iterations per dream cycle
     pub dream_iterations: usize,
     /// Pool configuration (only used for RetrievalBased strategy)
     pub pool_config: PoolConfig,
     /// Dataset configuration
@@ -115,87 +115,91 @@ impl<S: Solver> ExperimentHarness<S> {
 
         // Generate dataset
         let dataset = ColorDataset::generate(self.config.dataset_config.clone());
         let (train_samples, val_samples) = dataset.split(0.8);
 
         let mut step_metrics = Vec::new();
         let mut epoch_metrics = Vec::new();
 
         for epoch in 0..self.config.num_epochs {
             let epoch_start = Instant::now();
             let epoch_stats = self.run_epoch(epoch, &train_samples, &mut step_metrics);
 
             // Compute validation accuracy
             let val_accuracy = self.validate(&val_samples);
 
             epoch_metrics.push(EpochMetrics {
                 epoch,
                 mean_energy: epoch_stats.0,
                 mean_coherence: epoch_stats.1,
                 mean_violation: epoch_stats.2,
                 validation_accuracy: val_accuracy,
                 elapsed_ms: epoch_start.elapsed().as_millis(),
             });
         }
 
-        let final_accuracy = epoch_metrics.last().map(|m| m.validation_accuracy).unwrap_or(0.0);
+        let final_accuracy = epoch_metrics
+            .last()
+            .map(|m| m.validation_accuracy)
+            .unwrap_or(0.0);
         let convergence_epoch = self.find_convergence_epoch(&epoch_metrics, final_accuracy);
 
         ExperimentResult {
             strategy: format!("{:?}", self.config.strategy),
             step_metrics,
             epoch_metrics,
             final_accuracy,
             convergence_epoch,
             total_elapsed_ms: start_time.elapsed().as_millis(),
         }
     }
 
     /// Run a single training epoch
     fn run_epoch(
         &mut self,
         epoch: usize,
         train_samples: &[ColorSample],
         step_metrics: &mut Vec<StepMetrics>,
     ) -> (f64, f64, f64) {
         let mut sum_energy = 0.0;
         let mut sum_coherence = 0.0;
         let mut sum_violation = 0.0;
         let mut step_count = 0;
 
         for (batch_idx, sample) in train_samples.iter().enumerate() {
             let step_start = Instant::now();
 
             // Generate seed tensor based on strategy
             let seed_tensor = self.generate_seed_tensor(&sample.tensor);
 
             // Run dream cycle (multiple solver iterations)
             let mut current_tensor = seed_tensor;
             let mut final_result = None;
 
             for _ in 0..self.config.dream_iterations {
-                let result = self.solver
+                let result = self
+                    .solver
                     .evaluate(&current_tensor, false)
                     .expect("Solver evaluation failed");
 
                 final_result = Some(result.clone());
 
                 // Simple update: mix with input (simulation of gradient descent)
                 current_tensor = mix(&current_tensor, &sample.tensor);
             }
 
             let result = final_result.expect("No solver result");
 
             // Store in pool if using retrieval strategy
             if let Some(ref mut pool) = self.pool {
                 pool.add_if_coherent(current_tensor.clone(), result.clone());
             }
 
             // Record metrics
             sum_energy += result.energy;
             sum_coherence += result.coherence;
             sum_violation += result.violation;
             step_count += 1;
 
             step_metrics.push(StepMetrics {
                 epoch,
                 step: batch_idx,
@@ -251,62 +255,67 @@ impl<S: Solver> ExperimentHarness<S> {
                             layers,
                         );
 
                         for entry in similar {
                             seed = mix(&seed, &entry.tensor);
                         }
 
                         seed
                     }
                 } else {
                     panic!("Pool not initialized for retrieval strategy");
                 }
             }
         }
     }
 
     /// Validate on validation set (dummy implementation - returns coherence as proxy for accuracy)
     fn validate(&mut self, val_samples: &[ColorSample]) -> f64 {
         if val_samples.is_empty() {
             return 0.0;
         }
 
         let mut total_coherence = 0.0;
 
         for sample in val_samples.iter().take(50) {
-            let result = self.solver
+            let result = self
+                .solver
                 .evaluate(&sample.tensor, false)
                 .expect("Validation evaluation failed");
             total_coherence += result.coherence;
         }
 
         let sample_count = val_samples.len().min(50);
         total_coherence / sample_count as f64
     }
 
     /// Find the epoch where convergence occurred (90% of final accuracy)
-    fn find_convergence_epoch(&self, epoch_metrics: &[EpochMetrics], final_accuracy: f64) -> Option<usize> {
+    fn find_convergence_epoch(
+        &self,
+        epoch_metrics: &[EpochMetrics],
+        final_accuracy: f64,
+    ) -> Option<usize> {
         let target = final_accuracy * 0.9;
 
         for metrics in epoch_metrics {
             if metrics.validation_accuracy >= target {
                 return Some(metrics.epoch);
             }
         }
 
         None
     }
 
     /// Get pool statistics (if using retrieval strategy)
     pub fn pool_stats(&self) -> Option<PoolStats> {
         self.pool.as_ref().map(|p| p.stats())
     }
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
     use crate::ChromaticNativeSolver;
 
     #[test]
     fn test_experiment_control_group() {
         let config = ExperimentConfig {
diff --git a/src/dream/hnsw_index.rs b/src/dream/hnsw_index.rs
index 4ee5abd58ee3d09056ce26f470b5c22f06422776..6c4ffbd48a983366762153a8b88ea3d569076c79 100644
--- a/src/dream/hnsw_index.rs
+++ b/src/dream/hnsw_index.rs
@@ -1,96 +1,189 @@
 //! HNSW (Hierarchical Navigable Small World) index for fast k-NN retrieval
 //!
 //! This module provides a scalable approximate nearest neighbor (ANN) index
 //! that replaces the O(n) linear scan with O(log n) HNSW search.
 //!
 //! **Performance:** 100× speedup at 10K entries with 95-99% recall
 //!
 //! # When to enable HNSW
 //!
 //! The approximate index is most effective when either of the following is
 //! true:
 //!
 //! - The pool holds more than 5,000 entries.
 //! - Semantic queries must complete in under 100 milliseconds.
 //!
 //! Outside of those regimes the linear scan provides simpler, deterministic
 //! behaviour with lower memory overhead.
 
+use crate::checkpoint::CheckpointError;
 use crate::dream::error::{DreamError, DreamResult};
 use crate::dream::soft_index::{EntryId, Similarity};
+use hnsw_rs::hnswio::{HnswIo, ReloadOptions};
 use hnsw_rs::prelude::*;
+use serde::{Deserialize, Serialize};
 use std::collections::HashMap;
+use std::env;
+use std::fs;
+use std::path::{Path, PathBuf};
 use uuid::Uuid;
 
 /// HNSW-based approximate nearest neighbor index
 ///
 /// Provides fast similarity search with logarithmic complexity.
 ///
 /// # Architecture
 ///
 /// - **max_nb_connection (M):** Maximum connections per node (default: 16)
 /// - **ef_construction:** Quality parameter during build (default: 200)
 /// - **ef_search:** Quality parameter during search (default: 100)
 ///
 /// Higher values = better recall but slower/more memory
 ///
 /// # Example
 ///
 /// ```ignore
 /// let mut index = HnswIndex::new(64, 1000); // 64D embeddings, 1000 capacity
 /// index.add(EntryId::new_v4(), vec![0.1, 0.2, ...])?; // 64D vector
 /// index.build(Similarity::Cosine)?;
 /// let results = index.search(&query, 10, Similarity::Cosine)?;
 /// ```
 pub struct HnswIndex<'a> {
     /// HNSW index for cosine similarity
     hnsw_cosine: Option<Hnsw<'a, f32, DistCosine>>,
     /// HNSW index for Euclidean distance
     hnsw_euclidean: Option<Hnsw<'a, f32, DistL2>>,
     /// Mapping from EntryId to internal numeric identifier
     id_map: HashMap<Uuid, u32>,
     /// Mapping from internal numeric identifier back to EntryId
     id_slots: Vec<Option<EntryId>>,
     /// Stored embeddings indexed by internal identifier. Entries set to None have
     /// been evicted and are skipped during rebuilds.
     embeddings: Vec<Option<Vec<f32>>>,
     /// Tracks which internal nodes are still present in one or more active HNSW
     /// graphs despite having been logically removed from the pool. The u8 bitmask
     /// encodes which similarity modes (cosine/euclidean) still contain the node.
     ghost_metrics: HashMap<u32, u8>,
     /// Embedding dimension
     dim: usize,
     /// Maximum number of connections per node
     max_connections: usize,
     /// Construction quality parameter
     ef_construction: usize,
     /// Search quality parameter
     ef_search: usize,
 }
 
+/// Serialized representation of an ANN graph dump (graph + data).
+#[derive(Clone, Serialize, Deserialize)]
+pub(crate) struct HnswGraphSnapshot {
+    graph: Vec<u8>,
+    data: Vec<u8>,
+}
+
+/// Serializable snapshot capturing the full state of an [`HnswIndex`].
+#[derive(Clone, Serialize, Deserialize)]
+pub(crate) struct HnswIndexSnapshot {
+    dim: usize,
+    max_connections: usize,
+    ef_construction: usize,
+    ef_search: usize,
+    id_map: Vec<(EntryId, u32)>,
+    id_slots: Vec<Option<EntryId>>,
+    embeddings: Vec<Option<Vec<f32>>>,
+    ghost_metrics: Vec<(u32, u8)>,
+    cosine: Option<HnswGraphSnapshot>,
+    euclidean: Option<HnswGraphSnapshot>,
+}
+
+struct TempDirGuard {
+    path: PathBuf,
+}
+
+impl TempDirGuard {
+    fn new(prefix: &str) -> Result<Self, CheckpointError> {
+        let base = env::temp_dir();
+        let path = base.join(format!("{prefix}-{}", Uuid::new_v4()));
+        fs::create_dir_all(&path)?;
+        Ok(Self { path })
+    }
+
+    fn path(&self) -> &Path {
+        &self.path
+    }
+}
+
+impl Drop for TempDirGuard {
+    fn drop(&mut self) {
+        let _ = fs::remove_dir_all(&self.path);
+    }
+}
+
 impl<'a> HnswIndex<'a> {
+    /// Capture the full state of the index, including ANN graphs, for checkpointing.
+    pub(crate) fn snapshot(&self) -> Result<HnswIndexSnapshot, CheckpointError> {
+        let mut id_map: Vec<(EntryId, u32)> = self
+            .id_map
+            .iter()
+            .map(|(uuid, internal)| (*uuid, *internal))
+            .collect();
+        id_map.sort_by_key(|(uuid, _)| uuid.as_u128());
+
+        let mut ghost_metrics: Vec<(u32, u8)> = self
+            .ghost_metrics
+            .iter()
+            .map(|(id, mask)| (*id, *mask))
+            .collect();
+        ghost_metrics.sort_by_key(|(id, _)| *id);
+
+        let cosine = if let Some(index) = &self.hnsw_cosine {
+            Some(dump_hnsw_graph(index)?)
+        } else {
+            None
+        };
+
+        let euclidean = if let Some(index) = &self.hnsw_euclidean {
+            Some(dump_hnsw_graph(index)?)
+        } else {
+            None
+        };
+
+        Ok(HnswIndexSnapshot {
+            dim: self.dim,
+            max_connections: self.max_connections,
+            ef_construction: self.ef_construction,
+            ef_search: self.ef_search,
+            id_map,
+            id_slots: self.id_slots.clone(),
+            embeddings: self.embeddings.clone(),
+            ghost_metrics,
+            cosine,
+            euclidean,
+        })
+    }
+
     const GHOST_COSINE: u8 = 0b01;
     const GHOST_EUCLIDEAN: u8 = 0b10;
 
     fn mode_mask(mode: Similarity) -> u8 {
         match mode {
             Similarity::Cosine => Self::GHOST_COSINE,
             Similarity::Euclidean => Self::GHOST_EUCLIDEAN,
         }
     }
 
     fn clear_ghost_mask(&mut self, mask: u8) {
         self.ghost_metrics.retain(|_, value| {
             *value &= !mask;
             *value != 0
         });
     }
 
     fn ghost_count_for_mode(&self, mode: Similarity) -> usize {
         let mask = Self::mode_mask(mode);
         self.ghost_metrics
             .values()
             .filter(|value| (**value & mask) != 0)
             .count()
     }
 
@@ -472,50 +565,137 @@ impl<'a> HnswIndex<'a> {
         }
 
         if let Some(embedding) = self.embeddings.get_mut(idx) {
             *embedding = None;
         }
 
         self.mark_ghost(internal_id);
     }
 
     /// Query wrapper that mirrors the linear SoftIndex interface.
     pub fn query(
         &self,
         query: &[f32],
         k: usize,
         mode: Similarity,
     ) -> DreamResult<Vec<(EntryId, f32)>> {
         self.search(query, k, mode)
     }
 
     #[cfg(test)]
     pub(crate) fn ghost_count_for_mode_test(&self, mode: Similarity) -> usize {
         self.ghost_count_for_mode(mode)
     }
 }
 
+impl HnswIndex<'static> {
+    /// Reconstruct an index from a serialized snapshot.
+    pub(crate) fn from_snapshot(snapshot: HnswIndexSnapshot) -> Result<Self, CheckpointError> {
+        if snapshot.id_slots.len() != snapshot.embeddings.len() {
+            return Err(CheckpointError::InvalidFormat(
+                "HNSW snapshot slot and embedding counts differ".to_string(),
+            ));
+        }
+
+        let mut id_map = HashMap::with_capacity(snapshot.id_map.len());
+        for (entry_id, internal_id) in snapshot.id_map {
+            if id_map.insert(entry_id, internal_id).is_some() {
+                return Err(CheckpointError::InvalidFormat(format!(
+                    "Duplicate entry {entry_id} in HNSW snapshot"
+                )));
+            }
+        }
+
+        let ghost_metrics = snapshot.ghost_metrics.into_iter().collect();
+
+        let mut index = HnswIndex {
+            hnsw_cosine: None,
+            hnsw_euclidean: None,
+            id_map,
+            id_slots: snapshot.id_slots,
+            embeddings: snapshot.embeddings,
+            ghost_metrics,
+            dim: snapshot.dim,
+            max_connections: snapshot.max_connections,
+            ef_construction: snapshot.ef_construction,
+            ef_search: snapshot.ef_search,
+        };
+
+        if let Some(cosine_snapshot) = snapshot.cosine {
+            index.hnsw_cosine = Some(load_hnsw_graph::<DistCosine>(&cosine_snapshot)?);
+        }
+
+        if let Some(euclidean_snapshot) = snapshot.euclidean {
+            index.hnsw_euclidean = Some(load_hnsw_graph::<DistL2>(&euclidean_snapshot)?);
+        }
+
+        Ok(index)
+    }
+}
+
+fn dump_hnsw_graph<D>(index: &Hnsw<f32, D>) -> Result<HnswGraphSnapshot, CheckpointError>
+where
+    D: Distance<f32> + Send + Sync,
+{
+    let guard = TempDirGuard::new("csa-hnsw-dump")?;
+    let basename = format!("snapshot-{}", Uuid::new_v4());
+    index.file_dump(guard.path(), &basename).map_err(|err| {
+        CheckpointError::InvalidFormat(format!("Failed to dump HNSW graph: {err}"))
+    })?;
+
+    let graph_path = guard.path().join(format!("{basename}.hnsw.graph"));
+    let data_path = guard.path().join(format!("{basename}.hnsw.data"));
+
+    let graph = fs::read(&graph_path)?;
+    let data = fs::read(&data_path)?;
+
+    Ok(HnswGraphSnapshot { graph, data })
+}
+
+fn load_hnsw_graph<D>(
+    snapshot: &HnswGraphSnapshot,
+) -> Result<Hnsw<'static, f32, D>, CheckpointError>
+where
+    D: Distance<f32> + Default + Send + Sync + 'static,
+{
+    let guard = TempDirGuard::new("csa-hnsw-load")?;
+    let basename = format!("snapshot-{}", Uuid::new_v4());
+    let graph_path = guard.path().join(format!("{basename}.hnsw.graph"));
+    let data_path = guard.path().join(format!("{basename}.hnsw.data"));
+
+    fs::write(&graph_path, &snapshot.graph)?;
+    fs::write(&data_path, &snapshot.data)?;
+
+    let mut reloader = HnswIo::new(guard.path(), &basename);
+    reloader.set_options(ReloadOptions::new(false));
+    let hnsw = reloader.load_hnsw::<f32, D>().map_err(|err| {
+        CheckpointError::InvalidFormat(format!("Failed to reload HNSW graph: {err}"))
+    })?;
+
+    Ok(unsafe { std::mem::transmute::<Hnsw<'_, f32, D>, Hnsw<'static, f32, D>>(hnsw) })
+}
+
 /// Statistics for HNSW index
 #[derive(Debug, Clone)]
 pub struct HnswStats {
     /// Number of entries
     pub num_entries: usize,
     /// Embedding dimension
     pub dim: usize,
     /// Maximum connections per node
     pub max_connections: usize,
     /// Construction quality parameter
     pub ef_construction: usize,
     /// Search quality parameter
     pub ef_search: usize,
     /// Whether index has been built
     pub built: bool,
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
 
     fn create_test_embeddings(n: usize, dim: usize) -> Vec<(EntryId, Vec<f32>)> {
         (0..n)
             .map(|i| {
                 let id = EntryId::new_v4();
diff --git a/src/dream/hybrid_scoring.rs b/src/dream/hybrid_scoring.rs
index b8fdb30573f368d49caf8e42c01aa35337247e1f..c411eb5a042072d9129eb2d6476e5fd25a535946 100644
--- a/src/dream/hybrid_scoring.rs
+++ b/src/dream/hybrid_scoring.rs
@@ -1,79 +1,85 @@
 //! Hybrid scoring combining similarity, utility, class matching, and diversity.
 //!
 //! Implements multi-objective retrieval scoring for Phase 4:
 //! - α·similarity (from SoftIndex)
 //! - β·utility (from ΔLoss tracking)
 //! - γ·class_match (optional class conditioning)
 //! - δ·MMR_penalty (diversity enforcement)
 
+use crate::data::ColorClass;
 use crate::dream::simple_pool::DreamEntry;
 use crate::dream::soft_index::EntryId;
-use crate::data::ColorClass;
 use std::collections::HashMap;
 
 /// Weights for hybrid retrieval scoring
 #[derive(Debug, Clone, Copy)]
 pub struct RetrievalWeights {
     /// Similarity weight (default: 0.65)
     pub alpha: f32,
 
     /// Utility weight (default: 0.20)
     pub beta: f32,
 
     /// Class match weight (default: 0.10)
     pub gamma: f32,
 
     /// Duplicate penalty weight (default: 0.05)
     pub delta: f32,
 
     /// MMR lambda for diversity (default: 0.7, higher = more diversity)
     pub lambda: f32,
 }
 
 impl Default for RetrievalWeights {
     fn default() -> Self {
         Self {
             alpha: 0.65,
             beta: 0.20,
             gamma: 0.10,
             delta: 0.05,
             lambda: 0.7,
         }
     }
 }
 
 impl RetrievalWeights {
     /// Create new weights with validation
     pub fn new(alpha: f32, beta: f32, gamma: f32, delta: f32, lambda: f32) -> Self {
         assert!(alpha >= 0.0 && alpha <= 1.0, "alpha must be in [0, 1]");
         assert!(beta >= 0.0 && beta <= 1.0, "beta must be in [0, 1]");
         assert!(gamma >= 0.0 && gamma <= 1.0, "gamma must be in [0, 1]");
         assert!(delta >= 0.0 && delta <= 1.0, "delta must be in [0, 1]");
         assert!(lambda >= 0.0 && lambda <= 1.0, "lambda must be in [0, 1]");
 
-        Self { alpha, beta, gamma, delta, lambda }
+        Self {
+            alpha,
+            beta,
+            gamma,
+            delta,
+            lambda,
+        }
     }
 
     /// Normalize weights so alpha + beta + gamma + delta = 1.0
     pub fn normalize(&mut self) {
         let sum = self.alpha + self.beta + self.gamma + self.delta;
         if sum > 0.0 {
             self.alpha /= sum;
             self.beta /= sum;
             self.gamma /= sum;
             self.delta /= sum;
         }
     }
 }
 
 /// Rerank initial SoftIndex hits using hybrid scoring
 ///
 /// # Arguments
 /// * `hits` - Initial k-NN results from SoftIndex (id, similarity_score)
 /// * `weights` - Hybrid scoring weights
 /// * `entries` - Map from EntryId to DreamEntry for utility/class lookup
 /// * `query_class` - Optional class hint for class matching bonus
 ///
 /// # Returns
 /// * Vec<(EntryId, f32)> sorted by hybrid score (descending)
 pub fn rerank_hybrid(
@@ -92,106 +98,111 @@ pub fn rerank_hybrid(
     let sim_range = sim_max - sim_min;
 
     // Compute base hybrid scores (without diversity penalty)
     let mut scored: Vec<(EntryId, f32, f32)> = hits
         .iter()
         .filter_map(|(id, sim)| {
             entries.get(id).map(|entry| {
                 // Normalize similarity
                 let norm_sim = if sim_range > 1e-6 {
                     (sim - sim_min) / sim_range
                 } else {
                     1.0
                 };
 
                 // Utility score (normalized to [0, 1])
                 let utility = entry.util_mean.clamp(0.0, 1.0);
 
                 // Class match bonus
                 let class_match = match (query_class, entry.class_label) {
                     (Some(q), Some(e)) if q == e => 1.0,
                     (Some(_), Some(_)) => 0.0,
                     _ => 0.5, // Neutral if no class info
                 };
 
                 // Base score (before diversity penalty)
-                let base_score = weights.alpha * norm_sim
-                    + weights.beta * utility
-                    + weights.gamma * class_match;
+                let base_score =
+                    weights.alpha * norm_sim + weights.beta * utility + weights.gamma * class_match;
 
                 (*id, base_score, norm_sim)
             })
         })
         .collect();
 
     // Sort by base score descending
     scored.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
 
     // Apply MMR diversity penalty
     if weights.lambda > 0.0 {
         scored = apply_mmr_penalty(scored, weights.lambda, weights.delta, entries);
     }
 
     // Return final scores
-    scored.into_iter().map(|(id, score, _)| (id, score)).collect()
+    scored
+        .into_iter()
+        .map(|(id, score, _)| (id, score))
+        .collect()
 }
 
 /// Apply Maximum Marginal Relevance (MMR) penalty for diversity
 ///
 /// Iteratively selects entries, penalizing those too similar to already-selected ones.
 fn apply_mmr_penalty(
     candidates: Vec<(EntryId, f32, f32)>,
     lambda: f32,
     delta: f32,
     entries: &HashMap<EntryId, DreamEntry>,
 ) -> Vec<(EntryId, f32, f32)> {
     if candidates.len() <= 1 {
         return candidates;
     }
 
     let mut selected: Vec<(EntryId, f32, f32)> = Vec::new();
     let mut remaining = candidates;
 
     // Select first entry (highest base score)
     if let Some(first) = remaining.first() {
         selected.push(*first);
         remaining.remove(0);
     }
 
     // Iteratively select remaining entries with MMR penalty
     while !remaining.is_empty() {
         let mut best_idx = 0;
         let mut best_mmr_score = f32::NEG_INFINITY;
 
         for (idx, (id, base_score, _)) in remaining.iter().enumerate() {
             // Compute max similarity to already-selected entries
             let max_sim = selected
                 .iter()
                 .filter_map(|(sel_id, _, _)| {
                     let entry = entries.get(id)?;
                     let sel_entry = entries.get(sel_id)?;
-                    Some(chroma_similarity(&entry.chroma_signature, &sel_entry.chroma_signature))
+                    Some(chroma_similarity(
+                        &entry.chroma_signature,
+                        &sel_entry.chroma_signature,
+                    ))
                 })
                 .fold(0.0f32, |a, b| a.max(b));
 
             // MMR score: balance relevance and diversity
             // Higher lambda = more diversity, lower lambda = more relevance
             let mmr_score = lambda * base_score - delta * max_sim;
 
             if mmr_score > best_mmr_score {
                 best_mmr_score = mmr_score;
                 best_idx = idx;
             }
         }
 
         // Move best to selected
         let best = remaining.remove(best_idx);
         selected.push((best.0, best_mmr_score, best.2));
     }
 
     selected
 }
 
 /// Compute cosine similarity between two chromatic signatures
 #[inline]
 fn chroma_similarity(a: &[f32; 3], b: &[f32; 3]) -> f32 {
     let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
@@ -203,225 +214,248 @@ fn chroma_similarity(a: &[f32; 3], b: &[f32; 3]) -> f32 {
     }
 
     dot / (norm_a * norm_b)
 }
 
 /// Find min and max values in a slice
 fn min_max(values: &[f32]) -> (f32, f32) {
     let mut min = f32::INFINITY;
     let mut max = f32::NEG_INFINITY;
 
     for &v in values {
         if v < min {
             min = v;
         }
         if v > max {
             max = v;
         }
     }
 
     (min, max)
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
-    use crate::tensor::ChromaticTensor;
     use crate::solver::SolverResult;
+    use crate::tensor::ChromaticTensor;
 
-    fn mock_entry(id: EntryId, chroma: [f32; 3], utility: f32, class: Option<ColorClass>) -> (EntryId, DreamEntry) {
+    fn mock_entry(
+        id: EntryId,
+        chroma: [f32; 3],
+        utility: f32,
+        class: Option<ColorClass>,
+    ) -> (EntryId, DreamEntry) {
         let tensor = ChromaticTensor::new(4, 4, 3);
         let result = SolverResult {
             energy: 0.1,
             coherence: 0.8,
             violation: 0.05,
             grad: None,
             mask: None,
             meta: serde_json::json!({}),
         };
 
         // Create dummy spectral features for testing
         let spectral_features = crate::spectral::SpectralFeatures {
             entropy: 0.5,
             dominant_frequencies: [0, 0, 0],
             low_freq_energy: 0.33,
             mid_freq_energy: 0.33,
             high_freq_energy: 0.34,
             mean_psd: 1.0,
         };
 
         // Create UMS vector for testing (Phase 7)
         let ums_vector = vec![0.0f32; 512];
         let hue_category = 0usize;
 
-        (id, DreamEntry {
-            tensor,
-            result,
-            chroma_signature: chroma,
-            class_label: class,
-            utility: Some(utility),
-            timestamp: std::time::SystemTime::now(),
-            usage_count: 0,
-            spectral_features,
-            embed: None,
-            util_mean: utility,
-            ums_vector,
-            hue_category,
-        })
+        (
+            id,
+            DreamEntry {
+                tensor,
+                result,
+                chroma_signature: chroma,
+                class_label: class,
+                utility: Some(utility),
+                timestamp: std::time::SystemTime::now(),
+                usage_count: 0,
+                spectral_features,
+                embed: None,
+                util_mean: utility,
+                ums_vector,
+                hue_category,
+            },
+        )
     }
 
     #[test]
     fn test_weights_default() {
         let w = RetrievalWeights::default();
         assert!((w.alpha - 0.65).abs() < 0.01);
         assert!((w.beta - 0.20).abs() < 0.01);
         assert!((w.gamma - 0.10).abs() < 0.01);
         assert!((w.delta - 0.05).abs() < 0.01);
         assert!((w.lambda - 0.7).abs() < 0.01);
     }
 
     #[test]
     fn test_weights_normalize() {
         let mut w = RetrievalWeights::new(0.5, 0.3, 0.1, 0.1, 0.7);
         w.normalize();
 
         let sum = w.alpha + w.beta + w.gamma + w.delta;
         assert!((sum - 1.0).abs() < 0.01);
     }
 
     #[test]
     fn test_rerank_hybrid_basic() {
         let id1 = EntryId::new_v4();
         let id2 = EntryId::new_v4();
         let id3 = EntryId::new_v4();
 
         let mut entries = HashMap::new();
-        entries.insert(id1, mock_entry(id1, [1.0, 0.0, 0.0], 0.9, Some(ColorClass::Red)).1);
-        entries.insert(id2, mock_entry(id2, [0.0, 1.0, 0.0], 0.5, Some(ColorClass::Green)).1);
-        entries.insert(id3, mock_entry(id3, [0.0, 0.0, 1.0], 0.3, Some(ColorClass::Blue)).1);
+        entries.insert(
+            id1,
+            mock_entry(id1, [1.0, 0.0, 0.0], 0.9, Some(ColorClass::Red)).1,
+        );
+        entries.insert(
+            id2,
+            mock_entry(id2, [0.0, 1.0, 0.0], 0.5, Some(ColorClass::Green)).1,
+        );
+        entries.insert(
+            id3,
+            mock_entry(id3, [0.0, 0.0, 1.0], 0.3, Some(ColorClass::Blue)).1,
+        );
 
         let hits = vec![
             (id1, 0.95), // High similarity
             (id2, 0.80), // Medium similarity
             (id3, 0.60), // Low similarity
         ];
 
         let weights = RetrievalWeights::default();
         let results = rerank_hybrid(&hits, &weights, &entries, Some(ColorClass::Red));
 
         assert_eq!(results.len(), 3);
         assert_eq!(results[0].0, id1); // Best: high sim + high utility + class match
     }
 
     #[test]
     fn test_rerank_utility_boost() {
         let id1 = EntryId::new_v4();
         let id2 = EntryId::new_v4();
 
         let mut entries = HashMap::new();
         entries.insert(id1, mock_entry(id1, [1.0, 0.0, 0.0], 0.3, None).1); // Low utility
         entries.insert(id2, mock_entry(id2, [0.9, 0.0, 0.0], 0.9, None).1); // High utility
 
         let hits = vec![
-            (id1, 1.0), // Slightly higher similarity
+            (id1, 1.0),  // Slightly higher similarity
             (id2, 0.95), // Slightly lower similarity
         ];
 
         let weights = RetrievalWeights {
             alpha: 0.3, // Lower similarity weight
             beta: 0.7,  // Much higher utility weight
             gamma: 0.0,
             delta: 0.0,
             lambda: 0.0,
         };
 
         let results = rerank_hybrid(&hits, &weights, &entries, None);
 
         // id2 should win due to much higher utility
         assert_eq!(results[0].0, id2);
     }
 
     #[test]
     fn test_rerank_class_match() {
         let id1 = EntryId::new_v4();
         let id2 = EntryId::new_v4();
 
         let mut entries = HashMap::new();
-        entries.insert(id1, mock_entry(id1, [1.0, 0.0, 0.0], 0.5, Some(ColorClass::Red)).1);
-        entries.insert(id2, mock_entry(id2, [1.0, 0.0, 0.0], 0.5, Some(ColorClass::Blue)).1);
+        entries.insert(
+            id1,
+            mock_entry(id1, [1.0, 0.0, 0.0], 0.5, Some(ColorClass::Red)).1,
+        );
+        entries.insert(
+            id2,
+            mock_entry(id2, [1.0, 0.0, 0.0], 0.5, Some(ColorClass::Blue)).1,
+        );
 
-        let hits = vec![
-            (id1, 0.9),
-            (id2, 0.9),
-        ];
+        let hits = vec![(id1, 0.9), (id2, 0.9)];
 
         let weights = RetrievalWeights {
             alpha: 0.3,
             beta: 0.3,
             gamma: 0.4, // High class match weight
             delta: 0.0,
             lambda: 0.0,
         };
 
         let results = rerank_hybrid(&hits, &weights, &entries, Some(ColorClass::Red));
 
         // id1 should win due to class match
         assert_eq!(results[0].0, id1);
     }
 
     #[test]
     fn test_mmr_diversity() {
         let id1 = EntryId::new_v4();
         let id2 = EntryId::new_v4();
         let id3 = EntryId::new_v4();
 
         let mut entries = HashMap::new();
         entries.insert(id1, mock_entry(id1, [1.0, 0.0, 0.0], 0.8, None).1);
         entries.insert(id2, mock_entry(id2, [0.98, 0.02, 0.0], 0.78, None).1); // Very similar to id1
         entries.insert(id3, mock_entry(id3, [0.0, 1.0, 0.0], 0.76, None).1); // Different
 
         let hits = vec![
             (id1, 0.95),
             (id2, 0.94), // Similar to id1, slightly lower sim
             (id3, 0.93), // Different, slightly lower sim
         ];
 
         let weights = RetrievalWeights {
             alpha: 0.5,
             beta: 0.5,
             gamma: 0.0,
             delta: 0.5,  // Higher penalty for duplicates
             lambda: 0.9, // Very high diversity preference
         };
 
         let results = rerank_hybrid(&hits, &weights, &entries, None);
 
         // With high lambda, id3 should rank higher than id2 despite similar base scores
         assert_eq!(results[0].0, id1); // Best overall
 
         // Check that id3 ranks higher than id2 due to diversity
         let id2_rank = results.iter().position(|(id, _)| *id == id2).unwrap();
         let id3_rank = results.iter().position(|(id, _)| *id == id3).unwrap();
-        assert!(id3_rank < id2_rank, "id3 should rank higher than id2 due to diversity");
+        assert!(
+            id3_rank < id2_rank,
+            "id3 should rank higher than id2 due to diversity"
+        );
     }
 
     #[test]
     fn test_chroma_similarity() {
         let a = [1.0, 0.0, 0.0];
         let b = [1.0, 0.0, 0.0];
         let sim = chroma_similarity(&a, &b);
         assert!((sim - 1.0).abs() < 0.01); // Identical
 
         let c = [0.0, 1.0, 0.0];
         let sim = chroma_similarity(&a, &c);
         assert!((sim - 0.0).abs() < 0.01); // Orthogonal
     }
 
     #[test]
     fn test_empty_hits() {
         let entries = HashMap::new();
         let hits = vec![];
         let weights = RetrievalWeights::default();
         let results = rerank_hybrid(&hits, &weights, &entries, None);
         assert!(results.is_empty());
     }
 }
diff --git a/src/dream/memory.rs b/src/dream/memory.rs
index 9355df884173ca7cf9c3ad46803ec9ea6a66837a..87d1cb7e266e4b33784d8ec78d1f4672cb358cb3 100644
--- a/src/dream/memory.rs
+++ b/src/dream/memory.rs
@@ -1,61 +1,73 @@
 //! Memory budget tracking and management for dream pool
 //!
 //! This module implements memory usage tracking and budget enforcement
 //! to prevent unbounded memory growth in large dream pools.
 
 use crate::dream::simple_pool::DreamEntry;
+use serde::{Deserialize, Serialize};
+use std::convert::TryFrom;
 use std::mem;
 
 /// Memory budget tracker for dream pool
 ///
 /// Tracks current memory usage and enforces a maximum budget.
 /// Triggers eviction when usage exceeds threshold.
 ///
 /// # Example
 ///
 /// ```ignore
 /// let mut budget = MemoryBudget::new(100); // 100 MB limit
 /// budget.add_entry(entry_size);
 /// if budget.needs_eviction() {
 ///     // Evict entries
 /// }
 /// ```
 #[derive(Debug, Clone)]
 pub struct MemoryBudget {
     /// Maximum memory budget in bytes
     max_bytes: usize,
     /// Current memory usage in bytes (raw, without ANN overhead adjustment)
     current_bytes: usize,
     /// Number of entries currently tracked
     entry_count: usize,
     /// Eviction threshold (0.0-1.0), triggers at this fraction of max_bytes
     eviction_threshold: f32,
     /// Additional multiplier applied when ANN indexes mirror entry memory usage (e.g. HNSW ≈ 2×)
     ann_overhead_factor: f32,
 }
 
+/// Serializable representation of [`MemoryBudget`].
+#[derive(Clone, Serialize, Deserialize)]
+pub(crate) struct MemoryBudgetSnapshot {
+    max_bytes: u64,
+    current_bytes: u64,
+    entry_count: u64,
+    eviction_threshold: f32,
+    ann_overhead_factor: f32,
+}
+
 impl MemoryBudget {
     /// Create a new memory budget with given limit in megabytes
     ///
     /// # Arguments
     ///
     /// * `max_mb` - Maximum memory budget in megabytes
     ///
     /// # Example
     ///
     /// ```
     /// # use chromatic_cognition_core::dream::memory::MemoryBudget;
     /// let budget = MemoryBudget::new(100); // 100 MB limit
     /// ```
     pub fn new(max_mb: usize) -> Self {
         Self {
             max_bytes: max_mb * 1024 * 1024,
             current_bytes: 0,
             entry_count: 0,
             eviction_threshold: 0.9, // Trigger at 90%
             ann_overhead_factor: 1.0,
         }
     }
 
     /// Create a memory budget with custom eviction threshold
     ///
@@ -191,50 +203,91 @@ impl MemoryBudget {
     /// Get number of entries tracked
     pub fn entry_count(&self) -> usize {
         self.entry_count
     }
 
     /// Calculate required eviction count to return below threshold.
     ///
     /// Returns zero when the average entry size is unknown or the budget is already satisfied.
     pub fn calculate_eviction_count(&self, avg_entry_size: usize) -> usize {
         if avg_entry_size == 0 || self.entry_count == 0 {
             return 0;
         }
 
         let threshold_bytes = self.threshold_bytes();
         let usage = self.adjusted_usage_bytes();
 
         if usage <= threshold_bytes {
             return 0;
         }
 
         let bytes_to_free = usage - threshold_bytes;
         let count = (bytes_to_free + avg_entry_size - 1) / avg_entry_size;
         count.min(self.entry_count)
     }
 
+    /// Capture the full budget state for checkpointing.
+    pub(crate) fn snapshot(&self) -> MemoryBudgetSnapshot {
+        MemoryBudgetSnapshot {
+            max_bytes: u64::try_from(self.max_bytes).unwrap_or(u64::MAX),
+            current_bytes: u64::try_from(self.current_bytes).unwrap_or(u64::MAX),
+            entry_count: u64::try_from(self.entry_count).unwrap_or(u64::MAX),
+            eviction_threshold: self.eviction_threshold,
+            ann_overhead_factor: self.ann_overhead_factor,
+        }
+    }
+
+    /// Restore a budget instance from a previously captured snapshot.
+    pub(crate) fn from_snapshot(snapshot: MemoryBudgetSnapshot) -> Result<Self, String> {
+        let max_bytes = usize::try_from(snapshot.max_bytes)
+            .map_err(|_| "Memory budget max_bytes exceeds platform capacity".to_string())?;
+        let current_bytes = usize::try_from(snapshot.current_bytes)
+            .map_err(|_| "Memory budget current_bytes exceeds platform capacity".to_string())?;
+        let entry_count = usize::try_from(snapshot.entry_count)
+            .map_err(|_| "Memory budget entry_count exceeds platform capacity".to_string())?;
+
+        if !(0.0..=1.0).contains(&snapshot.eviction_threshold) {
+            return Err("Memory budget eviction_threshold out of range".to_string());
+        }
+
+        if !(1.0..=8.0).contains(&snapshot.ann_overhead_factor) {
+            return Err("Memory budget ann_overhead_factor out of range".to_string());
+        }
+
+        if current_bytes > max_bytes {
+            return Err("Memory budget current_bytes exceeds max_bytes".to_string());
+        }
+
+        Ok(Self {
+            max_bytes,
+            current_bytes,
+            entry_count,
+            eviction_threshold: snapshot.eviction_threshold,
+            ann_overhead_factor: snapshot.ann_overhead_factor,
+        })
+    }
+
     /// Get average entry size in bytes
     pub fn average_entry_size(&self) -> usize {
         if self.entry_count == 0 {
             0
         } else {
             self.current_bytes / self.entry_count
         }
     }
 
     /// Reset the budget tracker
     pub fn reset(&mut self) {
         self.current_bytes = 0;
         self.entry_count = 0;
     }
 
     /// Get memory statistics as a formatted string
     pub fn stats(&self) -> String {
         let adjusted_mb = self.adjusted_usage_bytes() as f64 / (1024.0 * 1024.0);
         format!(
             "Memory: {:.2} / {:.2} MB ({:.1}%), {} entries, avg {:.2} KB/entry, overhead x{:.1}",
             adjusted_mb,
             self.max_bytes as f64 / (1024.0 * 1024.0),
             self.usage_ratio() * 100.0,
             self.entry_count,
             self.average_entry_size() as f64 / 1024.0,
diff --git a/src/dream/prelude.rs b/src/dream/prelude.rs
index 8d4300d4e11636e142ebb4ee7696747818ccb3a0..c0b07743245e0262f4efe5a843a584fbc0c50f67 100644
--- a/src/dream/prelude.rs
+++ b/src/dream/prelude.rs
@@ -39,27 +39,27 @@
 //! use chromatic_cognition_core::dream::soft_index::SoftIndex;
 //! use chromatic_cognition_core::dream::hybrid_scoring::rerank_hybrid;
 //!
 //! // Analysis tools
 //! use chromatic_cognition_core::dream::analysis::compare_experiments;
 //!
 //! // Diversity functions
 //! use chromatic_cognition_core::dream::diversity::retrieve_diverse_mmr;
 //! ```
 
 // Re-export the most commonly used types
 
 // Core pool types
 pub use crate::dream::simple_pool::{DreamEntry, PoolConfig, SimpleDreamPool};
 
 // Phase 4: Soft retrieval essentials
 pub use crate::dream::embedding::EmbeddingMapper;
 pub use crate::dream::hybrid_scoring::RetrievalWeights;
 pub use crate::dream::retrieval_mode::RetrievalMode;
 
 // Phase 3B: Essential for experiments
 pub use crate::dream::bias::BiasProfile;
 pub use crate::dream::experiment::ExperimentHarness;
 
 // Commonly used enums
-pub use crate::dream::soft_index::Similarity;
 pub use crate::data::ColorClass;
+pub use crate::dream::soft_index::Similarity;
diff --git a/src/dream/query_cache.rs b/src/dream/query_cache.rs
index 7a73a7dcd761dc97e1b581a309b8a96dd1cc766e..e4259cfbb78a1a8dbba00263341631983f6a2abb 100644
--- a/src/dream/query_cache.rs
+++ b/src/dream/query_cache.rs
@@ -246,39 +246,35 @@ mod tests {
 
     #[test]
     fn test_clear() {
         let mut cache = QueryCache::new(16);
         let mapper = create_test_mapper();
 
         cache.get_or_compute(&[1.0, 0.0, 0.0], |q| encode_rgb(&mapper, q));
         cache.get_or_compute(&[1.0, 0.0, 0.0], |q| encode_rgb(&mapper, q));
 
         assert_eq!(cache.len(), 1);
         assert_eq!(cache.hits(), 1);
 
         cache.clear();
 
         assert_eq!(cache.len(), 0);
         assert_eq!(cache.hits(), 0);
         assert_eq!(cache.misses(), 0);
     }
 
     #[test]
     fn test_precision_tolerance() {
         let mut cache = QueryCache::new(16);
         let mapper = create_test_mapper();
 
         // First query
-        let embed1 = cache.get_or_compute(&[0.856, 0.234, 0.112], |q| {
-            encode_rgb(&mapper, q)
-        });
+        let embed1 = cache.get_or_compute(&[0.856, 0.234, 0.112], |q| encode_rgb(&mapper, q));
 
         // Second query with minor floating-point variation (within 0.001)
-        let embed2 = cache.get_or_compute(&[0.8564, 0.2336, 0.1124], |q| {
-            encode_rgb(&mapper, q)
-        });
+        let embed2 = cache.get_or_compute(&[0.8564, 0.2336, 0.1124], |q| encode_rgb(&mapper, q));
 
         // Should be cache hit (same after rounding)
         assert_eq!(cache.hits(), 1);
         assert_eq!(embed1, embed2);
     }
 }
diff --git a/src/dream/retrieval_mode.rs b/src/dream/retrieval_mode.rs
index 458f2285781f797fd7311ce36fba878967b6acc1..0d1a024efae96c5f4b29f5d6f3c44a1d4f6634af 100644
--- a/src/dream/retrieval_mode.rs
+++ b/src/dream/retrieval_mode.rs
@@ -1,30 +1,30 @@
 //! Retrieval mode configuration for Phase 4 training integration.
 //!
 //! Defines how the dream pool retrieval should operate during training.
 
-use serde::{Serialize, Deserialize};
+use serde::{Deserialize, Serialize};
 
 /// Retrieval mode for dream pool queries
 #[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
 pub enum RetrievalMode {
     /// Hard retrieval (Phase 3B): Class-aware cosine similarity on RGB
     /// Uses only chromatic signatures and optional class filtering
     Hard,
 
     /// Soft retrieval (Phase 4): Continuous embedding with ANN
     /// Uses EmbeddingMapper + SoftIndex + hybrid scoring
     Soft,
 
     /// Hybrid mode: Combine hard and soft retrieval results
     /// Retrieves from both methods and merges with deduplication
     Hybrid,
 }
 
 impl Default for RetrievalMode {
     fn default() -> Self {
         RetrievalMode::Hard
     }
 }
 
 impl RetrievalMode {
     /// Check if this mode requires soft index (Phase 4)
diff --git a/src/dream/simple_pool.rs b/src/dream/simple_pool.rs
index 7a890438a07724ccd90eb41c1121d949b2a24c6c..8d5dd5126680c2a6b051367478ca25721334440b 100644
--- a/src/dream/simple_pool.rs
+++ b/src/dream/simple_pool.rs
@@ -1,93 +1,137 @@
 //! SimpleDreamPool - In-memory dream storage with cosine similarity retrieval
 //!
 //! This is a minimal implementation designed for validation experiments.
 //! It stores ChromaticTensor dreams with their evaluation metrics and provides
 //! retrieval based on chromatic signature similarity.
 
 use crate::bridge::{encode_to_ums, ModalityMapper};
 use crate::checkpoint::{CheckpointError, Checkpointable};
 use crate::config::{
     BridgeBaseConfig, BridgeConfig, BridgeReversibilityConfig, BridgeSpectralConfig,
 };
 use crate::data::ColorClass;
 use crate::dream::embedding::{EmbeddingMapper, QuerySignature};
 use crate::dream::error::{DreamError, DreamResult};
-use crate::dream::hnsw_index::HnswIndex;
+use crate::dream::hnsw_index::{HnswIndex, HnswIndexSnapshot};
 use crate::dream::hybrid_scoring::{rerank_hybrid, RetrievalWeights};
-use crate::dream::memory::{estimate_entry_size, MemoryBudget};
+use crate::dream::memory::{estimate_entry_size, MemoryBudget, MemoryBudgetSnapshot};
 use crate::dream::query_cache::QueryCache;
-use crate::dream::soft_index::{EntryId, Similarity, SoftIndex};
+use crate::dream::soft_index::{EntryId, Similarity, SoftIndex, SoftIndexSnapshot};
 use crate::solver::SolverResult;
 use crate::spectral::{extract_spectral_features, SpectralFeatures, WindowFunction};
 use crate::tensor::ChromaticTensor;
+use ndarray::{Array3, Array4};
 use serde::{Deserialize, Serialize};
+use serde_json;
 use std::cmp::Ordering;
-use std::collections::{HashMap, VecDeque};
-use std::time::SystemTime;
+use std::collections::{HashMap, HashSet, VecDeque};
+use std::convert::TryFrom;
+use std::time::{Duration, SystemTime, UNIX_EPOCH};
 
-const POOL_CHECKPOINT_VERSION: u32 = 1;
+const POOL_CHECKPOINT_VERSION: u32 = 2;
 
 #[derive(Serialize, Deserialize)]
 struct SimpleDreamPoolCheckpoint {
     version: u32,
     config: PoolConfig,
-    entries: Vec<DreamEntry>,
+    entries: Vec<DreamEntryCheckpoint>,
     entry_ids: Vec<EntryId>,
+    id_map: Vec<(EntryId, usize)>,
+    soft_index: Option<SoftIndexSnapshot>,
+    hnsw_index: Option<HnswIndexSnapshot>,
+    memory_budget: Option<MemoryBudgetSnapshot>,
+    evictions_since_rebuild: usize,
 }
 
 /// A stored dream entry with tensor and evaluation metrics
 ///
 /// Enhanced for Phase 3B with class awareness, utility tracking, and timestamps
 /// Enhanced for Phase 4 with spectral features and embeddings
 /// Enhanced for Phase 7 (Phase 2 Cognitive Integration) with UMS vector
 #[derive(Clone, Serialize, Deserialize)]
 pub struct DreamEntry {
     pub tensor: ChromaticTensor,
     pub result: SolverResult,
     pub chroma_signature: [f32; 3],
     /// Optional class label for class-aware retrieval (Phase 3B)
     pub class_label: Option<ColorClass>,
     /// Utility score from feedback (Phase 3B)
     pub utility: Option<f32>,
     /// Timestamp for recency tracking (Phase 3B)
     pub timestamp: SystemTime,
     /// Number of times this dream has been retrieved (Phase 3B)
     pub usage_count: usize,
     /// Spectral features for embedding (Phase 4) - Always computed on creation
     pub spectral_features: SpectralFeatures,
     /// Cached embedding vector (Phase 4)
     pub embed: Option<Vec<f32>>,
     /// Aggregated mean utility (Phase 4)
     pub util_mean: f32,
     /// Unified Modality Space vector (Phase 7 / Phase 2 Cognitive Integration)
     /// 512D deterministic encoding for Chromatic Semantic Archive (CSA)
     pub ums_vector: Vec<f32>,
     /// Hue category index [0-11] for CSA partitioning (Phase 7)
     pub hue_category: usize,
 }
 
+#[derive(Serialize, Deserialize)]
+struct ChromaticTensorCheckpoint {
+    colors: Vec<f32>,
+    certainty: Vec<f32>,
+    rows: usize,
+    cols: usize,
+    layers: usize,
+}
+
+#[derive(Serialize, Deserialize)]
+struct DreamEntryCheckpoint {
+    tensor: ChromaticTensorCheckpoint,
+    result: SolverResultCheckpoint,
+    chroma_signature: [f32; 3],
+    class_label: Option<ColorClass>,
+    utility: Option<f32>,
+    timestamp_secs: i128,
+    timestamp_nanos: u32,
+    usage_count: usize,
+    spectral_features: SpectralFeatures,
+    embed: Option<Vec<f32>>,
+    util_mean: f32,
+    ums_vector: Vec<f32>,
+    hue_category: usize,
+}
+
+#[derive(Serialize, Deserialize)]
+struct SolverResultCheckpoint {
+    energy: f64,
+    coherence: f64,
+    violation: f64,
+    grad: Option<Vec<f32>>,
+    mask: Option<Vec<f32>>,
+    meta_json: Vec<u8>,
+}
+
 impl DreamEntry {
     /// Create a new dream entry from a tensor and its evaluation result
     ///
     /// Spectral features are computed immediately using Hann windowing.
     /// This one-time computation enables faster embedding generation later.
     ///
     /// Phase 7: UMS vector is computed immediately for Chromatic Semantic Archive.
     pub fn new(tensor: ChromaticTensor, result: SolverResult) -> Self {
         let chroma_signature = tensor.mean_rgb();
         let spectral_features = extract_spectral_features(&tensor, WindowFunction::Hann);
 
         // Phase 7: Compute UMS vector and hue category for CSA
         let mapper = Self::default_modality_mapper();
         let ums = encode_to_ums(&mapper, &tensor);
         let ums_vector = ums.components().to_vec();
 
         // Extract hue from chroma signature and map to category
         let rgb = chroma_signature;
         let hue_radians = Self::rgb_to_hue(rgb);
         let hue_category = mapper.map_hue_to_category(hue_radians);
 
         Self {
             tensor,
             result,
             chroma_signature,
@@ -197,50 +241,217 @@ impl DreamEntry {
         if delta < f32::EPSILON {
             return 0.0; // Achromatic (gray)
         }
 
         let hue_deg = if (max - r).abs() < f32::EPSILON {
             // Red is max
             60.0 * (((g - b) / delta) % 6.0)
         } else if (max - g).abs() < f32::EPSILON {
             // Green is max
             60.0 * (((b - r) / delta) + 2.0)
         } else {
             // Blue is max
             60.0 * (((r - g) / delta) + 4.0)
         };
 
         // Convert to radians and ensure positive
         let hue_rad = hue_deg.to_radians();
         if hue_rad < 0.0 {
             hue_rad + std::f32::consts::TAU
         } else {
             hue_rad
         }
     }
 }
 
+fn encode_system_time(ts: SystemTime) -> (i128, u32) {
+    match ts.duration_since(UNIX_EPOCH) {
+        Ok(duration) => (duration.as_secs() as i128, duration.subsec_nanos()),
+        Err(err) => {
+            let duration = err.duration();
+            (-(duration.as_secs() as i128), duration.subsec_nanos())
+        }
+    }
+}
+
+fn decode_system_time(secs: i128, nanos: u32) -> Result<SystemTime, CheckpointError> {
+    if nanos >= 1_000_000_000 {
+        return Err(CheckpointError::InvalidFormat(
+            "Timestamp nanoseconds out of range".to_string(),
+        ));
+    }
+
+    if secs >= 0 {
+        let secs_u64 = u64::try_from(secs).map_err(|_| {
+            CheckpointError::InvalidFormat("Timestamp seconds overflow".to_string())
+        })?;
+        Ok(UNIX_EPOCH + Duration::new(secs_u64, nanos))
+    } else {
+        if secs == i128::MIN {
+            return Err(CheckpointError::InvalidFormat(
+                "Timestamp seconds underflow".to_string(),
+            ));
+        }
+        let abs_secs = (-secs) as i128;
+        let abs_secs_u64 = u64::try_from(abs_secs).map_err(|_| {
+            CheckpointError::InvalidFormat("Timestamp seconds overflow".to_string())
+        })?;
+        let duration = Duration::new(abs_secs_u64, nanos);
+        Ok(UNIX_EPOCH - duration)
+    }
+}
+
+impl DreamEntryCheckpoint {
+    fn capture(entry: &DreamEntry) -> Result<Self, CheckpointError> {
+        let (timestamp_secs, timestamp_nanos) = encode_system_time(entry.timestamp);
+        Ok(Self {
+            tensor: ChromaticTensorCheckpoint::from(&entry.tensor),
+            result: SolverResultCheckpoint::capture(&entry.result)?,
+            chroma_signature: entry.chroma_signature,
+            class_label: entry.class_label,
+            utility: entry.utility,
+            timestamp_secs,
+            timestamp_nanos,
+            usage_count: entry.usage_count,
+            spectral_features: entry.spectral_features.clone(),
+            embed: entry.embed.clone(),
+            util_mean: entry.util_mean,
+            ums_vector: entry.ums_vector.clone(),
+            hue_category: entry.hue_category,
+        })
+    }
+
+    fn into_entry(self) -> Result<DreamEntry, CheckpointError> {
+        let timestamp = decode_system_time(self.timestamp_secs, self.timestamp_nanos)?;
+        Ok(DreamEntry {
+            tensor: self.tensor.into_tensor()?,
+            result: self.result.into_result()?,
+            chroma_signature: self.chroma_signature,
+            class_label: self.class_label,
+            utility: self.utility,
+            timestamp,
+            usage_count: self.usage_count,
+            spectral_features: self.spectral_features,
+            embed: self.embed,
+            util_mean: self.util_mean,
+            ums_vector: self.ums_vector,
+            hue_category: self.hue_category,
+        })
+    }
+}
+
+impl SolverResultCheckpoint {
+    fn capture(result: &SolverResult) -> Result<Self, CheckpointError> {
+        let meta_json = serde_json::to_vec(&result.meta).map_err(|err| {
+            CheckpointError::InvalidFormat(format!(
+                "Failed to serialize solver metadata for checkpoint: {err}"
+            ))
+        })?;
+
+        Ok(Self {
+            energy: result.energy,
+            coherence: result.coherence,
+            violation: result.violation,
+            grad: result.grad.clone(),
+            mask: result.mask.clone(),
+            meta_json,
+        })
+    }
+
+    fn into_result(self) -> Result<SolverResult, CheckpointError> {
+        let meta = serde_json::from_slice(&self.meta_json).map_err(|err| {
+            CheckpointError::InvalidFormat(format!(
+                "Failed to deserialize solver metadata from checkpoint: {err}"
+            ))
+        })?;
+
+        Ok(SolverResult {
+            energy: self.energy,
+            coherence: self.coherence,
+            violation: self.violation,
+            grad: self.grad,
+            mask: self.mask,
+            meta,
+        })
+    }
+}
+
+impl From<&ChromaticTensor> for ChromaticTensorCheckpoint {
+    fn from(tensor: &ChromaticTensor) -> Self {
+        let (rows, cols, layers, _) = tensor.colors.dim();
+        Self {
+            colors: tensor.colors.iter().cloned().collect(),
+            certainty: tensor.certainty.iter().cloned().collect(),
+            rows,
+            cols,
+            layers,
+        }
+    }
+}
+
+impl ChromaticTensorCheckpoint {
+    fn into_tensor(self) -> Result<ChromaticTensor, CheckpointError> {
+        let color_len = self
+            .rows
+            .checked_mul(self.cols)
+            .and_then(|v| v.checked_mul(self.layers))
+            .and_then(|v| v.checked_mul(3))
+            .ok_or_else(|| {
+                CheckpointError::InvalidFormat("Tensor dimensions overflow".to_string())
+            })?;
+        if self.colors.len() != color_len {
+            return Err(CheckpointError::InvalidFormat(
+                "Color tensor length mismatch".to_string(),
+            ));
+        }
+
+        let certainty_len = self
+            .rows
+            .checked_mul(self.cols)
+            .and_then(|v| v.checked_mul(self.layers))
+            .ok_or_else(|| {
+                CheckpointError::InvalidFormat("Certainty tensor dimensions overflow".to_string())
+            })?;
+        if self.certainty.len() != certainty_len {
+            return Err(CheckpointError::InvalidFormat(
+                "Certainty tensor length mismatch".to_string(),
+            ));
+        }
+
+        let colors = Array4::from_shape_vec((self.rows, self.cols, self.layers, 3), self.colors)
+            .map_err(|_| {
+                CheckpointError::InvalidFormat("Invalid color tensor shape".to_string())
+            })?;
+        let certainty = Array3::from_shape_vec((self.rows, self.cols, self.layers), self.certainty)
+            .map_err(|_| {
+                CheckpointError::InvalidFormat("Invalid certainty tensor shape".to_string())
+            })?;
+
+        Ok(ChromaticTensor::from_arrays(colors, certainty))
+    }
+}
+
 /// Configuration for SimpleDreamPool
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct PoolConfig {
     /// Maximum number of dreams to store in memory
     pub max_size: usize,
     /// Minimum coherence threshold for persistence (0.0-1.0)
     pub coherence_threshold: f64,
     /// Number of similar dreams to retrieve
     pub retrieval_limit: usize,
     /// Use HNSW index for scalable retrieval (Phase 4 optimization)
     /// When false, uses linear SoftIndex (simpler but O(n) search)
     /// Enable this when the pool regularly exceeds ~5,000 entries or when
     /// retrieval latency must remain under 100 ms.
     pub use_hnsw: bool,
     /// Memory budget in megabytes (Phase 4 optimization)
     /// When None, no memory limit is enforced (legacy behavior)
     /// When Some(mb), triggers automatic eviction at 90% of limit
     pub memory_budget_mb: Option<usize>,
 }
 
 impl Default for PoolConfig {
     fn default() -> Self {
         Self {
             max_size: 1000,
             coherence_threshold: 0.75,
@@ -338,123 +549,50 @@ impl SimpleDreamPool {
                         gamma: 1.0,
                         sample_rate: 44_100,
                     },
                     spectral: BridgeSpectralConfig {
                         fft_size: 4096,
                         accum_format: "Q16.48".to_string(),
                         reduction_mode: "pairwise_neumaier".to_string(),
                         categorical_count: 12,
                     },
                     reversibility: BridgeReversibilityConfig {
                         delta_e_tolerance: 0.001,
                     },
                 }
             });
 
         ModalityMapper::new(bridge_config)
     }
 
     fn attach_semantic_embedding(&self, entry: &mut DreamEntry) -> Vec<f32> {
         let ums = encode_to_ums(&self.modality_mapper, &entry.tensor);
         let embedding = ums.components().to_vec();
         entry.embed = Some(embedding.clone());
         embedding
     }
 
-    fn rebuild_after_restore(&mut self) -> Result<(), CheckpointError> {
-        self.id_to_entry.clear();
-
-        if self.entries.is_empty() {
-            self.soft_index = None;
-            if let Some(index) = self.hnsw_index.as_mut() {
-                index.clear();
-            }
-            self.evictions_since_rebuild = 0;
-            return Ok(());
-        }
-
-        let mut embeddings: Vec<(EntryId, Vec<f32>)> = Vec::with_capacity(self.entries.len());
-        let mut expected_dim: Option<usize> = None;
-
-        for (entry_id, entry) in self.entry_ids.iter().copied().zip(self.entries.iter_mut()) {
-            let embedding = if let Some(existing) = entry.embed.clone() {
-                existing
-            } else {
-                let ums = encode_to_ums(&self.modality_mapper, &entry.tensor);
-                let generated = ums.components().to_vec();
-                entry.embed = Some(generated.clone());
-                generated
-            };
-
-            if let Some(dim) = expected_dim {
-                if embedding.len() != dim {
-                    return Err(CheckpointError::InvalidFormat(format!(
-                        "Inconsistent embedding dimension for entry {entry_id}"
-                    )));
-                }
-            } else {
-                expected_dim = Some(embedding.len());
-            }
-
-            embeddings.push((entry_id, embedding));
-            self.id_to_entry.insert(entry_id, entry.clone());
-        }
-
-        if let Some(dim) = expected_dim {
-            let mut index = SoftIndex::new(dim);
-            for (id, embedding) in embeddings {
-                index.add(id, embedding)?;
-            }
-            index.build();
-            self.soft_index = Some(index);
-        } else {
-            self.soft_index = None;
-        }
-
-        if self.config.use_hnsw {
-            self.rebuild_semantic_index_internal()?;
-        } else {
-            self.hnsw_index = None;
-        }
-
-        if let Some(mb) = self.config.memory_budget_mb {
-            let mut budget = MemoryBudget::new(mb);
-            if self.config.use_hnsw {
-                budget.set_ann_overhead_factor(2.0);
-            }
-            for entry in self.entries.iter() {
-                budget.add_entry(estimate_entry_size(entry));
-            }
-            self.memory_budget = Some(budget);
-        } else {
-            self.memory_budget = None;
-        }
-
-        self.evictions_since_rebuild = 0;
-        Ok(())
-    }
-
     fn rebuild_semantic_index_internal(&mut self) -> DreamResult<()> {
         if self.entries.is_empty() {
             if let Some(index) = self.hnsw_index.as_mut() {
                 index.clear();
             }
             return Ok(());
         }
 
         let dim = self
             .entries
             .front()
             .and_then(|entry| entry.embed.as_ref().map(|vec| vec.len()))
             .or_else(|| {
                 self.entries.front().map(|entry| {
                     encode_to_ums(&self.modality_mapper, &entry.tensor)
                         .components()
                         .len()
                 })
             })
             .unwrap_or(0);
 
         if dim == 0 {
             return Err(DreamError::index_corrupted(
                 "Semantic embedding dimension resolved to zero during HNSW rebuild",
             ));
@@ -1366,79 +1504,170 @@ impl SimpleDreamPool {
         // Return top-K entries
         scored
             .into_iter()
             .take(k)
             .map(|(_, entry)| entry.clone())
             .collect()
     }
 
     /// Check if any index is built (Phase 4)
     pub fn has_soft_index(&self) -> bool {
         self.soft_index.is_some() || self.hnsw_index.is_some()
     }
 
     /// Get number of entries in the active index (Phase 4)
     pub fn soft_index_size(&self) -> usize {
         if let Some(hnsw) = &self.hnsw_index {
             hnsw.len()
         } else {
             self.soft_index.as_ref().map_or(0, |idx| idx.len())
         }
     }
 }
 
 impl Checkpointable for SimpleDreamPool {
     fn save_checkpoint<P: AsRef<std::path::Path>>(&self, path: P) -> Result<(), CheckpointError> {
+        let entries: Vec<DreamEntryCheckpoint> = self
+            .entries
+            .iter()
+            .map(DreamEntryCheckpoint::capture)
+            .collect::<Result<_, _>>()?;
+
+        let mut id_map: Vec<(EntryId, usize)> = self
+            .entry_ids
+            .iter()
+            .copied()
+            .enumerate()
+            .map(|(idx, entry_id)| (entry_id, idx))
+            .collect();
+        id_map.sort_by_key(|(entry_id, _)| entry_id.as_u128());
+
+        let soft_index = self.soft_index.as_ref().map(|index| index.snapshot());
+        let hnsw_index = if let Some(index) = &self.hnsw_index {
+            Some(index.snapshot()?)
+        } else {
+            None
+        };
+        let memory_budget = self.memory_budget.as_ref().map(|budget| budget.snapshot());
+
         let snapshot = SimpleDreamPoolCheckpoint {
             version: POOL_CHECKPOINT_VERSION,
             config: self.config.clone(),
-            entries: self.entries.iter().cloned().collect(),
+            entries,
             entry_ids: self.entry_ids.iter().copied().collect(),
+            id_map,
+            soft_index,
+            hnsw_index,
+            memory_budget,
+            evictions_since_rebuild: self.evictions_since_rebuild,
         };
 
         Self::write_snapshot(&snapshot, path)
     }
 
     fn load_checkpoint<P: AsRef<std::path::Path>>(path: P) -> Result<Self, CheckpointError> {
         let snapshot: SimpleDreamPoolCheckpoint = Self::read_snapshot(path)?;
         if snapshot.version != POOL_CHECKPOINT_VERSION {
             return Err(CheckpointError::VersionMismatch {
                 expected: POOL_CHECKPOINT_VERSION,
                 found: snapshot.version,
             });
         }
 
         if snapshot.entries.len() != snapshot.entry_ids.len() {
             return Err(CheckpointError::InvalidFormat(
                 "Entry ID list length does not match entries".to_string(),
             ));
         }
 
-        let mut pool = SimpleDreamPool::new(snapshot.config);
-        pool.entries = VecDeque::from(snapshot.entries);
-        pool.entry_ids = VecDeque::from(snapshot.entry_ids);
-        pool.rebuild_after_restore()?;
+        if snapshot.id_map.len() != snapshot.entry_ids.len() {
+            return Err(CheckpointError::InvalidFormat(
+                "id_map length does not match stored entry IDs".to_string(),
+            ));
+        }
+
+        let SimpleDreamPoolCheckpoint {
+            version: _,
+            config,
+            entries,
+            entry_ids,
+            id_map,
+            soft_index,
+            hnsw_index,
+            memory_budget,
+            evictions_since_rebuild,
+        } = snapshot;
+
+        let mut pool = SimpleDreamPool::new(config);
+        let concrete_entries: Vec<DreamEntry> = entries
+            .into_iter()
+            .map(DreamEntryCheckpoint::into_entry)
+            .collect::<Result<_, _>>()?;
+        pool.entries = VecDeque::from(concrete_entries);
+        pool.entry_ids = VecDeque::from(entry_ids);
+
+        let mut id_to_entry = HashMap::with_capacity(pool.entries.len());
+        for (entry_id, index) in id_map.into_iter() {
+            if index >= pool.entries.len() {
+                return Err(CheckpointError::InvalidFormat(
+                    "id_map index out of range".to_string(),
+                ));
+            }
+            let entry = pool.entries.get(index).cloned().ok_or_else(|| {
+                CheckpointError::InvalidFormat(
+                    "Failed to resolve entry for id_map index".to_string(),
+                )
+            })?;
+            if id_to_entry.insert(entry_id, entry).is_some() {
+                return Err(CheckpointError::InvalidFormat(format!(
+                    "Duplicate entry {entry_id} detected while restoring id_map"
+                )));
+            }
+        }
+
+        let id_set: HashSet<EntryId> = pool.entry_ids.iter().copied().collect();
+        if !id_to_entry.keys().all(|entry_id| id_set.contains(entry_id)) {
+            return Err(CheckpointError::InvalidFormat(
+                "id_map contains entries missing from entry_ids".to_string(),
+            ));
+        }
+
+        pool.id_to_entry = id_to_entry;
+        pool.soft_index = soft_index.map(SoftIndex::from_snapshot);
+        pool.hnsw_index = match hnsw_index {
+            Some(index_snapshot) => Some(HnswIndex::from_snapshot(index_snapshot)?),
+            None => None,
+        };
+        pool.memory_budget = match memory_budget {
+            Some(budget_snapshot) => Some(
+                MemoryBudget::from_snapshot(budget_snapshot)
+                    .map_err(CheckpointError::InvalidFormat)?,
+            ),
+            None => None,
+        };
+        pool.evictions_since_rebuild = evictions_since_rebuild;
+
         Ok(pool)
     }
 }
 
 /// Pool statistics for monitoring and analysis
 #[derive(Debug, Clone)]
 pub struct PoolStats {
     pub count: usize,
     pub mean_coherence: f64,
     pub mean_energy: f64,
     pub mean_violation: f64,
 }
 
 /// Compute cosine similarity between two 3D vectors
 ///
 /// Returns a value in [-1, 1] where 1 means identical direction,
 /// 0 means orthogonal, and -1 means opposite direction.
 fn cosine_similarity(a: &[f32; 3], b: &[f32; 3]) -> f32 {
     let dot = a[0] * b[0] + a[1] * b[1] + a[2] * b[2];
     let mag_a = (a[0] * a[0] + a[1] * a[1] + a[2] * a[2]).sqrt();
     let mag_b = (b[0] * b[0] + b[1] * b[1] + b[2] * b[2]).sqrt();
 
     if mag_a == 0.0 || mag_b == 0.0 {
         return 0.0;
     }
@@ -1699,26 +1928,83 @@ mod tests {
         }
 
         for i in 5..10 {
             let tensor = ChromaticTensor::from_seed(i, 8, 8, 2);
             let result = SolverResult {
                 energy: 0.1,
                 coherence: 0.8,
                 violation: 0.05,
                 grad: None,
                 mask: None,
                 meta: json!({}),
             };
             let mut entry = DreamEntry::new(tensor, result);
             entry.set_utility(0.1); // Low utility
 
             pool.entries.push_back(entry);
         }
 
         // Retrieve only high-utility dreams (utility >= 0.5)
         let query = [0.5, 0.5, 0.5];
         let high_utility = pool.retrieve_by_utility(&query, 10, 0.5);
 
         assert_eq!(high_utility.len(), 5);
         assert!(high_utility.iter().all(|d| d.utility.unwrap_or(0.0) >= 0.5));
     }
+
+    #[test]
+    fn test_checkpoint_roundtrip_preserves_indices() {
+        use crate::dream::embedding::EmbeddingMapper;
+        use crate::solver::SolverResult;
+        use crate::ChromaticTensor;
+        use serde_json::json;
+        use uuid::Uuid;
+
+        let mut config = PoolConfig::default();
+        config.use_hnsw = true;
+        config.max_size = 16;
+        let mut pool = SimpleDreamPool::new(config.clone());
+
+        let result = SolverResult {
+            energy: 0.1,
+            coherence: 0.95,
+            violation: 0.02,
+            grad: None,
+            mask: None,
+            meta: json!({}),
+        };
+
+        for seed in 0..4 {
+            let tensor = ChromaticTensor::from_seed(seed, 8, 8, 2);
+            assert!(pool.add_if_coherent(tensor, result.clone()));
+        }
+
+        let mapper = EmbeddingMapper::new(64);
+        pool.rebuild_soft_index(&mapper, None);
+        assert!(pool.soft_index.is_some());
+        assert!(pool.hnsw_index.as_ref().map(|idx| idx.len()).unwrap_or(0) > 0);
+
+        let checkpoint_path =
+            std::env::temp_dir().join(format!("simple_pool-checkpoint-{}.bin", Uuid::new_v4()));
+
+        pool.save_checkpoint(&checkpoint_path).unwrap();
+        let restored = SimpleDreamPool::load_checkpoint(&checkpoint_path).unwrap();
+        let _ = std::fs::remove_file(&checkpoint_path);
+
+        assert_eq!(restored.len(), pool.len());
+        assert_eq!(
+            restored
+                .hnsw_index
+                .as_ref()
+                .map(|idx| idx.len())
+                .unwrap_or(0),
+            pool.hnsw_index.as_ref().map(|idx| idx.len()).unwrap_or(0)
+        );
+        assert_eq!(
+            restored.evictions_since_rebuild,
+            pool.evictions_since_rebuild
+        );
+        assert_eq!(restored.entry_ids.len(), pool.entry_ids.len());
+        assert_eq!(restored.id_to_entry.len(), pool.id_to_entry.len());
+        assert!(restored.soft_index.is_some());
+    }
 }
diff --git a/src/dream/soft_index.rs b/src/dream/soft_index.rs
index e9c3141a31a05ab84552da310816c0143e5ac0ea..529b28af81122750d01e10e4c1087baa3d959a87 100644
--- a/src/dream/soft_index.rs
+++ b/src/dream/soft_index.rs
@@ -1,153 +1,189 @@
 //! Soft index for approximate nearest neighbor search with embeddings.
 //!
 //! In-memory ANN-lite supporting cosine and euclidean similarity.
 
 use crate::dream::error::{DreamError, DreamResult};
+use serde::{Deserialize, Serialize};
 
 /// Unique identifier for indexed entries
 pub type EntryId = uuid::Uuid;
 
 /// Similarity metric for retrieval
 #[derive(Debug, Clone, Copy, PartialEq)]
 pub enum Similarity {
     /// Cosine similarity: dot(a,b) / (||a|| * ||b||)
     Cosine,
     /// Euclidean distance: ||a - b||
     Euclidean,
 }
 
 /// In-memory soft index for semantic retrieval
 pub struct SoftIndex {
     /// Embedding dimension
     dim: usize,
 
     /// Entry IDs
     ids: Vec<EntryId>,
 
     /// Embedding vectors
     vecs: Vec<Vec<f32>>,
 
     /// Pre-computed norms for cosine similarity
     norms: Vec<f32>,
 }
 
+/// Serializable snapshot of a [`SoftIndex`].
+#[derive(Clone, Serialize, Deserialize)]
+pub(crate) struct SoftIndexSnapshot {
+    dim: usize,
+    ids: Vec<EntryId>,
+    vecs: Vec<Vec<f32>>,
+    norms: Vec<f32>,
+}
+
 impl SoftIndex {
     /// Create a new soft index
     pub fn new(dim: usize) -> Self {
         Self {
             dim,
             ids: Vec::new(),
             vecs: Vec::new(),
             norms: Vec::new(),
         }
     }
 
     /// Add an entry to the index
     ///
     /// # Errors
     ///
     /// Returns `DimensionMismatch` if vector dimension doesn't match index dimension
     pub fn add(&mut self, id: EntryId, vec: Vec<f32>) -> DreamResult<()> {
         if vec.len() != self.dim {
             return Err(DreamError::dimension_mismatch(
                 self.dim,
                 vec.len(),
-                "SoftIndex add"
+                "SoftIndex add",
             ));
         }
 
         self.ids.push(id);
         self.vecs.push(vec);
         self.norms.push(0.0); // Will be computed in build()
         Ok(())
     }
 
     /// Build the index (compute norms)
     pub fn build(&mut self) {
         self.norms = self.vecs.iter().map(|v| l2_norm(v)).collect();
     }
 
     /// Query for K nearest neighbors
     ///
     /// # Errors
     ///
     /// Returns `DimensionMismatch` if query dimension doesn't match index dimension
-    pub fn query(&self, query: &[f32], k: usize, mode: Similarity) -> DreamResult<Vec<(EntryId, f32)>> {
+    pub fn query(
+        &self,
+        query: &[f32],
+        k: usize,
+        mode: Similarity,
+    ) -> DreamResult<Vec<(EntryId, f32)>> {
         if query.len() != self.dim {
             return Err(DreamError::dimension_mismatch(
                 self.dim,
                 query.len(),
-                "SoftIndex query"
+                "SoftIndex query",
             ));
         }
 
         if self.ids.is_empty() {
             return Ok(Vec::new());
         }
 
         // Compute similarities
         let query_norm = l2_norm(query);
-        let mut scores: Vec<(usize, f32)> = self.vecs
+        let mut scores: Vec<(usize, f32)> = self
+            .vecs
             .iter()
             .enumerate()
             .map(|(idx, vec)| {
                 let score = match mode {
                     Similarity::Cosine => cosine_sim(query, vec, query_norm, self.norms[idx]),
                     Similarity::Euclidean => -euclidean_dist(query, vec), // Negative for sorting
                 };
                 (idx, score)
             })
             .collect();
 
         // Sort by score (descending)
         scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
 
         // Return top-K
         Ok(scores
             .into_iter()
             .take(k)
             .map(|(idx, score)| (self.ids[idx], score))
             .collect())
     }
 
     /// Get number of entries
     pub fn len(&self) -> usize {
         self.ids.len()
     }
 
     /// Check if index is empty
     pub fn is_empty(&self) -> bool {
         self.ids.is_empty()
     }
 
     /// Clear the index
     pub fn clear(&mut self) {
         self.ids.clear();
         self.vecs.clear();
         self.norms.clear();
     }
+
+    /// Capture the full state of the index for checkpointing.
+    pub(crate) fn snapshot(&self) -> SoftIndexSnapshot {
+        SoftIndexSnapshot {
+            dim: self.dim,
+            ids: self.ids.clone(),
+            vecs: self.vecs.clone(),
+            norms: self.norms.clone(),
+        }
+    }
+
+    /// Rebuild an index from a previously captured snapshot.
+    pub(crate) fn from_snapshot(snapshot: SoftIndexSnapshot) -> Self {
+        Self {
+            dim: snapshot.dim,
+            ids: snapshot.ids,
+            vecs: snapshot.vecs,
+            norms: snapshot.norms,
+        }
+    }
 }
 
 /// Compute L2 norm of a vector
 #[inline]
 fn l2_norm(v: &[f32]) -> f32 {
     v.iter().map(|x| x * x).sum::<f32>().sqrt()
 }
 
 /// Compute cosine similarity with pre-computed norms
 #[inline]
 fn cosine_sim(a: &[f32], b: &[f32], norm_a: f32, norm_b: f32) -> f32 {
     if norm_a < 1e-8 || norm_b < 1e-8 {
         return 0.0;
     }
 
     let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
     dot / (norm_a * norm_b)
 }
 
 /// Compute Euclidean distance
 #[inline]
 fn euclidean_dist(a: &[f32], b: &[f32]) -> f32 {
     a.iter()
         .zip(b.iter())
         .map(|(x, y)| (x - y).powi(2))
diff --git a/src/dream/tests/mod.rs b/src/dream/tests/mod.rs
index 3366ae35a81efac968565d7ff2ad11322eeb96a6..00d50c28d68353ef901a77f1adfdbb46ce3e845b 100644
--- a/src/dream/tests/mod.rs
+++ b/src/dream/tests/mod.rs
@@ -1,33 +1,33 @@
 //! Integration tests for dream pool system
 //!
 //! This module contains comprehensive integration tests that verify
 //! the entire dream pool pipeline works correctly end-to-end.
 
-use crate::dream::*;
 use crate::dream::embedding::QuerySignature;
 use crate::dream::soft_index::EntryId;
+use crate::dream::*;
 use crate::solver::SolverResult;
 use crate::tensor::ChromaticTensor;
 use serde_json::json;
 
 /// Test that the full retrieval pipeline works with all components
 #[test]
 fn test_full_retrieval_pipeline() {
     // Create a pool with some entries
     let config = PoolConfig::default();
     let mut pool = SimpleDreamPool::new(config);
 
     // Add diverse entries with different colors
     for i in 0..20 {
         let tensor = ChromaticTensor::new(4, 4, 3);
         let result = SolverResult {
             energy: (0.1 + (i as f64) * 0.01),
             coherence: (0.9 - (i as f64) * 0.01),
             violation: 0.0,
             grad: None,
             mask: None,
             meta: json!({"test": i}),
         };
         pool.add_if_coherent(tensor, result);
     }
 
@@ -105,288 +105,304 @@ fn test_mmr_diversity_enforcement() {
         let tensor = ChromaticTensor::new(2, 2, 2);
         let result = SolverResult {
             energy: 0.1,
             coherence: 0.9,
             violation: 0.0,
             grad: None,
             mask: None,
             meta: json!({}),
         };
         let mut entry = DreamEntry::new(tensor, result);
         entry.chroma_signature = [1.0 - (i as f32) * 0.05, 0.0, 0.0];
         candidates.push(entry);
     }
 
     // Without diversity (lambda=1.0), should select most similar
     let no_diversity = retrieve_diverse_mmr(&candidates, &[1.0, 0.0, 0.0], 3, 1.0, 0.0);
 
     // With diversity (lambda=0.3), should select more diverse set
     let with_diversity = retrieve_diverse_mmr(&candidates, &[1.0, 0.0, 0.0], 3, 0.3, 0.0);
 
     // Both should return 3 entries
     assert_eq!(no_diversity.len(), 3);
     assert_eq!(with_diversity.len(), 3);
 
     // First entry should be the same (most relevant)
-    assert_eq!(no_diversity[0].chroma_signature, with_diversity[0].chroma_signature);
+    assert_eq!(
+        no_diversity[0].chroma_signature,
+        with_diversity[0].chroma_signature
+    );
 }
 
 /// Test memory budget prevents unbounded growth
 #[test]
 fn test_memory_budget_prevents_unbounded_growth() {
     use crate::dream::memory::MemoryBudget;
 
     let mut budget = MemoryBudget::new(1024 * 1024); // 1 MB limit
 
     // Add entries until we hit the limit
     for _ in 0..100 {
         budget.add_entry(10 * 1024); // 10 KB each
 
         if budget.needs_eviction() {
             // Budget correctly detects when eviction is needed
             assert!(budget.usage_ratio() > 0.9);
             break;
         }
     }
 }
 
 /// Test HNSW index scales to large numbers of entries
 #[test]
 fn test_hnsw_scalability() {
     // Test that we can handle large pools with HNSW
     // For now, test with SoftIndex which is simpler for integration tests
-    use crate::dream::soft_index::{SoftIndex, EntryId, Similarity};
+    use crate::dream::soft_index::{EntryId, Similarity, SoftIndex};
 
     let mut index = SoftIndex::new(32);
 
     // Add 1000 entries
     for i in 0..1000 {
         let id = EntryId::new_v4();
-        let embedding: Vec<f32> = (0..32)
-            .map(|j| ((i * 32 + j) as f32) / 32000.0)
-            .collect();
+        let embedding: Vec<f32> = (0..32).map(|j| ((i * 32 + j) as f32) / 32000.0).collect();
         index.add(id, embedding).unwrap();
     }
 
     // Build index
     index.build();
 
     // Query should complete
     let query: Vec<f32> = (0..32).map(|i| (i as f32) / 32.0).collect();
     let results = index.query(&query, 10, Similarity::Cosine).unwrap();
 
     assert_eq!(results.len(), 10);
 }
 
 /// Test error recovery when index is corrupted
 #[test]
 fn test_error_recovery_on_index_failure() {
     let config = PoolConfig::default();
     let mut pool = SimpleDreamPool::new(config);
 
     // Add entries
     for _ in 0..5 {
         let tensor = ChromaticTensor::new(4, 4, 3);
         let result = SolverResult {
             energy: 0.1,
             coherence: 0.9,
             violation: 0.0,
             grad: None,
             mask: None,
             meta: json!({}),
         };
         pool.add_if_coherent(tensor, result);
     }
 
     let mapper = EmbeddingMapper::new(64);
     pool.rebuild_soft_index(&mapper, None);
 
     // Even if we clear the pool, retrieval shouldn't crash
     let query = QuerySignature::from_chroma([1.0, 0.0, 0.0]);
     let weights = RetrievalWeights::default();
     let results = pool.retrieve_soft(&query, 5, &weights, Similarity::Cosine, &mapper, None);
 
     // Should handle gracefully (might return empty)
     assert!(results.len() <= 5);
 }
 
-    fn build_hnsw_pool(entry_count: usize) -> SimpleDreamPool {
-        let config = PoolConfig {
-            max_size: entry_count + 100,
-            coherence_threshold: 0.0,
-            retrieval_limit: 16,
-            use_hnsw: true,
-            memory_budget_mb: None,
-        };
+fn build_hnsw_pool(entry_count: usize) -> SimpleDreamPool {
+    let config = PoolConfig {
+        max_size: entry_count + 100,
+        coherence_threshold: 0.0,
+        retrieval_limit: 16,
+        use_hnsw: true,
+        memory_budget_mb: None,
+    };
 
-        let mut pool = SimpleDreamPool::new(config);
+    let mut pool = SimpleDreamPool::new(config);
 
-        for seed in 0..entry_count {
-            let tensor = ChromaticTensor::from_seed(seed as u64 + 1, 8, 8, 3);
-            let result = SolverResult {
-                energy: 0.05,
-                coherence: 0.9,
-                violation: 0.01,
-                grad: None,
-                mask: None,
-                meta: json!({ "seed": seed }),
-            };
-            pool.add(tensor, result);
-        }
+    for seed in 0..entry_count {
+        let tensor = ChromaticTensor::from_seed(seed as u64 + 1, 8, 8, 3);
+        let result = SolverResult {
+            energy: 0.05,
+            coherence: 0.9,
+            violation: 0.01,
+            grad: None,
+            mask: None,
+            meta: json!({ "seed": seed }),
+        };
+        pool.add(tensor, result);
+    }
 
-        let mapper = EmbeddingMapper::new(64);
-        pool.rebuild_soft_index(&mapper, None);
+    let mapper = EmbeddingMapper::new(64);
+    pool.rebuild_soft_index(&mapper, None);
 
-        assert!(pool.hnsw_built_for_test(), "HNSW index should be constructed for tests");
-        assert_eq!(
-            pool.evictions_since_rebuild_for_test(),
-            0,
-            "Eviction counter should reset after rebuild",
-        );
+    assert!(
+        pool.hnsw_built_for_test(),
+        "HNSW index should be constructed for tests"
+    );
+    assert_eq!(
+        pool.evictions_since_rebuild_for_test(),
+        0,
+        "Eviction counter should reset after rebuild",
+    );
 
-        assert_eq!(
-            pool.hnsw_ghosts_for_test()
-                .expect("HNSW index must be available after rebuild"),
-            0,
-            "Freshly rebuilt index should not contain ghost nodes",
-        );
+    assert_eq!(
+        pool.hnsw_ghosts_for_test()
+            .expect("HNSW index must be available after rebuild"),
+        0,
+        "Freshly rebuilt index should not contain ghost nodes",
+    );
 
-        pool
-    }
+    pool
+}
 
-    #[test]
-    fn test_index_survives_light_eviction() {
-        let mut pool = build_hnsw_pool(500);
+#[test]
+fn test_index_survives_light_eviction() {
+    let mut pool = build_hnsw_pool(500);
 
-        pool.evict_for_test(5);
+    pool.evict_for_test(5);
 
-        assert_eq!(pool.len(), 495, "Pool should remove only the requested entries");
-        assert!(
-            pool.has_soft_index_for_test(),
-            "Linear index should remain after light churn",
-        );
-        assert_eq!(
-            pool.evictions_since_rebuild_for_test(),
-            5,
-            "Eviction counter should reflect light churn",
-        );
+    assert_eq!(
+        pool.len(),
+        495,
+        "Pool should remove only the requested entries"
+    );
+    assert!(
+        pool.has_soft_index_for_test(),
+        "Linear index should remain after light churn",
+    );
+    assert_eq!(
+        pool.evictions_since_rebuild_for_test(),
+        5,
+        "Eviction counter should reflect light churn",
+    );
 
-        assert!(pool.hnsw_built_for_test(), "HNSW graph should remain available");
-        assert_eq!(
-            pool.hnsw_len_for_test().expect("HNSW index should persist"),
-            495,
-            "HNSW id map should match pool size",
-        );
-        assert_eq!(
-            pool.hnsw_ghosts_for_test()
-                .expect("HNSW index should persist after light churn"),
-            5,
-            "Light churn should leave ghost nodes without forcing rebuild",
-        );
-    }
+    assert!(
+        pool.hnsw_built_for_test(),
+        "HNSW graph should remain available"
+    );
+    assert_eq!(
+        pool.hnsw_len_for_test().expect("HNSW index should persist"),
+        495,
+        "HNSW id map should match pool size",
+    );
+    assert_eq!(
+        pool.hnsw_ghosts_for_test()
+            .expect("HNSW index should persist after light churn"),
+        5,
+        "Light churn should leave ghost nodes without forcing rebuild",
+    );
+}
 
-    #[test]
-    fn test_index_invalidates_after_heavy_churn() {
-        let mut pool = build_hnsw_pool(500);
+#[test]
+fn test_index_invalidates_after_heavy_churn() {
+    let mut pool = build_hnsw_pool(500);
 
-        pool.evict_for_test(60);
+    pool.evict_for_test(60);
 
-        assert_eq!(pool.len(), 440, "Pool should evict requested number of entries");
-        assert!(
-            !pool.has_soft_index_for_test(),
-            "Heavy churn should drop the linear index",
-        );
-        assert_eq!(
-            pool.evictions_since_rebuild_for_test(),
-            0,
-            "Eviction counter resets after rebuild",
-        );
+    assert_eq!(
+        pool.len(),
+        440,
+        "Pool should evict requested number of entries"
+    );
+    assert!(
+        !pool.has_soft_index_for_test(),
+        "Heavy churn should drop the linear index",
+    );
+    assert_eq!(
+        pool.evictions_since_rebuild_for_test(),
+        0,
+        "Eviction counter resets after rebuild",
+    );
 
-        assert!(pool.hnsw_built_for_test(), "Rebuilt HNSW graph should be marked as available");
-        assert_eq!(
-            pool.hnsw_len_for_test().expect("HNSW index should be present"),
-            440,
-            "Rebuilt graph should only contain active entries",
-        );
-        assert_eq!(
-            pool.hnsw_ghosts_for_test()
-                .expect("HNSW index should be present after rebuild"),
-            0,
-            "Rebuild should clear ghost nodes introduced by heavy churn",
-        );
-    }
+    assert!(
+        pool.hnsw_built_for_test(),
+        "Rebuilt HNSW graph should be marked as available"
+    );
+    assert_eq!(
+        pool.hnsw_len_for_test()
+            .expect("HNSW index should be present"),
+        440,
+        "Rebuilt graph should only contain active entries",
+    );
+    assert_eq!(
+        pool.hnsw_ghosts_for_test()
+            .expect("HNSW index should be present after rebuild"),
+        0,
+        "Rebuild should clear ghost nodes introduced by heavy churn",
+    );
+}
 
 /// Test concurrent access patterns (basic safety check)
 #[test]
 fn test_concurrent_reads() {
     use std::sync::Arc;
     use std::thread;
 
     let config = PoolConfig::default();
     let mut pool = SimpleDreamPool::new(config);
 
     // Add entries
     for _ in 0..20 {
         let tensor = ChromaticTensor::new(4, 4, 3);
         let result = SolverResult {
             energy: 0.1,
             coherence: 0.9,
             violation: 0.0,
             grad: None,
             mask: None,
             meta: json!({}),
         };
         pool.add_if_coherent(tensor, result);
     }
 
     let mapper = EmbeddingMapper::new(64);
     pool.rebuild_soft_index(&mapper, None);
 
     // Wrap in Arc to share across threads
     let pool = Arc::new(pool);
     let mapper = Arc::new(mapper);
 
     // Spawn multiple reader threads
     let mut handles = vec![];
     for i in 0..4 {
         let pool_clone = Arc::clone(&pool);
         let mapper_clone = Arc::clone(&mapper);
 
         let handle = thread::spawn(move || {
-            let query = QuerySignature::from_chroma([
-                (i as f32) / 4.0,
-                1.0 - (i as f32) / 4.0,
-                0.0
-            ]);
+            let query =
+                QuerySignature::from_chroma([(i as f32) / 4.0, 1.0 - (i as f32) / 4.0, 0.0]);
             let weights = RetrievalWeights::default();
             let results = pool_clone.retrieve_soft(
                 &query,
                 5,
                 &weights,
                 Similarity::Cosine,
                 &*mapper_clone,
-                None
+                None,
             );
             results.len()
         });
 
         handles.push(handle);
     }
 
     // Wait for all threads
     for handle in handles {
         let count = handle.join().unwrap();
         assert!(count <= 5);
     }
 }
 
 /// Test large batch operations
 #[test]
 fn test_large_batch_operations() {
     let config = PoolConfig::default();
     let mut pool = SimpleDreamPool::new(config);
 
     // Add 100 entries in batch
     for _ in 0..100 {
         let tensor = ChromaticTensor::new(4, 4, 3);
         let result = SolverResult {
             energy: 0.1,
@@ -394,149 +410,157 @@ fn test_large_batch_operations() {
             violation: 0.0,
             grad: None,
             mask: None,
             meta: json!({}),
         };
         pool.add_if_coherent(tensor, result);
     }
 
     assert_eq!(pool.len(), 100);
 
     // Rebuild index should handle large batches
     let mapper = EmbeddingMapper::new(64);
     pool.rebuild_soft_index(&mapper, None);
 
     // Retrieval should work with large pool
     let query = QuerySignature::from_chroma([1.0, 0.0, 0.0]);
     let weights = RetrievalWeights::default();
     let results = pool.retrieve_soft(&query, 10, &weights, Similarity::Cosine, &mapper, None);
 
     assert!(results.len() <= 10);
 }
 
 /// Test hybrid scoring weights combination
 #[test]
 fn test_hybrid_scoring_weights() {
-    use crate::dream::hybrid_scoring::{RetrievalWeights, rerank_hybrid};
+    use crate::dream::hybrid_scoring::{rerank_hybrid, RetrievalWeights};
     use std::collections::HashMap;
 
     // Create test hits
     let id1 = EntryId::new_v4();
     let id2 = EntryId::new_v4();
     let hits = vec![(id1, 0.9), (id2, 0.7)];
 
     // Create test entries
     let tensor1 = ChromaticTensor::new(2, 2, 2);
     let result1 = SolverResult {
         energy: 0.1,
         coherence: 0.95,
         violation: 0.0,
         grad: None,
         mask: None,
         meta: json!({}),
     };
     let entry1 = DreamEntry::new(tensor1, result1);
 
     let tensor2 = ChromaticTensor::new(2, 2, 2);
     let result2 = SolverResult {
         energy: 0.2,
         coherence: 0.85,
         violation: 0.0,
         grad: None,
         mask: None,
         meta: json!({}),
     };
     let entry2 = DreamEntry::new(tensor2, result2);
 
     let mut entries = HashMap::new();
     entries.insert(id1, entry1);
     entries.insert(id2, entry2);
 
     // Test different weight configurations
     let equal_weights = RetrievalWeights {
-        alpha: 0.4,    // Similarity
-        beta: 0.3,     // Utility
-        gamma: 0.2,    // Class match
-        delta: 0.1,    // Duplicate penalty
-        lambda: 0.5,   // MMR diversity
+        alpha: 0.4,  // Similarity
+        beta: 0.3,   // Utility
+        gamma: 0.2,  // Class match
+        delta: 0.1,  // Duplicate penalty
+        lambda: 0.5, // MMR diversity
     };
 
     let results = rerank_hybrid(&hits, &equal_weights, &entries, None);
     assert_eq!(results.len(), 2);
 }
 
 /// Test UMS round-trip fidelity (Phase 7 / Phase 2 Cognitive Integration)
 ///
 /// Verifies the complete data path:
 /// ChromaticTensor → UMS Encode → UMS Decode → Original Tensor Features
 ///
 /// Asserts that ΔE94 distance between starting and ending color vectors is ≤ 1.0 × 10^-3
 ///
 /// Note: decode_from_ums returns HSL (hue, saturation, luminance), where hue is in radians.
 /// This test validates that the round-trip encoding preserves color fidelity within the
 /// specified tolerance using perceptual color difference (ΔE94).
 #[test]
 fn test_ums_round_trip_fidelity() {
-    use crate::bridge::{encode_to_ums, decode_from_ums, ModalityMapper};
+    use crate::bridge::{decode_from_ums, encode_to_ums, ModalityMapper};
     use crate::config::BridgeConfig;
-    use crate::spectral::color::delta_e94;
     use crate::spectral::canonical_hue;
+    use crate::spectral::color::delta_e94;
 
     // Helper to convert HSL to RGB (inline version)
     fn hsl_to_rgb(h_norm: f32, saturation: f32, luminance: f32) -> [f32; 3] {
         let c = (1.0 - (2.0 * luminance - 1.0).abs()) * saturation;
         let h_prime = h_norm * 6.0;
         let x = c * (1.0 - ((h_prime % 2.0) - 1.0).abs());
 
         let (r1, g1, b1) = match h_prime.floor() as i32 {
             0 => (c, x, 0.0),
             1 => (x, c, 0.0),
             2 => (0.0, c, x),
             3 => (0.0, x, c),
             4 => (x, 0.0, c),
             5 | 6 => (c, 0.0, x),
             _ => (0.0, 0.0, 0.0),
         };
 
         let m = luminance - c / 2.0;
-        [(r1 + m).clamp(0.0, 1.0), (g1 + m).clamp(0.0, 1.0), (b1 + m).clamp(0.0, 1.0)]
+        [
+            (r1 + m).clamp(0.0, 1.0),
+            (g1 + m).clamp(0.0, 1.0),
+            (b1 + m).clamp(0.0, 1.0),
+        ]
     }
 
     // Load bridge configuration
     let config = BridgeConfig::from_str(include_str!("../../../config/bridge.toml"))
         .expect("valid bridge config");
     let mapper = ModalityMapper::new(config.clone());
     let tolerance = config.reversibility.delta_e_tolerance;
 
     // Test with multiple seeds to ensure robustness
     for seed in [42, 123, 456, 789, 1024] {
         let tensor = ChromaticTensor::from_seed(seed, 16, 16, 4);
         let original_rgb = tensor.mean_rgb();
 
         // Encode to UMS
         let ums_vector = encode_to_ums(&mapper, &tensor);
-        assert_eq!(ums_vector.components().len(), 512, "UMS vector must be 512D");
+        assert_eq!(
+            ums_vector.components().len(),
+            512,
+            "UMS vector must be 512D"
+        );
 
         // Decode from UMS (returns HSL: [hue_radians, saturation, luminance])
         let decoded_hsl = decode_from_ums(&ums_vector);
 
         // Convert HSL to RGB for comparison
         let hue_norm = canonical_hue(decoded_hsl[0]) / std::f32::consts::TAU;
         let decoded_rgb = hsl_to_rgb(hue_norm, decoded_hsl[1], decoded_hsl[2]);
 
         // Compute ΔE94 perceptual color difference
         let delta_e = delta_e94(original_rgb, decoded_rgb);
 
         // Assert fidelity requirement: ΔE94 ≤ tolerance (1.0 × 10^-3 from config)
         assert!(
             delta_e <= tolerance,
             "UMS round-trip fidelity requirement violated: ΔE94 = {} > {} (seed={}, original={:?}, decoded_rgb={:?}, decoded_hsl={:?})",
             delta_e,
             tolerance,
             seed,
             original_rgb,
             decoded_rgb,
             decoded_hsl
         );
 
         // Additional sanity checks
         for channel in 0..3 {
@@ -568,137 +592,156 @@ fn test_dream_entry_ums_integration() {
 
     let entry = DreamEntry::new(tensor.clone(), result);
 
     // Verify UMS vector is computed
     assert_eq!(entry.ums_vector.len(), 512, "UMS vector must be 512D");
 
     // Verify hue category is in valid range [0-11]
     assert!(
         entry.hue_category < 12,
         "Hue category must be in [0, 11], got {}",
         entry.hue_category
     );
 
     // Verify UMS encoding is deterministic
     let result2 = SolverResult {
         energy: 0.2,
         coherence: 0.8,
         violation: 0.0,
         grad: None,
         mask: None,
         meta: json!({}),
     };
     let entry2 = DreamEntry::new(tensor.clone(), result2);
 
     // Same tensor should produce same UMS vector and hue category
-    assert_eq!(entry.ums_vector, entry2.ums_vector, "UMS encoding must be deterministic");
-    assert_eq!(entry.hue_category, entry2.hue_category, "Hue category must be deterministic");
+    assert_eq!(
+        entry.ums_vector, entry2.ums_vector,
+        "UMS encoding must be deterministic"
+    );
+    assert_eq!(
+        entry.hue_category, entry2.hue_category,
+        "Hue category must be deterministic"
+    );
 }
 
 /// Test category-based hybrid retrieval (Phase 3)
 #[test]
 fn test_retrieve_hybrid_category_filtering() {
     let config = PoolConfig::default();
     let mut pool = SimpleDreamPool::new(config);
 
     // Add entries with different hue categories
     // Create tensors with distinct RGB values to ensure different categories
     for i in 0..30 {
         let mut tensor = ChromaticTensor::new(8, 8, 2);
         // Create distinct hues by varying the dominant color channel
         let hue_offset = (i as f32) / 30.0;
         for row in 0..8 {
             for col in 0..8 {
                 for layer in 0..2 {
                     tensor.colors[[row, col, layer, 0]] = (hue_offset + 0.1_f32).clamp(0.0, 1.0);
                     tensor.colors[[row, col, layer, 1]] = 0.5_f32.clamp(0.0, 1.0);
                     tensor.colors[[row, col, layer, 2]] = (1.0_f32 - hue_offset).clamp(0.0, 1.0);
                 }
             }
         }
 
         let result = SolverResult {
             energy: 0.1,
             coherence: 0.9,
             violation: 0.0,
             grad: None,
             mask: None,
             meta: json!({"index": i}),
         };
 
         pool.add_if_coherent(tensor, result);
     }
 
     assert!(pool.len() > 0, "Pool should have entries");
 
     // Test retrieve_hybrid: should filter by query's category and rank by UMS
     let query_tensor = ChromaticTensor::from_seed(42, 8, 8, 2);
     let results = pool.retrieve_hybrid(&query_tensor, 5);
 
     // Verify results
     assert!(results.len() <= 5, "Should return at most 5 results");
 
     // All results should be from the same category as the query
     if !results.is_empty() {
-        let query_entry = DreamEntry::new(query_tensor.clone(), SolverResult {
-            energy: 0.1,
-            coherence: 0.9,
-            violation: 0.0,
-            grad: None,
-            mask: None,
-            meta: json!({}),
-        });
+        let query_entry = DreamEntry::new(
+            query_tensor.clone(),
+            SolverResult {
+                energy: 0.1,
+                coherence: 0.9,
+                violation: 0.0,
+                grad: None,
+                mask: None,
+                meta: json!({}),
+            },
+        );
         let query_category = query_entry.hue_category;
 
         for result in &results {
             assert_eq!(
                 result.hue_category, query_category,
                 "All results should be from query's category"
             );
         }
     }
 }
 
 /// Test retrieve_by_category (Phase 3)
 #[test]
 fn test_retrieve_by_category() {
     let config = PoolConfig::default();
     let mut pool = SimpleDreamPool::new(config);
 
     // Add entries across different categories
     for i in 0..24 {
         let tensor = ChromaticTensor::from_seed(i, 8, 8, 2);
         let result = SolverResult {
             energy: 0.1,
             coherence: 0.9,
             violation: 0.0,
             grad: None,
             mask: None,
             meta: json!({"index": i}),
         };
 
         pool.add_if_coherent(tensor, result);
     }
 
     // Create a query UMS vector (use an actual entry's UMS for testing)
     let query_tensor = ChromaticTensor::from_seed(42, 8, 8, 2);
-    let query_entry = DreamEntry::new(query_tensor, SolverResult {
-        energy: 0.1,
-        coherence: 0.9,
-        violation: 0.0,
-        grad: None,
-        mask: None,
-        meta: json!({}),
-    });
+    let query_entry = DreamEntry::new(
+        query_tensor,
+        SolverResult {
+            energy: 0.1,
+            coherence: 0.9,
+            violation: 0.0,
+            grad: None,
+            mask: None,
+            meta: json!({}),
+        },
+    );
 
     // Test retrieve_by_category for category 0
     let results = pool.retrieve_by_category(0, &query_entry.ums_vector, 3);
 
     // Verify all results are from category 0
     for result in &results {
-        assert_eq!(result.hue_category, 0, "All results should be from category 0");
+        assert_eq!(
+            result.hue_category, 0,
+            "All results should be from category 0"
+        );
     }
 
     // Test invalid category (should return empty with warning)
     let results = pool.retrieve_by_category(12, &query_entry.ums_vector, 3);
-    assert_eq!(results.len(), 0, "Invalid category should return empty results");
+    assert_eq!(
+        results.len(),
+        0,
+        "Invalid category should return empty results"
+    );
 }
diff --git a/src/learner/classifier.rs b/src/learner/classifier.rs
index 8d8980aa64c32c0cc8a3241c69315dffdcfaa2a6..c97cf1e0ba23cb8e91619e0110b3e782cc51b546 100644
--- a/src/learner/classifier.rs
+++ b/src/learner/classifier.rs
@@ -1,35 +1,35 @@
 //! Color classification model
 //!
 //! Implements a simple MLP (Multi-Layer Perceptron) for classifying
 //! ChromaticTensor inputs into 10 color classes.
 
-use crate::tensor::ChromaticTensor;
 use crate::data::ColorClass;
+use crate::tensor::ChromaticTensor;
 use ndarray::{Array1, Array2};
 use rand::{Rng, SeedableRng};
-use serde::{Serialize, Deserialize};
+use serde::{Deserialize, Serialize};
 
 /// Configuration for the classifier
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct ClassifierConfig {
     /// Input size (flattened tensor dimensions)
     pub input_size: usize,
     /// Hidden layer size
     pub hidden_size: usize,
     /// Output size (number of classes)
     pub output_size: usize,
     /// Random seed for weight initialization
     pub seed: u64,
 }
 
 impl Default for ClassifierConfig {
     fn default() -> Self {
         Self {
             input_size: 16 * 16 * 4 * 3, // 16x16x4 tensor with RGB = 3072
             hidden_size: 128,
             output_size: 10, // 10 color classes
             seed: 42,
         }
     }
 }
 
@@ -68,62 +68,60 @@ pub struct Weights {
 pub struct Gradients {
     pub dw1: Array2<f32>,
     pub db1: Array1<f32>,
     pub dw2: Array2<f32>,
     pub db2: Array1<f32>,
 }
 
 /// Simple MLP classifier: Input → Hidden (ReLU) → Output (Softmax)
 pub struct MLPClassifier {
     config: ClassifierConfig,
     // Layer 1: input → hidden
     w1: Array2<f32>, // [hidden_size, input_size]
     b1: Array1<f32>, // [hidden_size]
     // Layer 2: hidden → output
     w2: Array2<f32>, // [output_size, hidden_size]
     b2: Array1<f32>, // [output_size]
 }
 
 impl MLPClassifier {
     /// Create a new MLP classifier with random initialization
     pub fn new(config: ClassifierConfig) -> Self {
         let mut rng = rand::rngs::StdRng::seed_from_u64(config.seed);
 
         // Xavier initialization for weights
         let w1_scale = (2.0 / config.input_size as f32).sqrt();
-        let w1 = Array2::from_shape_fn(
-            (config.hidden_size, config.input_size),
-            |_| (rng.gen::<f32>() - 0.5) * 2.0 * w1_scale,
-        );
+        let w1 = Array2::from_shape_fn((config.hidden_size, config.input_size), |_| {
+            (rng.gen::<f32>() - 0.5) * 2.0 * w1_scale
+        });
 
         let b1 = Array1::zeros(config.hidden_size);
 
         let w2_scale = (2.0 / config.hidden_size as f32).sqrt();
-        let w2 = Array2::from_shape_fn(
-            (config.output_size, config.hidden_size),
-            |_| (rng.gen::<f32>() - 0.5) * 2.0 * w2_scale,
-        );
+        let w2 = Array2::from_shape_fn((config.output_size, config.hidden_size), |_| {
+            (rng.gen::<f32>() - 0.5) * 2.0 * w2_scale
+        });
 
         let b2 = Array1::zeros(config.output_size);
 
         Self {
             config,
             w1,
             b1,
             w2,
             b2,
         }
     }
 
     /// Flatten a ChromaticTensor into a 1D feature vector
     fn flatten_tensor(&self, tensor: &ChromaticTensor) -> Array1<f32> {
         let (rows, cols, layers) = tensor.dims();
         let mut features = Array1::zeros(rows * cols * layers * 3);
 
         let mut idx = 0;
         for r in 0..rows {
             for c in 0..cols {
                 for l in 0..layers {
                     let rgb = tensor.get_rgb(r, c, l);
                     features[idx] = rgb[0];
                     features[idx + 1] = rgb[1];
                     features[idx + 2] = rgb[2];
@@ -243,147 +241,172 @@ impl ColorClassifier for MLPClassifier {
 
         (avg_loss, Gradients { dw1, db1, dw2, db2 })
     }
 
     fn update_weights(&mut self, gradients: &Gradients, learning_rate: f32) {
         // Gradient descent: W = W - lr * dW
         self.w1 = &self.w1 - &(&gradients.dw1 * learning_rate);
         self.b1 = &self.b1 - &(&gradients.db1 * learning_rate);
         self.w2 = &self.w2 - &(&gradients.dw2 * learning_rate);
         self.b2 = &self.b2 - &(&gradients.db2 * learning_rate);
     }
 
     fn get_weights(&self) -> Weights {
         Weights {
             w1: self.w1.iter().cloned().collect(),
             b1: self.b1.iter().cloned().collect(),
             w2: self.w2.iter().cloned().collect(),
             b2: self.b2.iter().cloned().collect(),
         }
     }
 
     fn set_weights(&mut self, weights: Weights) {
         self.w1 = Array2::from_shape_vec(
             (self.config.hidden_size, self.config.input_size),
             weights.w1,
-        ).expect("Invalid w1 shape");
+        )
+        .expect("Invalid w1 shape");
 
         self.b1 = Array1::from_vec(weights.b1);
 
         self.w2 = Array2::from_shape_vec(
             (self.config.output_size, self.config.hidden_size),
             weights.w2,
-        ).expect("Invalid w2 shape");
+        )
+        .expect("Invalid w2 shape");
 
         self.b2 = Array1::from_vec(weights.b2);
     }
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
     use crate::data::{ColorDataset, DatasetConfig};
 
     #[test]
     fn test_mlp_creation() {
         let config = ClassifierConfig::default();
         let classifier = MLPClassifier::new(config);
 
         assert_eq!(classifier.w1.dim(), (128, 3072));
         assert_eq!(classifier.b1.dim(), 128);
         assert_eq!(classifier.w2.dim(), (10, 128));
         assert_eq!(classifier.b2.dim(), 10);
     }
 
     #[test]
     fn test_forward_pass() {
         let config = ClassifierConfig::default();
         let classifier = MLPClassifier::new(config);
 
         let tensor = ChromaticTensor::from_seed(42, 16, 16, 4);
         let output = classifier.forward(&tensor);
 
         assert_eq!(output.len(), 10);
 
         // Check softmax: probabilities sum to 1
         let sum: f32 = output.iter().sum();
         assert!((sum - 1.0).abs() < 1e-5);
 
         // All probabilities should be positive
         assert!(output.iter().all(|&p| p >= 0.0 && p <= 1.0));
     }
 
     #[test]
     fn test_predict() {
         let config = ClassifierConfig::default();
         let classifier = MLPClassifier::new(config);
 
         let tensor = ChromaticTensor::from_seed(42, 16, 16, 4);
         let prediction = classifier.predict(&tensor);
 
         // Should return a valid color class
-        assert!(matches!(prediction, ColorClass::Red | ColorClass::Green |
-                        ColorClass::Blue | ColorClass::Yellow | ColorClass::Cyan |
-                        ColorClass::Magenta | ColorClass::Orange | ColorClass::Purple |
-                        ColorClass::White | ColorClass::Black));
+        assert!(matches!(
+            prediction,
+            ColorClass::Red
+                | ColorClass::Green
+                | ColorClass::Blue
+                | ColorClass::Yellow
+                | ColorClass::Cyan
+                | ColorClass::Magenta
+                | ColorClass::Orange
+                | ColorClass::Purple
+                | ColorClass::White
+                | ColorClass::Black
+        ));
     }
 
     #[test]
     fn test_compute_loss_and_gradients() {
         let config = ClassifierConfig::default();
         let classifier = MLPClassifier::new(config);
 
         // Create small batch
         let dataset_config = DatasetConfig {
             tensor_size: (16, 16, 4),
             samples_per_class: 1,
             ..Default::default()
         };
         let dataset = ColorDataset::generate(dataset_config);
 
-        let tensors: Vec<_> = dataset.samples.iter().take(5).map(|s| s.tensor.clone()).collect();
+        let tensors: Vec<_> = dataset
+            .samples
+            .iter()
+            .take(5)
+            .map(|s| s.tensor.clone())
+            .collect();
         let labels: Vec<_> = dataset.samples.iter().take(5).map(|s| s.label).collect();
 
         let (loss, gradients) = classifier.compute_loss(&tensors, &labels);
 
         // Loss should be positive
         assert!(loss > 0.0);
         assert!(loss.is_finite());
 
         // Gradients should have correct shapes
         assert_eq!(gradients.dw1.dim(), classifier.w1.dim());
         assert_eq!(gradients.db1.dim(), classifier.b1.dim());
         assert_eq!(gradients.dw2.dim(), classifier.w2.dim());
         assert_eq!(gradients.db2.dim(), classifier.b2.dim());
 
         // Gradients should be finite
         assert!(gradients.dw1.iter().all(|&v| v.is_finite()));
         assert!(gradients.db1.iter().all(|&v| v.is_finite()));
     }
 
     #[test]
     fn test_weight_update() {
         let config = ClassifierConfig::default();
         let mut classifier = MLPClassifier::new(config);
 
         let initial_w1 = classifier.w1.clone();
 
         // Create dummy gradients
         let dataset_config = DatasetConfig {
             tensor_size: (16, 16, 4),
             samples_per_class: 1,
             ..Default::default()
         };
         let dataset = ColorDataset::generate(dataset_config);
 
-        let tensors: Vec<_> = dataset.samples.iter().take(2).map(|s| s.tensor.clone()).collect();
+        let tensors: Vec<_> = dataset
+            .samples
+            .iter()
+            .take(2)
+            .map(|s| s.tensor.clone())
+            .collect();
         let labels: Vec<_> = dataset.samples.iter().take(2).map(|s| s.label).collect();
 
         let (_, gradients) = classifier.compute_loss(&tensors, &labels);
 
         // Update weights
         classifier.update_weights(&gradients, 0.01);
 
         // Weights should have changed
-        assert!(classifier.w1.iter().zip(initial_w1.iter()).any(|(a, b)| (a - b).abs() > 1e-6));
+        assert!(classifier
+            .w1
+            .iter()
+            .zip(initial_w1.iter())
+            .any(|(a, b)| (a - b).abs() > 1e-6));
     }
 }
diff --git a/src/learner/feedback.rs b/src/learner/feedback.rs
index 6c59ec9a89ae82263920ac5fa7c63fccee539a12..c3bc36b80394e08c1e772dc51e1ff38271f7b251 100644
--- a/src/learner/feedback.rs
+++ b/src/learner/feedback.rs
@@ -223,91 +223,87 @@ impl UtilityAggregator {
     }
 
     /// Filter records by minimum utility threshold.
     pub fn filter_by_utility(&self, min_utility: f32) -> Vec<&FeedbackRecord> {
         self.records
             .iter()
             .filter(|r| r.utility >= min_utility)
             .collect()
     }
 
     /// Get records from a specific class.
     pub fn filter_by_class(&self, class: ColorClass) -> Vec<&FeedbackRecord> {
         self.records
             .iter()
             .filter(|r| r.class_label == Some(class))
             .collect()
     }
 
     /// Compute correlation between spectral entropy and utility.
     ///
     /// Returns None if insufficient data with spectral features.
     pub fn entropy_utility_correlation(&self) -> Option<f32> {
         let pairs: Vec<(f32, f32)> = self
             .records
             .iter()
-            .filter_map(|r| {
-                r.spectral_features
-                    .as_ref()
-                    .map(|f| (f.entropy, r.utility))
-            })
+            .filter_map(|r| r.spectral_features.as_ref().map(|f| (f.entropy, r.utility)))
             .collect();
 
         if pairs.len() < 2 {
             return None;
         }
 
         Some(compute_correlation(&pairs))
     }
 
     /// Clear all records.
     pub fn clear(&mut self) {
         self.records.clear();
         self.class_stats.clear();
     }
 
     /// Recompute class statistics from records.
     fn recompute_stats(&mut self) {
         // Reset all stats
         for stats in self.class_stats.values_mut() {
             stats.mean_utility = 0.0;
             stats.mean_entropy = None;
         }
 
         // Group records by class
         let mut class_records: HashMap<ColorClass, Vec<&FeedbackRecord>> = HashMap::new();
         for record in &self.records {
             if let Some(class) = record.class_label {
                 class_records.entry(class).or_default().push(record);
             }
         }
 
         // Compute mean utility and entropy per class
         for (class, records) in class_records {
             if let Some(stats) = self.class_stats.get_mut(&class) {
-                let mean_utility: f32 = records.iter().map(|r| r.utility).sum::<f32>()
-                    / records.len() as f32;
+                let mean_utility: f32 =
+                    records.iter().map(|r| r.utility).sum::<f32>() / records.len() as f32;
                 stats.mean_utility = mean_utility;
 
                 // Compute mean entropy if available
                 let entropies: Vec<f32> = records
                     .iter()
                     .filter_map(|r| r.spectral_features.as_ref().map(|f| f.entropy))
                     .collect();
 
                 if !entropies.is_empty() {
                     let mean_entropy = entropies.iter().sum::<f32>() / entropies.len() as f32;
                     stats.mean_entropy = Some(mean_entropy);
                 }
             }
         }
     }
 }
 
 impl Default for UtilityAggregator {
     fn default() -> Self {
         Self::new()
     }
 }
 
 /// Compute Pearson correlation coefficient between two variables.
 ///
@@ -328,122 +324,128 @@ fn compute_correlation(pairs: &[(f32, f32)]) -> f32 {
     let mut cov = 0.0;
     let mut var_x = 0.0;
     let mut var_y = 0.0;
 
     for (x, y) in pairs {
         let dx = x - mean_x;
         let dy = y - mean_y;
         cov += dx * dy;
         var_x += dx * dx;
         var_y += dy * dy;
     }
 
     if var_x < 1e-10 || var_y < 1e-10 {
         return 0.0; // No variance
     }
 
     cov / (var_x * var_y).sqrt()
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
 
     #[test]
     fn test_feedback_record_creation() {
-        let record = FeedbackRecord::new(
-            [1.0, 0.0, 0.0],
-            Some(ColorClass::Red),
-            0.5,
-            0.3,
-            10,
-        );
+        let record = FeedbackRecord::new([1.0, 0.0, 0.0], Some(ColorClass::Red), 0.5, 0.3, 10);
 
         assert_eq!(record.chroma_signature, [1.0, 0.0, 0.0]);
         assert_eq!(record.class_label, Some(ColorClass::Red));
         assert_eq!(record.loss_before, 0.5);
         assert_eq!(record.loss_after, 0.3);
         assert!((record.delta_loss + 0.2).abs() < 0.01); // Loss decreased (floating point)
         assert!(record.utility > 0.0); // Positive utility (helpful)
         assert!(record.was_helpful());
         assert!(!record.was_harmful());
     }
 
     #[test]
     fn test_feedback_record_harmful() {
-        let record = FeedbackRecord::new(
-            [0.0, 1.0, 0.0],
-            Some(ColorClass::Green),
-            0.3,
-            0.5,
-            10,
-        );
+        let record = FeedbackRecord::new([0.0, 1.0, 0.0], Some(ColorClass::Green), 0.3, 0.5, 10);
 
         assert!((record.delta_loss - 0.2).abs() < 0.01); // Loss increased (floating point)
         assert!(record.utility < 0.0); // Negative utility (harmful)
         assert!(!record.was_helpful());
         assert!(record.was_harmful());
     }
 
     #[test]
     fn test_utility_aggregator_add() {
         let mut agg = UtilityAggregator::new();
         assert!(agg.is_empty());
 
         let record1 = FeedbackRecord::new([1.0, 0.0, 0.0], Some(ColorClass::Red), 0.5, 0.3, 1);
         let record2 = FeedbackRecord::new([0.0, 1.0, 0.0], Some(ColorClass::Green), 0.4, 0.2, 2);
 
         agg.add_record(record1);
         agg.add_record(record2);
 
         assert_eq!(agg.len(), 2);
         assert!(!agg.is_empty());
     }
 
     #[test]
     fn test_utility_aggregator_mean() {
         let mut agg = UtilityAggregator::new();
 
         // Helpful dream (utility = 0.2)
         agg.add_record(FeedbackRecord::new([1.0, 0.0, 0.0], None, 0.5, 0.3, 1));
         // Harmful dream (utility = -0.2)
         agg.add_record(FeedbackRecord::new([0.0, 1.0, 0.0], None, 0.3, 0.5, 2));
 
         let mean = agg.mean_utility();
         assert!((mean - 0.0).abs() < 0.01); // Should be ~0 (balanced)
     }
 
     #[test]
     fn test_utility_aggregator_class_stats() {
         let mut agg = UtilityAggregator::new();
 
         // Two helpful red dreams
-        agg.add_record(FeedbackRecord::new([1.0, 0.0, 0.0], Some(ColorClass::Red), 0.5, 0.3, 1));
-        agg.add_record(FeedbackRecord::new([0.9, 0.1, 0.0], Some(ColorClass::Red), 0.4, 0.2, 2));
+        agg.add_record(FeedbackRecord::new(
+            [1.0, 0.0, 0.0],
+            Some(ColorClass::Red),
+            0.5,
+            0.3,
+            1,
+        ));
+        agg.add_record(FeedbackRecord::new(
+            [0.9, 0.1, 0.0],
+            Some(ColorClass::Red),
+            0.4,
+            0.2,
+            2,
+        ));
 
         // One harmful green dream
-        agg.add_record(FeedbackRecord::new([0.0, 1.0, 0.0], Some(ColorClass::Green), 0.3, 0.5, 3));
+        agg.add_record(FeedbackRecord::new(
+            [0.0, 1.0, 0.0],
+            Some(ColorClass::Green),
+            0.3,
+            0.5,
+            3,
+        ));
 
         let red_stats = agg.class_stats(ColorClass::Red).unwrap();
         assert_eq!(red_stats.count, 2);
         assert_eq!(red_stats.helpful_count, 2);
         assert_eq!(red_stats.harmful_count, 0);
         assert!(red_stats.mean_utility > 0.0);
 
         let green_stats = agg.class_stats(ColorClass::Green).unwrap();
         assert_eq!(green_stats.count, 1);
         assert_eq!(green_stats.helpful_count, 0);
         assert_eq!(green_stats.harmful_count, 1);
         assert!(green_stats.mean_utility < 0.0);
     }
 
     #[test]
     fn test_top_k_helpful() {
         let mut agg = UtilityAggregator::new();
 
         agg.add_record(FeedbackRecord::new([1.0, 0.0, 0.0], None, 0.5, 0.1, 1)); // utility = 0.4
         agg.add_record(FeedbackRecord::new([0.0, 1.0, 0.0], None, 0.5, 0.3, 2)); // utility = 0.2
         agg.add_record(FeedbackRecord::new([0.0, 0.0, 1.0], None, 0.5, 0.4, 3)); // utility = 0.1
 
         let top2 = agg.top_k_helpful(2);
         assert_eq!(top2.len(), 2);
         assert!(top2[0].utility > top2[1].utility);
diff --git a/src/learner/mod.rs b/src/learner/mod.rs
index ef4432f40a37076438394ab05f41ce58f893421a..1c914c3764522a3a98c4ad7532109bd1786edc4d 100644
--- a/src/learner/mod.rs
+++ b/src/learner/mod.rs
@@ -1,20 +1,20 @@
 //! Learner module - analytical half of the Dreamer-Learner system
 //!
 //! The Learner extracts structure from Dream Pool entries, evaluates utility,
 //! and provides feedback to bias the Dreamer via the retrieval mechanism.
 //!
 //! This is the Minimal Viable Learner (MVP) that implements:
 //! - Color classification via MLP
 //! - Gradient descent training
 //! - Dream Pool integration for retrieval-based seeding
 //! - Basic feedback collection (Δloss tracking)
 //!
 //! Future expansion path to full LEARNER MANIFEST v1.0 features.
 
 pub mod classifier;
 pub mod feedback;
 pub mod training;
 
-pub use classifier::{ColorClassifier, MLPClassifier, ClassifierConfig};
-pub use feedback::{FeedbackRecord, UtilityAggregator, ClassUtilityStats};
-pub use training::{TrainingConfig, TrainingResult, train_with_dreams};
+pub use classifier::{ClassifierConfig, ColorClassifier, MLPClassifier};
+pub use feedback::{ClassUtilityStats, FeedbackRecord, UtilityAggregator};
+pub use training::{train_with_dreams, TrainingConfig, TrainingResult};
diff --git a/src/learner/training.rs b/src/learner/training.rs
index d5d62daad95d689c86e42b686d3e6c0746105413..d1dc03b3f6454588856f5ef22f58310bec1ac23b 100644
--- a/src/learner/training.rs
+++ b/src/learner/training.rs
@@ -1,37 +1,37 @@
 //! Training loop with Dream Pool integration
 //!
 //! Implements gradient descent training with optional retrieval-based seeding
 //! from the Dream Pool to accelerate convergence.
 
 use crate::data::ColorSample;
 use crate::dream::{RetrievalMode, SimpleDreamPool};
 use crate::learner::classifier::{ColorClassifier, MLPClassifier};
 use crate::solver::Solver;
-use crate::tensor::ChromaticTensor;
 use crate::tensor::operations::mix;
-use serde::{Serialize, Deserialize};
+use crate::tensor::ChromaticTensor;
+use serde::{Deserialize, Serialize};
 use std::time::Instant;
 
 /// Training configuration
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct TrainingConfig {
     /// Number of training epochs
     pub num_epochs: usize,
     /// Batch size for training
     pub batch_size: usize,
     /// Learning rate
     pub learning_rate: f32,
     /// Learning rate decay per epoch
     pub lr_decay: f32,
     /// Whether to use Dream Pool retrieval
     pub use_dream_pool: bool,
     /// Number of dreams to retrieve per sample
     pub num_dreams_retrieve: usize,
     /// Retrieval mode: Hard (Phase 3B), Soft (Phase 4), or Hybrid
     pub retrieval_mode: RetrievalMode,
     /// Seed for reproducibility
     pub seed: u64,
 }
 
 impl Default for TrainingConfig {
     fn default() -> Self {
@@ -51,54 +51,51 @@ impl Default for TrainingConfig {
 /// Training metrics for a single epoch
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct EpochMetrics {
     pub epoch: usize,
     pub train_loss: f32,
     pub train_accuracy: f32,
     pub val_loss: f32,
     pub val_accuracy: f32,
     pub learning_rate: f32,
     pub elapsed_ms: u128,
     pub dreams_used: usize,
 }
 
 /// Complete training result
 #[derive(Debug, Clone, Serialize)]
 pub struct TrainingResult {
     pub config: TrainingConfig,
     pub epoch_metrics: Vec<EpochMetrics>,
     pub final_train_accuracy: f32,
     pub final_val_accuracy: f32,
     pub total_elapsed_ms: u128,
     pub converged_epoch: Option<usize>,
 }
 
 /// Compute accuracy on a dataset
-fn compute_accuracy<C: ColorClassifier>(
-    classifier: &C,
-    samples: &[ColorSample],
-) -> f32 {
+fn compute_accuracy<C: ColorClassifier>(classifier: &C, samples: &[ColorSample]) -> f32 {
     if samples.is_empty() {
         return 0.0;
     }
 
     let correct = samples
         .iter()
         .filter(|sample| {
             let predicted = classifier.predict(&sample.tensor);
             predicted == sample.label
         })
         .count();
 
     correct as f32 / samples.len() as f32
 }
 
 /// Augment a tensor with retrieved dreams from the pool
 fn augment_with_dreams(
     tensor: &ChromaticTensor,
     pool: &SimpleDreamPool,
     num_dreams: usize,
 ) -> ChromaticTensor {
     if pool.is_empty() {
         return tensor.clone();
     }
 
@@ -203,80 +200,78 @@ pub fn train_with_dreams<S: Solver>(
         // Compute validation loss
         let val_tensors: Vec<_> = val_data.iter().map(|s| s.tensor.clone()).collect();
         let val_labels: Vec<_> = val_data.iter().map(|s| s.label).collect();
         let (val_loss, _) = classifier.compute_loss(&val_tensors, &val_labels);
 
         epoch_metrics.push(EpochMetrics {
             epoch,
             train_loss: avg_train_loss,
             train_accuracy,
             val_loss,
             val_accuracy,
             learning_rate: current_lr,
             elapsed_ms: epoch_start.elapsed().as_millis(),
             dreams_used,
         });
 
         // Check for convergence (95% val accuracy)
         if converged_epoch.is_none() && val_accuracy >= 0.95 {
             converged_epoch = Some(epoch);
         }
 
         // Learning rate decay
         current_lr *= config.lr_decay;
     }
 
-    let final_train_accuracy = epoch_metrics.last().map(|m| m.train_accuracy).unwrap_or(0.0);
+    let final_train_accuracy = epoch_metrics
+        .last()
+        .map(|m| m.train_accuracy)
+        .unwrap_or(0.0);
     let final_val_accuracy = epoch_metrics.last().map(|m| m.val_accuracy).unwrap_or(0.0);
 
     TrainingResult {
         config,
         epoch_metrics,
         final_train_accuracy,
         final_val_accuracy,
         total_elapsed_ms: start_time.elapsed().as_millis(),
         converged_epoch,
     }
 }
 
 /// Simple training without Dream Pool (for baseline comparison)
 pub fn train_baseline(
     classifier: MLPClassifier,
     train_data: &[ColorSample],
     val_data: &[ColorSample],
     config: TrainingConfig,
 ) -> TrainingResult {
     let mut config = config;
     config.use_dream_pool = false;
 
     train_with_dreams::<crate::ChromaticNativeSolver>(
-        classifier,
-        train_data,
-        val_data,
-        config,
-        None,
-        None,
+        classifier, train_data, val_data, config, None, None,
     )
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
     use crate::data::{ColorDataset, DatasetConfig};
     use crate::dream::simple_pool::PoolConfig;
     use crate::learner::classifier::ClassifierConfig;
     use crate::ChromaticNativeSolver;
 
     #[test]
     fn test_compute_accuracy() {
         let config = ClassifierConfig::default();
         let classifier = MLPClassifier::new(config);
 
         let dataset_config = DatasetConfig {
             tensor_size: (16, 16, 4),
             samples_per_class: 10,
             ..Default::default()
         };
         let dataset = ColorDataset::generate(dataset_config);
 
         let accuracy = compute_accuracy(&classifier, &dataset.samples);
 
diff --git a/src/neural/loss.rs b/src/neural/loss.rs
index f2f057f42dfbbf6eb8708f578708090aa82860a8..e60332556f12d66ca97307ac60227392e80132d3 100644
--- a/src/neural/loss.rs
+++ b/src/neural/loss.rs
@@ -34,74 +34,88 @@ pub fn mse_loss_with_gradients(
 /// Takes the mean color of the final tensor as logits for each class.
 ///
 /// # Arguments
 ///
 /// * `predicted` - Predicted chromatic tensor (final layer)
 /// * `label` - Target class label
 /// * `num_classes` - Total number of classes
 ///
 /// # Returns
 ///
 /// Tuple of (loss value, gradient w.r.t. predicted colors)
 pub fn cross_entropy_loss(
     predicted: &ChromaticTensor,
     label: usize,
     num_classes: usize,
 ) -> (f32, ChromaticTensor) {
     // Extract logits from mean RGB values
     let stats = predicted.statistics();
     let logits = stats.mean_rgb;
 
     // For 3-class problem, use RGB channels as logits
     // For other cases, we'd need to extend this
     assert!(num_classes <= 3, "Currently only supports up to 3 classes");
 
     // Softmax
-    let max_logit = logits[..num_classes].iter().cloned().fold(f32::NEG_INFINITY, f32::max);
-    let exp_sum: f32 = logits[..num_classes].iter().map(|&x| (x - max_logit).exp()).sum();
-    let probs: Vec<f32> = logits[..num_classes].iter().map(|&x| (x - max_logit).exp() / exp_sum).collect();
+    let max_logit = logits[..num_classes]
+        .iter()
+        .cloned()
+        .fold(f32::NEG_INFINITY, f32::max);
+    let exp_sum: f32 = logits[..num_classes]
+        .iter()
+        .map(|&x| (x - max_logit).exp())
+        .sum();
+    let probs: Vec<f32> = logits[..num_classes]
+        .iter()
+        .map(|&x| (x - max_logit).exp() / exp_sum)
+        .collect();
 
     // Cross-entropy loss
     let loss = -probs[label].ln();
 
     // Gradient: softmax gradient
     let mut grad_logits = [0.0f32; 3];
     for i in 0..num_classes {
         if i == label {
             grad_logits[i] = probs[i] - 1.0;
         } else {
             grad_logits[i] = probs[i];
         }
     }
 
     // Scale gradient back to tensor
     // Gradient is uniform across all spatial locations and layers
     let (rows, cols, layers, _) = predicted.shape();
     let n = (rows * cols * layers) as f32;
 
     let mut gradient = predicted.clone();
-    for val in gradient.colors.as_slice_mut().expect("contiguous").chunks_exact_mut(3) {
+    for val in gradient
+        .colors
+        .as_slice_mut()
+        .expect("contiguous")
+        .chunks_exact_mut(3)
+    {
         val[0] = grad_logits[0] / n;
         val[1] = grad_logits[1] / n;
         val[2] = grad_logits[2] / n;
     }
 
     (loss, gradient)
 }
 
 /// Computes accuracy for classification.
 ///
 /// # Arguments
 ///
 /// * `predicted` - Predicted chromatic tensor
 /// * `label` - True label
 /// * `num_classes` - Number of classes
 ///
 /// # Returns
 ///
 /// 1.0 if prediction is correct, 0.0 otherwise
 pub fn accuracy(predicted: &ChromaticTensor, label: usize, num_classes: usize) -> f32 {
     let stats = predicted.statistics();
     let logits = stats.mean_rgb;
 
     // Find argmax
     let mut max_idx = 0;
@@ -130,38 +144,43 @@ mod tests {
         let target = ChromaticTensor::from_seed(100, 4, 4, 2);
 
         let (loss, gradient) = mse_loss_with_gradients(&predicted, &target);
 
         assert!(loss > 0.0);
         assert_eq!(gradient.shape(), predicted.shape());
     }
 
     #[test]
     fn test_cross_entropy() {
         let predicted = ChromaticTensor::from_seed(42, 4, 4, 2);
         let label = 0;
         let num_classes = 3;
 
         let (loss, gradient) = cross_entropy_loss(&predicted, label, num_classes);
 
         assert!(loss > 0.0);
         assert_eq!(gradient.shape(), predicted.shape());
     }
 
     #[test]
     fn test_accuracy() {
         let mut tensor = ChromaticTensor::from_seed(42, 4, 4, 2);
 
         // Manually set colors to favor class 0 (red)
-        for val in tensor.colors.as_slice_mut().expect("contiguous").chunks_exact_mut(3) {
+        for val in tensor
+            .colors
+            .as_slice_mut()
+            .expect("contiguous")
+            .chunks_exact_mut(3)
+        {
             val[0] = 1.0; // Red
             val[1] = 0.0; // Green
             val[2] = 0.0; // Blue
         }
 
         let acc = accuracy(&tensor, 0, 3);
         assert_eq!(acc, 1.0);
 
         let acc_wrong = accuracy(&tensor, 1, 3);
         assert_eq!(acc_wrong, 0.0);
     }
 }
diff --git a/src/neural/network.rs b/src/neural/network.rs
index 71b44d1dcee36b7f18131fe1afd81771f5b06b73..e54fd3f702d039ae482a0b2c6ce65c3fa09269d5 100644
--- a/src/neural/network.rs
+++ b/src/neural/network.rs
@@ -279,71 +279,73 @@ impl ChromaticNetwork {
     ///
     /// # Returns
     ///
     /// Tuple of (average loss, average accuracy)
     pub fn evaluate(&mut self, inputs: &[ChromaticTensor], labels: &[usize]) -> (f32, f32) {
         let mut total_loss = 0.0;
         let mut total_acc = 0.0;
 
         for (input, &label) in inputs.iter().zip(labels.iter()) {
             let (loss, acc) = self.compute_loss(input, label);
             total_loss += loss;
             total_acc += acc;
         }
 
         let n = inputs.len() as f32;
         (total_loss / n, total_acc / n)
     }
 }
 
 impl Checkpointable for ChromaticNetwork {
     fn save_checkpoint<P: AsRef<std::path::Path>>(&self, path: P) -> Result<(), CheckpointError> {
         let snapshot = ChromaticNetworkCheckpoint {
             version: NETWORK_CHECKPOINT_VERSION,
             layers: self.layers.clone(),
             num_classes: self.num_classes,
-            config: Some(NetworkConfigSnapshot::from_layers(&self.layers, self.num_classes)),
+            config: Some(NetworkConfigSnapshot::from_layers(
+                &self.layers,
+                self.num_classes,
+            )),
             optimizer_state: self.optimizer_state.clone(),
         };
 
         Self::write_snapshot(&snapshot, path)
     }
 
     fn load_checkpoint<P: AsRef<std::path::Path>>(path: P) -> Result<Self, CheckpointError> {
         let snapshot: ChromaticNetworkCheckpoint = Self::read_snapshot(path)?;
         let mut snapshot = snapshot;
         if snapshot.version != NETWORK_CHECKPOINT_VERSION {
             return Err(CheckpointError::VersionMismatch {
                 expected: NETWORK_CHECKPOINT_VERSION,
                 found: snapshot.version,
             });
         }
 
-        let config = snapshot
-            .config
-            .take()
-            .unwrap_or_else(|| NetworkConfigSnapshot::from_layers(&snapshot.layers, snapshot.num_classes));
+        let config = snapshot.config.take().unwrap_or_else(|| {
+            NetworkConfigSnapshot::from_layers(&snapshot.layers, snapshot.num_classes)
+        });
         config.validate_layers(&snapshot.layers)?;
 
         Ok(Self {
             layers: snapshot.layers,
             num_classes: snapshot.num_classes,
             optimizer_state: snapshot.optimizer_state,
         })
     }
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
 
     #[test]
     fn test_network_creation() {
         let net = ChromaticNetwork::simple((16, 16, 4), 3, 42);
         assert_eq!(net.layers.len(), 2);
         assert_eq!(net.num_classes, 3);
         assert!(net.optimizer_state().is_none());
     }
 
     #[test]
     fn test_forward_pass() {
         let mut net = ChromaticNetwork::simple((8, 8, 2), 3, 42);
diff --git a/src/neural/optimizer.rs b/src/neural/optimizer.rs
index b9e2413fde43a95f6d1253dc6f92731b056d83b3..8fcb1c012b1dee3dd97d07e0c92c7cc50bfb542b 100644
--- a/src/neural/optimizer.rs
+++ b/src/neural/optimizer.rs
@@ -75,61 +75,62 @@ impl SGDOptimizer {
     }
 
     pub fn to_state(&self) -> SGDOptimizerState {
         SGDOptimizerState {
             learning_rate: self.learning_rate,
             momentum: self.momentum,
             weight_decay: self.weight_decay,
             velocities: self.velocities.clone(),
         }
     }
 
     pub fn apply_state(&mut self, state: SGDOptimizerState) {
         self.learning_rate = state.learning_rate;
         self.momentum = state.momentum;
         self.weight_decay = state.weight_decay;
         self.velocities = state.velocities;
     }
 
     /// Updates a parameter using accumulated gradients.
     ///
     /// # Arguments
     ///
     /// * `param_name` - Unique identifier for this parameter
     /// * `param` - Parameter tensor to update (modified in-place)
     /// * `gradient` - Gradient tensor
-    pub fn step(&mut self, param_name: &str, param: &mut ChromaticTensor, gradient: &ChromaticTensor) {
+    pub fn step(
+        &mut self,
+        param_name: &str,
+        param: &mut ChromaticTensor,
+        gradient: &ChromaticTensor,
+    ) {
         // Get or initialize velocity for this parameter
         let velocity = self
             .velocities
             .entry(param_name.to_string())
             .or_insert_with(|| {
-                ChromaticTensor::new(
-                    param.shape().0,
-                    param.shape().1,
-                    param.shape().2,
-                )
+                ChromaticTensor::new(param.shape().0, param.shape().1, param.shape().2)
             });
 
         // Apply weight decay (L2 regularization)
         let mut grad_with_decay = gradient.clone();
         if self.weight_decay > 0.0 {
             grad_with_decay = grad_with_decay + param.clone() * self.weight_decay;
         }
 
         // Update velocity: v = momentum * v + lr * gradient
         *velocity = velocity.clone() * self.momentum + grad_with_decay * self.learning_rate;
 
         // Update parameter: param = param - velocity
         *param = param.clone() - velocity.clone();
     }
 
     /// Resets all accumulated velocities.
     pub fn zero_grad(&mut self) {
         self.velocities.clear();
     }
 }
 
 /// Adam optimizer (Adaptive Moment Estimation).
 ///
 /// Implements adaptive learning rates for each parameter.
 pub struct AdamOptimizer {
@@ -169,88 +170,95 @@ impl AdamOptimizer {
     pub fn to_state(&self) -> AdamOptimizerState {
         AdamOptimizerState {
             learning_rate: self.learning_rate,
             beta1: self.beta1,
             beta2: self.beta2,
             epsilon: self.epsilon,
             weight_decay: self.weight_decay,
             first_moments: self.first_moments.clone(),
             second_moments: self.second_moments.clone(),
             t: self.t,
         }
     }
 
     pub fn apply_state(&mut self, state: AdamOptimizerState) {
         self.learning_rate = state.learning_rate;
         self.beta1 = state.beta1;
         self.beta2 = state.beta2;
         self.epsilon = state.epsilon;
         self.weight_decay = state.weight_decay;
         self.first_moments = state.first_moments;
         self.second_moments = state.second_moments;
         self.t = state.t;
     }
 
     /// Updates a parameter using Adam algorithm.
-    pub fn step(&mut self, param_name: &str, param: &mut ChromaticTensor, gradient: &ChromaticTensor) {
+    pub fn step(
+        &mut self,
+        param_name: &str,
+        param: &mut ChromaticTensor,
+        gradient: &ChromaticTensor,
+    ) {
         self.t += 1;
 
         // Get or initialize moments
         let m = self
             .first_moments
             .entry(param_name.to_string())
             .or_insert_with(|| {
                 ChromaticTensor::new(param.shape().0, param.shape().1, param.shape().2)
             });
 
         let v = self
             .second_moments
             .entry(param_name.to_string())
             .or_insert_with(|| {
                 ChromaticTensor::new(param.shape().0, param.shape().1, param.shape().2)
             });
 
         // Apply weight decay
         let mut grad_with_decay = gradient.clone();
         if self.weight_decay > 0.0 {
             grad_with_decay = grad_with_decay + param.clone() * self.weight_decay;
         }
 
         // Update biased first moment: m = beta1 * m + (1 - beta1) * gradient
         *m = m.clone() * self.beta1 + grad_with_decay.clone() * (1.0 - self.beta1);
 
         // Update biased second moment: v = beta2 * v + (1 - beta2) * gradient^2
         let grad_squared = element_wise_square(&grad_with_decay);
         *v = v.clone() * self.beta2 + grad_squared * (1.0 - self.beta2);
 
         // Bias correction
         let m_hat = m.clone() * (1.0 / (1.0 - self.beta1.powi(self.t as i32)));
         let v_hat = v.clone() * (1.0 / (1.0 - self.beta2.powi(self.t as i32)));
 
         // Update parameter: param = param - lr * m_hat / (sqrt(v_hat) + epsilon)
         let v_sqrt = element_wise_sqrt(&v_hat);
-        let denominator = v_sqrt + ChromaticTensor::new(param.shape().0, param.shape().1, param.shape().2) * self.epsilon;
+        let denominator = v_sqrt
+            + ChromaticTensor::new(param.shape().0, param.shape().1, param.shape().2)
+                * self.epsilon;
         let update = element_wise_divide(&m_hat, &denominator) * self.learning_rate;
 
         *param = param.clone() - update;
     }
 
     /// Resets all accumulated moments.
     pub fn zero_grad(&mut self) {
         self.first_moments.clear();
         self.second_moments.clear();
         self.t = 0;
     }
 }
 
 // Helper functions for element-wise operations
 fn element_wise_square(tensor: &ChromaticTensor) -> ChromaticTensor {
     let mut result = tensor.clone();
     for val in result.colors.as_slice_mut().expect("contiguous") {
         *val = *val * *val;
     }
     result
 }
 
 fn element_wise_sqrt(tensor: &ChromaticTensor) -> ChromaticTensor {
     let mut result = tensor.clone();
     for val in result.colors.as_slice_mut().expect("contiguous") {
diff --git a/src/solver/native.rs b/src/solver/native.rs
index ccfed136ed90cfaacda9701d6ff0f3286ea62908..8d64fa2eb6471664bdc0791c9a5406f1b472d439 100644
--- a/src/solver/native.rs
+++ b/src/solver/native.rs
@@ -1,76 +1,85 @@
+use crate::solver::{Solver, SolverResult};
 /// Native Rust implementation of chromatic field solver
 ///
 /// This solver computes color-theory-informed metrics directly without
 /// external dependencies. It provides:
 ///
 /// - **Energy**: Total variation (smoothness) + saturation penalty
 /// - **Coherence**: Color harmony based on complementary balance and hue consistency
 /// - **Violation**: Gamut clipping, extreme saturation, local discontinuities
 ///
 /// All metrics have analytical gradients for efficient training.
-
 use crate::tensor::ChromaticTensor;
-use crate::solver::{Solver, SolverResult};
 use anyhow::Result;
 use serde_json::json;
 
 /// Native Rust solver with color-space metrics
 pub struct ChromaticNativeSolver {
     /// Weight for total variation term in energy
     pub lambda_tv: f32,
 
     /// Weight for saturation penalty term in energy
     pub lambda_sat: f32,
 
     /// Target saturation (0.5 = neutral, deviations penalized)
     pub target_saturation: f32,
 
     /// Threshold for local discontinuity detection (ΔE)
     pub discontinuity_threshold: f32,
 }
 
 impl Default for ChromaticNativeSolver {
     fn default() -> Self {
         Self {
             lambda_tv: 1.0,
             lambda_sat: 0.1,
             target_saturation: 0.5,
             discontinuity_threshold: 0.3,
         }
     }
 }
 
 impl ChromaticNativeSolver {
     /// Create a new native solver with default parameters
     pub fn new() -> Self {
         Self::default()
     }
 
     /// Create a solver with custom parameters
-    pub fn with_params(lambda_tv: f32, lambda_sat: f32, target_saturation: f32, discontinuity_threshold: f32) -> Self {
-        Self { lambda_tv, lambda_sat, target_saturation, discontinuity_threshold }
+    pub fn with_params(
+        lambda_tv: f32,
+        lambda_sat: f32,
+        target_saturation: f32,
+        discontinuity_threshold: f32,
+    ) -> Self {
+        Self {
+            lambda_tv,
+            lambda_sat,
+            target_saturation,
+            discontinuity_threshold,
+        }
     }
 
     /// Compute total variation (spatial smoothness penalty)
     ///
     /// TV(F) = Σ ‖F[i,j,l] - F[i+1,j,l]‖ + ‖F[i,j,l] - F[i,j+1,l]‖
     fn compute_total_variation(&self, field: &ChromaticTensor) -> f32 {
         let mut tv = 0.0;
 
         for l in 0..field.layers() {
             for r in 0..field.rows() {
                 for c in 0..field.cols() {
                     let curr = field.get_rgb(r, c, l);
 
                     // Right neighbor
                     if r + 1 < field.rows() {
                         let right = field.get_rgb(r + 1, c, l);
                         tv += rgb_distance(&curr, &right);
                     }
 
                     // Down neighbor
                     if c + 1 < field.cols() {
                         let down = field.get_rgb(r, c + 1, l);
                         tv += rgb_distance(&curr, &down);
                     }
                 }
@@ -87,85 +96,85 @@ impl ChromaticNativeSolver {
         let mut penalty = 0.0;
         let total_cells = (field.rows() * field.cols() * field.layers()) as f32;
 
         for l in 0..field.layers() {
             for r in 0..field.rows() {
                 for c in 0..field.cols() {
                     let rgb = field.get_rgb(r, c, l);
                     let saturation = rgb_saturation(&rgb);
                     let diff = saturation - self.target_saturation;
                     penalty += diff * diff;
                 }
             }
         }
 
         penalty / total_cells
     }
 
     /// Compute color harmony score (0-1, higher is better)
     ///
     /// Measures:
     /// - Complementary balance (red-cyan, green-magenta, yellow-blue)
     /// - Hue consistency (lower std dev = more coherent)
     fn compute_color_harmony(&self, field: &ChromaticTensor) -> f32 {
         // 1. Complementary balance
         let mean_rgb = field.mean_rgb();
-        let complementary_balance = 1.0 - (
-            (mean_rgb[0] - 0.5).abs() +
-            (mean_rgb[1] - 0.5).abs() +
-            (mean_rgb[2] - 0.5).abs()
-        ) / 1.5; // Normalize to [0,1]
+        let complementary_balance = 1.0
+            - ((mean_rgb[0] - 0.5).abs() + (mean_rgb[1] - 0.5).abs() + (mean_rgb[2] - 0.5).abs())
+                / 1.5; // Normalize to [0,1]
 
         // 2. Hue consistency
         let hue_std = self.compute_hue_std_dev(field);
         let hue_consistency = 1.0 - (hue_std / 180.0).min(1.0);
 
         // Combine metrics
         0.6 * complementary_balance + 0.4 * hue_consistency
     }
 
     /// Compute standard deviation of hue angles
     fn compute_hue_std_dev(&self, field: &ChromaticTensor) -> f32 {
         let mut hues = Vec::new();
 
         for l in 0..field.layers() {
             for r in 0..field.rows() {
                 for c in 0..field.cols() {
                     let rgb = field.get_rgb(r, c, l);
                     let (_, _, hue) = rgb_to_hsv(&rgb);
                     hues.push(hue);
                 }
             }
         }
 
         let mean_hue: f32 = hues.iter().sum::<f32>() / hues.len() as f32;
-        let variance: f32 = hues.iter()
+        let variance: f32 = hues
+            .iter()
             .map(|&h| {
                 let diff = angle_difference(h, mean_hue);
                 diff * diff
             })
-            .sum::<f32>() / hues.len() as f32;
+            .sum::<f32>()
+            / hues.len() as f32;
 
         variance.sqrt()
     }
 
     /// Compute constraint violation score (0-1, lower is better)
     ///
     /// Measures:
     /// - Out-of-gamut pixels (RGB outside [0,1])
     /// - Extreme saturation (> 0.95)
     /// - Local discontinuities (sharp color jumps)
     fn compute_constraint_violation(&self, field: &ChromaticTensor) -> f32 {
         let total_cells = (field.rows() * field.cols() * field.layers()) as f32;
         let mut violation_count = 0.0;
 
         for l in 0..field.layers() {
             for r in 0..field.rows() {
                 for c in 0..field.cols() {
                     let rgb = field.get_rgb(r, c, l);
 
                     // 1. Out-of-gamut check
                     if rgb.iter().any(|&v| v < 0.0 || v > 1.0) {
                         violation_count += 1.0;
                     }
 
                     // 2. Extreme saturation check
diff --git a/src/tensor/gradient.rs b/src/tensor/gradient.rs
index 744750746fd3e7f6b520e04d09965214edc76738..5686d22e1ea50efd0e34836cba49d45168cf93f2 100644
--- a/src/tensor/gradient.rs
+++ b/src/tensor/gradient.rs
@@ -1,29 +1,29 @@
 use std::io;
 use std::path::Path;
 
-use ndarray::{Array3, s};
+use ndarray::{s, Array3};
 use plotters::prelude::*;
 use rayon::prelude::*;
 
 use super::ChromaticTensor;
 
 #[derive(Debug, Clone)]
 pub struct GradientLayer {
     pub image: Array3<f32>,
 }
 
 impl GradientLayer {
     pub fn from_tensor(tensor: &ChromaticTensor) -> Self {
         let (rows, cols, layers, _) = tensor.colors.dim();
         let mut image = Array3::zeros((rows, cols, 3));
 
         image
             .indexed_iter_mut()
             .par_bridge()
             .for_each(|((row, col, channel), value)| {
                 let mut numerator = 0.0f32;
                 let mut denominator = 0.0f32;
                 for layer in 0..layers {
                     let weight = tensor.certainty[[row, col, layer]].max(0.0);
                     let color = tensor.colors[[row, col, layer, channel]];
                     numerator += color * weight;
diff --git a/tests/operations.rs b/tests/operations.rs
index 043f6838fd0711cd3289aeb06abe61df4fa540f2..c95a2014dcabbd52b31444454c52f07a4408f964 100644
--- a/tests/operations.rs
+++ b/tests/operations.rs
@@ -1,27 +1,27 @@
 use chromatic_cognition_core::{
-    ChromaticTensor, GradientLayer, complement, filter, mix, mse_loss, saturate,
+    complement, filter, mix, mse_loss, saturate, ChromaticTensor, GradientLayer,
 };
 use ndarray::{Array3, Array4};
 
 fn sample_tensor(values: [[f32; 3]; 2]) -> ChromaticTensor {
     let colors = Array4::from_shape_vec((1, 1, 2, 3), values.into_iter().flatten().collect())
         .expect("shape matches");
     let certainty = Array3::from_elem((1, 1, 2), 1.0);
     ChromaticTensor::from_arrays(colors, certainty)
 }
 
 #[test]
 fn mix_adds_and_normalizes() {
     let a = sample_tensor([[0.2, 0.4, 0.6], [0.1, 0.2, 0.3]]);
     let b = sample_tensor([[0.3, 0.3, 0.3], [0.2, 0.5, 0.7]]);
     let mixed = mix(&a, &b);
     let data = mixed.colors;
     assert!((data[[0, 0, 0, 0]] - 0.5).abs() < 1e-6);
     assert!((data[[0, 0, 0, 1]] - 0.7).abs() < 1e-6);
     assert!((data[[0, 0, 1, 2]] - 1.0).abs() < 1e-6);
 }
 
 #[test]
 fn filter_subtracts_and_clamps() {
     let a = sample_tensor([[0.4, 0.5, 0.6], [0.8, 0.1, 0.3]]);
     let b = sample_tensor([[0.2, 0.7, 0.1], [0.4, 0.5, 0.9]]);
@@ -33,42 +33,47 @@ fn filter_subtracts_and_clamps() {
 }
 
 #[test]
 fn complement_inverts_green_and_blue() {
     let tensor = sample_tensor([[0.1, 0.2, 0.3], [0.5, 0.6, 0.7]]);
     let complemented = complement(&tensor);
     let data = complemented.colors;
     assert!((data[[0, 0, 0, 0]] - 0.1).abs() < 1e-6);
     assert!((data[[0, 0, 0, 1]] - 0.8).abs() < 1e-6);
     assert!((data[[0, 0, 1, 2]] - 0.3).abs() < 1e-6);
 }
 
 #[test]
 fn saturate_stretches_chroma() {
     let tensor = sample_tensor([[0.3, 0.4, 0.5], [0.2, 0.4, 0.6]]);
     let saturated = saturate(&tensor, 1.5);
     let data = saturated.colors;
     assert!(data[[0, 0, 0, 0]] < tensor.colors[[0, 0, 0, 0]]);
     assert!(data[[0, 0, 0, 2]] > tensor.colors[[0, 0, 0, 2]]);
 }
 
 #[test]
 fn gradient_layer_averages_across_layers() {
     let tensor = sample_tensor([[0.0, 0.5, 1.0], [1.0, 0.5, 0.0]]);
     let gradient = GradientLayer::from_tensor(&tensor);
-    println!("R: {}, G: {}, B: {}", gradient.image[[0, 0, 0]], gradient.image[[0, 0, 1]], gradient.image[[0, 0, 2]]);
+    println!(
+        "R: {}, G: {}, B: {}",
+        gradient.image[[0, 0, 0]],
+        gradient.image[[0, 0, 1]],
+        gradient.image[[0, 0, 2]]
+    );
     println!("Expected: R: 0.5, G: 0.5, B: 0.5");
     // The test expects to average [0.0, 0.5, 1.0] and [1.0, 0.5, 0.0] across 2 layers
     // Result should be [(0.0+1.0)/2, (0.5+0.5)/2, (1.0+0.0)/2] = [0.5, 0.5, 0.5]
     assert!((gradient.image[[0, 0, 0]] - 0.5).abs() < 1e-6);
     assert!((gradient.image[[0, 0, 1]] - 0.5).abs() < 1e-6);
     assert!((gradient.image[[0, 0, 2]] - 0.5).abs() < 1e-6);
 }
 
 #[test]
 fn mse_loss_computes_mean_squared_error() {
     let a = sample_tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]);
     let b = sample_tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2]]);
     let metrics = mse_loss(&a, &b);
     assert!(metrics.loss > 0.0);
     assert!(metrics.mean_rgb[0] > 0.0);
 }
diff --git a/tests/validation_harness.rs b/tests/validation_harness.rs
index bb277f8722ec65ca5dac84d7e75a5a495b818a51..f8c2876bf16690ce80bcd2d4a37438fc50ef4998 100644
--- a/tests/validation_harness.rs
+++ b/tests/validation_harness.rs
@@ -1,27 +1,27 @@
-use chromatic_cognition_core::{mix, ChromaticTensor, SimpleDreamPool, SolverResult};
 use chromatic_cognition_core::dream::simple_pool::{DreamEntry, PoolConfig};
+use chromatic_cognition_core::{mix, ChromaticTensor, SimpleDreamPool, SolverResult};
 use rand::rngs::StdRng;
 use rand::{Rng, SeedableRng};
 use serde::{Deserialize, Serialize};
 use serde_json::json;
 
 const NUM_EPOCHS: usize = 100;
 const POOL_MAX_SIZE: usize = 50;
 const COHERENCE_THRESHOLD: f64 = 0.75;
 const RETRIEVAL_LIMIT: usize = 3;
 const ALPHA_BLEND: f32 = 0.5;
 const RANDOM_SEED: u64 = 42;
 
 #[derive(Clone, Copy, Debug)]
 enum Group {
     Control,
     Test,
 }
 
 impl Group {
     fn as_str(&self) -> &'static str {
         match self {
             Group::Control => "Control",
             Group::Test => "Test",
         }
     }
@@ -69,109 +69,124 @@ fn execute_harness(seed: u64) -> HarnessRun {
         retrieval_limit: RETRIEVAL_LIMIT,
         use_hnsw: true,
         memory_budget_mb: Some(500),
     };
 
     let mut control_pool = SimpleDreamPool::new(pool_config.clone());
     let mut test_pool = SimpleDreamPool::new(pool_config);
 
     let mut control_rng = StdRng::seed_from_u64(seed);
     let mut test_rng = StdRng::seed_from_u64(seed);
 
     let mut csv_rows = vec!["epoch,group,avg_loss,avg_coherence".to_string()];
     let mut json_records = Vec::with_capacity(NUM_EPOCHS * 2);
 
     let mut final_control = None;
     let mut final_test = None;
 
     for epoch in 0..NUM_EPOCHS {
         let control_seed = ChromaticTensor::from_seed(control_rng.gen(), 8, 8, 2);
         let control_result = mock_dream_cycle(epoch, &[]);
 
         if control_result.coherence >= COHERENCE_THRESHOLD {
             control_pool.add_if_coherent(control_seed.clone(), control_result.clone());
         }
 
-        let control_record = EpochRecord::new(epoch, Group::Control, control_result.energy, control_result.coherence);
+        let control_record = EpochRecord::new(
+            epoch,
+            Group::Control,
+            control_result.energy,
+            control_result.coherence,
+        );
         csv_rows.push(control_record.to_csv_row());
         json_records.push(control_record.clone());
         final_control = Some(control_record);
 
         let mut test_seed = ChromaticTensor::from_seed(test_rng.gen(), 8, 8, 2);
         let query_signature = test_seed.mean_rgb();
         let retrieved = test_pool.retrieve_similar(&query_signature, RETRIEVAL_LIMIT);
 
         if !retrieved.is_empty() {
             for entry in &retrieved {
                 test_seed = blend_with_alpha(&test_seed, &entry.tensor, ALPHA_BLEND);
             }
         }
 
         let test_result = mock_dream_cycle(epoch, &retrieved);
 
         if test_result.coherence >= COHERENCE_THRESHOLD {
             test_pool.add_if_coherent(test_seed.clone(), test_result.clone());
         }
 
-        let test_record = EpochRecord::new(epoch, Group::Test, test_result.energy, test_result.coherence);
+        let test_record = EpochRecord::new(
+            epoch,
+            Group::Test,
+            test_result.energy,
+            test_result.coherence,
+        );
         csv_rows.push(test_record.to_csv_row());
         json_records.push(test_record.clone());
         final_test = Some(test_record);
     }
 
     let stats_control = control_pool.stats();
     let stats_test = test_pool.stats();
 
     HarnessRun {
         csv: csv_rows.join("\n"),
         json: serde_json::to_string(&json_records).expect("json serialization should succeed"),
         final_control: final_control.expect("control record present"),
         final_test: final_test.expect("test record present"),
         control_pool_count: stats_control.count,
         test_pool_count: stats_test.count,
     }
 }
 
-fn blend_with_alpha(base: &ChromaticTensor, retrieved: &ChromaticTensor, alpha: f32) -> ChromaticTensor {
+fn blend_with_alpha(
+    base: &ChromaticTensor,
+    retrieved: &ChromaticTensor,
+    alpha: f32,
+) -> ChromaticTensor {
     assert_eq!(base.colors.dim(), retrieved.colors.dim());
     assert_eq!(base.certainty.dim(), retrieved.certainty.dim());
 
     if (alpha - 0.5).abs() < f32::EPSILON {
         return mix(base, retrieved);
     }
 
     let mut colors = base.colors.clone();
     let mut certainty = base.certainty.clone();
 
     let (rows, cols, layers, channels) = colors.dim();
     for row in 0..rows {
         for col in 0..cols {
             for layer in 0..layers {
                 for channel in 0..channels {
                     let base_val = base.colors[[row, col, layer, channel]];
                     let retrieved_val = retrieved.colors[[row, col, layer, channel]];
-                    colors[[row, col, layer, channel]] = base_val * (1.0 - alpha) + retrieved_val * alpha;
+                    colors[[row, col, layer, channel]] =
+                        base_val * (1.0 - alpha) + retrieved_val * alpha;
                 }
                 let base_cert = base.certainty[[row, col, layer]];
                 let retrieved_cert = retrieved.certainty[[row, col, layer]];
                 certainty[[row, col, layer]] = base_cert * (1.0 - alpha) + retrieved_cert * alpha;
             }
         }
     }
 
     ChromaticTensor::from_arrays(colors, certainty)
 }
 
 fn mock_dream_cycle(epoch: usize, retrieved: &[DreamEntry]) -> SolverResult {
     let base_coherence = 0.76 + 0.0015 * epoch as f64;
 
     let retrieval_bonus = if retrieved.is_empty() {
         0.0
     } else {
         let mean_retrieved = retrieved
             .iter()
             .map(|entry| entry.result.coherence)
             .sum::<f64>()
             / retrieved.len() as f64;
         0.03 + (mean_retrieved - COHERENCE_THRESHOLD).max(0.0) * 0.1
     };
 
@@ -200,46 +215,50 @@ fn parse_csv_records(csv: &str) -> Vec<EpochRecord> {
         .map(|line| {
             let parts: Vec<&str> = line.split(',').collect();
             assert_eq!(parts.len(), 4, "each CSV row must have four columns");
 
             EpochRecord {
                 epoch: parts[0].parse().expect("epoch should parse"),
                 group: parts[1].to_string(),
                 avg_loss: parts[2].parse().expect("loss should parse"),
                 avg_coherence: parts[3].parse().expect("coherence should parse"),
             }
         })
         .collect()
 }
 
 #[test]
 fn validation_harness_retrieval_outperforms_control() {
     let first_run = execute_harness(RANDOM_SEED);
 
     assert!(first_run.final_test.avg_loss < first_run.final_control.avg_loss);
     assert!(first_run.final_test.avg_coherence > first_run.final_control.avg_coherence);
     assert!(first_run.control_pool_count <= POOL_MAX_SIZE);
     assert!(first_run.test_pool_count <= POOL_MAX_SIZE);
 
     let csv_records = parse_csv_records(&first_run.csv);
     assert_eq!(csv_records.len(), NUM_EPOCHS * 2);
-    assert_eq!(csv_records.first().map(|r| r.group.as_str()), Some("Control"));
+    assert_eq!(
+        csv_records.first().map(|r| r.group.as_str()),
+        Some("Control")
+    );
     let csv_last = csv_records.last().expect("csv should contain final row");
     assert_eq!(csv_last.epoch, first_run.final_test.epoch);
     assert_eq!(csv_last.group, first_run.final_test.group);
     assert!((csv_last.avg_loss - first_run.final_test.avg_loss).abs() < 1e-5);
     assert!((csv_last.avg_coherence - first_run.final_test.avg_coherence).abs() < 1e-5);
 
-    let json_records: Vec<EpochRecord> = serde_json::from_str(&first_run.json).expect("json parsing succeeds");
+    let json_records: Vec<EpochRecord> =
+        serde_json::from_str(&first_run.json).expect("json parsing succeeds");
     assert_eq!(json_records.len(), NUM_EPOCHS * 2);
     let json_last = json_records.last().expect("json should contain final row");
     assert_eq!(json_last.epoch, first_run.final_test.epoch);
     assert_eq!(json_last.group, first_run.final_test.group);
     assert!((json_last.avg_loss - first_run.final_test.avg_loss).abs() < 1e-12);
     assert!((json_last.avg_coherence - first_run.final_test.avg_coherence).abs() < 1e-12);
 
     let second_run = execute_harness(RANDOM_SEED);
     assert_eq!(first_run.csv, second_run.csv);
     assert_eq!(first_run.json, second_run.json);
     assert_eq!(first_run.final_control, second_run.final_control);
     assert_eq!(first_run.final_test, second_run.final_test);
 }
diff --git a/vendor/tracing_stub/Cargo.lock b/vendor/tracing_stub/Cargo.lock
new file mode 100644
index 0000000000000000000000000000000000000000..35803ea03242661f33a0cbcbd25eed7c9debcc45
--- /dev/null
+++ b/vendor/tracing_stub/Cargo.lock
@@ -0,0 +1,7 @@
+# This file is automatically @generated by Cargo.
+# It is not intended for manual editing.
+version = 4
+
+[[package]]
+name = "tracing"
+version = "0.1.0"
 
EOF
)